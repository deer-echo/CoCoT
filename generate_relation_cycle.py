"""
Qwen2-VLæ¨ç†é“¾æ„å»ºç³»ç»Ÿ
=====================
åŸºäºbboxä¿¡æ¯æ„å»ºä»é—®é¢˜åˆ°ç­”æ¡ˆçš„æ¨ç†é“¾ï¼š

æ ¸å¿ƒæ€è·¯ï¼š
é—®é¢˜å…³é”®è¯ â†’ bbox1 â†’ bbox2 â†’ ... â†’ ç­”æ¡ˆ

æ¨ç†å…³ç³»ï¼š
1. å…ˆåå…³ç³» - æœ‰é€»è¾‘é¡ºåºï¼ŒAå¿…é¡»åœ¨Bä¹‹å‰
2. å¹¶åˆ—å…³ç³» - åŒç­‰é‡è¦ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†
3. æ²¡æœ‰å…³ç³» - ä¸ç›¸å…³ï¼Œä¸å‚ä¸æ¨ç†é“¾

è¾“å‡ºï¼šç»“æ„åŒ–çš„æ¨ç†é“¾ï¼Œæ˜¾ç¤ºä»é—®é¢˜åˆ°ç­”æ¡ˆçš„å®Œæ•´è·¯å¾„
"""

import os
import sys
import json

def select_gpu_before_torch():
    """åœ¨å¯¼å…¥torchä¹‹å‰é€‰æ‹©GPU"""
    print("ğŸš€ Qwen2-VLæ¨ç†é“¾æ„å»ºå™¨")
    print("=" * 50)

    # ä¸´æ—¶å¯¼å…¥torchæ¥æ£€æµ‹GPU
    import subprocess

    try:
        # ä½¿ç”¨nvidia-smiè·å–GPUä¿¡æ¯
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used,memory.free',
                               '--format=csv,noheader,nounits'],
                              capture_output=True, text=True, timeout=10)

        if result.returncode != 0:
            print("âŒ æ— æ³•è·å–GPUä¿¡æ¯ï¼Œå°†ä½¿ç”¨é»˜è®¤è®¾ç½®")
            return '0'

        gpu_info = result.stdout.strip().split('\n')
        gpu_count = len(gpu_info)

        if gpu_count == 1:
            print(f"ğŸ¯ åªæœ‰1ä¸ªGPUå¯ç”¨ï¼Œè‡ªåŠ¨é€‰æ‹©GPU 0")
            return '0'

        print(f"\nğŸ¯ æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUï¼Œé€‰æ‹©ä½¿ç”¨æ–¹å¼:")

        for i, info in enumerate(gpu_info):
            parts = info.split(', ')
            if len(parts) >= 4:
                name = parts[0]
                total_mb = float(parts[1])
                used_mb = float(parts[2])
                free_mb = float(parts[3])

                total_gb = total_mb / 1024
                used_gb = used_mb / 1024
                free_gb = free_mb / 1024

                print(f"   GPU {i}: {name}")
                print(f"      æ€»å†…å­˜: {total_gb:.1f}GB")
                print(f"      å·²ä½¿ç”¨: {used_gb:.1f}GB")
                print(f"      å¯ç”¨: {free_gb:.1f}GB")

                # å¦‚æœGPUä½¿ç”¨ç‡å¾ˆé«˜ï¼Œç»™å‡ºæç¤º
                usage_percent = (used_gb / total_gb) * 100
                if usage_percent > 50:
                    print(f"      âš ï¸ ä½¿ç”¨ç‡: {usage_percent:.1f}% (è¾ƒé«˜)")
                elif usage_percent > 10:
                    print(f"      ğŸ“Š ä½¿ç”¨ç‡: {usage_percent:.1f}%")
                print()

        print("é€‰æ‹©ä½¿ç”¨æ–¹å¼:")
        print("   0. ä½¿ç”¨æ‰€æœ‰GPU")
        print("   1. ä½¿ç”¨ä¸¤ä¸ªGPU (æ¨èï¼Œæ›´å¿«)")
        print("   2. ä½¿ç”¨å•ä¸ªGPU")

        while True:
            try:
                choice = input("è¯·é€‰æ‹© (0/1/2): ").strip()
                if choice == "0":
                    print(f"âœ… å·²é€‰æ‹©ä½¿ç”¨æ‰€æœ‰ {gpu_count} ä¸ªGPU")
                    return ','.join(str(i) for i in range(gpu_count))
                elif choice == "1":
                    # ä½¿ç”¨ä¸¤ä¸ªGPU
                    if gpu_count >= 2:
                        print("è¯·é€‰æ‹©è¦ä½¿ç”¨çš„ä¸¤ä¸ªGPU:")
                        selected_gpus = []

                        # é€‰æ‹©ç¬¬ä¸€ä¸ªGPU
                        while True:
                            try:
                                gpu1 = input(f"è¯·é€‰æ‹©ç¬¬ä¸€ä¸ªGPU (0-{gpu_count-1}): ").strip()
                                gpu1_id = int(gpu1)
                                if 0 <= gpu1_id < gpu_count:
                                    selected_gpus.append(gpu1_id)
                                    break
                                else:
                                    print(f"âŒ è¯·è¾“å…¥0åˆ°{gpu_count-1}ä¹‹é—´çš„æ•°å­—")
                            except ValueError:
                                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

                        # é€‰æ‹©ç¬¬äºŒä¸ªGPU
                        while True:
                            try:
                                gpu2 = input(f"è¯·é€‰æ‹©ç¬¬äºŒä¸ªGPU (0-{gpu_count-1}ï¼Œä¸èƒ½ä¸ç¬¬ä¸€ä¸ªç›¸åŒ): ").strip()
                                gpu2_id = int(gpu2)
                                if 0 <= gpu2_id < gpu_count:
                                    if gpu2_id != selected_gpus[0]:
                                        selected_gpus.append(gpu2_id)
                                        break
                                    else:
                                        print("âŒ ç¬¬äºŒä¸ªGPUä¸èƒ½ä¸ç¬¬ä¸€ä¸ªç›¸åŒ")
                                else:
                                    print(f"âŒ è¯·è¾“å…¥0åˆ°{gpu_count-1}ä¹‹é—´çš„æ•°å­—")
                            except ValueError:
                                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

                        gpu_str = ','.join(str(gpu) for gpu in selected_gpus)
                        print(f"âœ… å·²é€‰æ‹©ä½¿ç”¨GPU {selected_gpus[0]}å’Œ{selected_gpus[1]}")
                        return gpu_str
                    else:
                        print("âŒ å¯ç”¨GPUæ•°é‡ä¸è¶³2ä¸ªï¼Œè¯·é€‰æ‹©å…¶ä»–é€‰é¡¹")
                elif choice == "2":
                    while True:
                        try:
                            gpu_choice = input(f"è¯·é€‰æ‹©å•ä¸ªGPU (0-{gpu_count-1}): ").strip()
                            gpu_id = int(gpu_choice)
                            if 0 <= gpu_id < gpu_count:
                                print(f"âœ… å·²é€‰æ‹©GPU {gpu_id}")
                                return str(gpu_id)
                            else:
                                print(f"âŒ è¯·è¾“å…¥0åˆ°{gpu_count-1}ä¹‹é—´çš„æ•°å­—")
                        except ValueError:
                            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                else:
                    print("âŒ è¯·è¾“å…¥0ã€1æˆ–2")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            except KeyboardInterrupt:
                print("\nâŒ ç”¨æˆ·å–æ¶ˆ")
                sys.exit(1)

    except Exception as e:
        print(f"âŒ è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
        print("å°†ä½¿ç”¨é»˜è®¤GPU 0")
        return '0'

# åœ¨å¯¼å…¥torchä¹‹å‰é€‰æ‹©GPUå¹¶è®¾ç½®ç¯å¢ƒå˜é‡
# æ£€æŸ¥æ˜¯å¦å·²ç»è®¾ç½®äº†CUDA_VISIBLE_DEVICESï¼ˆè¢«å…¶ä»–è„šæœ¬è°ƒç”¨æ—¶ï¼‰
if 'CUDA_VISIBLE_DEVICES' not in os.environ:
    # åªæœ‰åœ¨æ²¡æœ‰é¢„è®¾GPUæ—¶æ‰è¿›è¡Œäº¤äº’å¼é€‰æ‹©
    selected_gpu = select_gpu_before_torch()
    os.environ['CUDA_VISIBLE_DEVICES'] = selected_gpu
    print(f"ğŸ”§ è®¾ç½®CUDA_VISIBLE_DEVICES = {selected_gpu}")
else:
    # ä½¿ç”¨å·²ç»è®¾ç½®çš„GPU
    selected_gpu = os.environ['CUDA_VISIBLE_DEVICES']
    print(f"ğŸ”§ ä½¿ç”¨é¢„è®¾çš„CUDA_VISIBLE_DEVICES = {selected_gpu}")

# ğŸš¨ é‡è¦ï¼šå¿…é¡»åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®CUDAç¯å¢ƒå˜é‡
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_USE_CUDA_DSA'] = '0'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,expandable_segments:False'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# ç°åœ¨å¯ä»¥å®‰å…¨å¯¼å…¥torchå’Œå…¶ä»–ä¾èµ–
import torch
from PIL import Image
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# ==================== ç”Ÿæˆæ§åˆ¶é…ç½® ====================
# æ§åˆ¶é‡æ–°ç”Ÿæˆçš„èŒƒå›´
REGENERATION_CONFIG = {
    # é‡æ–°ç”Ÿæˆæ‰€æœ‰å¤šbboxé—®é¢˜ (bbox_count > 1)
    "regenerate_multi_bbox": False,

    # é‡æ–°ç”Ÿæˆæ‰€æœ‰å•bboxé—®é¢˜ (bbox_count == 1)
    "regenerate_single_bbox": False,

    # é‡æ–°ç”Ÿæˆæ‰€æœ‰é—®é¢˜ (å¿½ç•¥ç°æœ‰ç»“æœ)
    "regenerate_all": False,

    # è‡ªåŠ¨é‡æ–°ç”Ÿæˆå¤±è´¥çš„æ•°æ® (æ¨ç†æ­¥æ•°ä¸º0æˆ–é”™è¯¯çŠ¶æ€)
    "regenerate_failed": True,

    # åªå¤„ç†ç‰¹å®šbboxæ•°é‡çš„é—®é¢˜ (Noneè¡¨ç¤ºå¤„ç†æ‰€æœ‰)
    "target_bbox_count": None,  # ä¾‹å¦‚: 2, 3, 4 ç­‰

    # æœ€å°bboxæ•°é‡é˜ˆå€¼ (å°äºæ­¤æ•°é‡çš„é—®é¢˜ä¼šè¢«è·³è¿‡)
    "min_bbox_count": 1,

    # æœ€å¤§bboxæ•°é‡é˜ˆå€¼ (å¤§äºæ­¤æ•°é‡çš„é—®é¢˜ä¼šè¢«è·³è¿‡)
    "max_bbox_count": None,  # Noneè¡¨ç¤ºæ— é™åˆ¶

    # æ˜¯å¦è·³è¿‡å·²æœ‰æ¨ç†é“¾çš„é—®é¢˜
    "skip_existing": True,

    # è¯¦ç»†æ—¥å¿—è¾“å‡º
    "verbose_logging": True,

    # æ–°å¢ï¼šbboxç”Ÿæˆæ¨¡å¼é€‰æ‹©
    "bbox_generation_mode": "auto"  # "single", "multi", "auto"
}

# æ•°æ®é›†é…ç½®
DATASETS = {
    "docvqa": { # 13.0 ready 11995
        "name": "DocVQA",
        "image_folder": "playground/data/cot/docvqa",
        "bbox_file": "images_bbox/DocVQA_complex_one_agent.json",
        "output_file": "reasoning_chains/DocVQA_complex_reasoning_chains_one_agent.json",
        "image_id_field": "image_name",
        "question_id_field": "question_id",
        "default_max_samples": None  # ä½¿ç”¨æ‰€æœ‰æ•°æ®
    },
    "infovqa": {  # 13.1  1.4w/21668
        "name": "InfoVQA",
        "image_folder": "playground/data/cot/infographicsvqa",
        "bbox_file": "images_bbox/InfoVQA_complex_one_agent.json",
        "output_file": "reasoning_chains/InfoVQA_complex_reasoning_chains_one_agent.json",
        "image_id_field": "image_name",
        "question_id_field": "question_id",
        "default_max_samples": 21668  # ä½¿ç”¨æ‰€æœ‰æ•°æ®
    },
    "textvqa": { # 13.2 0.79 W/12508
        "name": "TextVQA", # 13.2 ready
        "image_folder": "playground/data/cot/textvqa",
        "bbox_file": "images_bbox/TextVQA_complex_one_agent.json",
        "output_file": "reasoning_chains/TextVQA_complex_reasoning_chains_one_agent.json",
        "image_id_field": "image_name",
        "question_id_field": "question_id",
        "default_max_samples": None  # ä½¿ç”¨æ‰€æœ‰æ•°æ®
    },
    "visual7w": { # ready 148.1/17954
        "name": "Visual7W",
        "image_folder": "playground/data/cot/v7w",
        "bbox_file": "images_bbox/Visual7W_complex_one_agent.json",
        "output_file": "reasoning_chains/Visual7W_complex_reasoning_chains_one_agent.json",
        "image_id_field": "image_name",
        "question_id_field": "question_id",
        "default_max_samples": None  # ä½¿ç”¨æ‰€æœ‰æ•°æ®
    },
    "gqa": { # 148.2/37592
        "name": "GQA",
        "image_folder": "playground/data/cot/gqa",
        "bbox_file": "images_bbox/GQA_complex_one_agent.json",
        "output_file": "reasoning_chains/GQA_complex_reasoning_chains_one_agent.json",
        "image_id_field": "image_name",
        "question_id_field": "question_id",
        "default_max_samples": None  # ä½¿ç”¨æ‰€æœ‰æ•°æ®
    },
    "vqav2": { # ready 148.3/35383
        "name": "VQAv2",
        "image_folder": "playground/data/cot/coco",
        "bbox_file": "images_bbox/VQAv2_complex_one_agent.json",
        "output_file": "reasoning_chains/VQAv2_complex_reasoning_chains_one_agent.json",
        "image_id_field": "image_name",
        "question_id_field": "question_id",
        "default_max_samples": None  # ä½¿ç”¨æ‰€æœ‰æ•°æ®
    }
}

# é…ç½®è·¯å¾„
MODEL_PATH = "Qwen2-VL-7B-Instruct"

# å…¨å±€å˜é‡
model = None
processor = None

def crop_bbox_from_image(image, bbox_info):
    """ä»å›¾åƒä¸­è£å‰ªbboxåŒºåŸŸ"""
    try:
        # è·å–bboxåæ ‡
        bbox_coords = bbox_info.get('bbox_coordinates', bbox_info.get('bbox', []))

        if not bbox_coords or len(bbox_coords) != 4:
            return None

        # è·å–å›¾åƒå°ºå¯¸
        img_width, img_height = image.size

        # å¤„ç†å½’ä¸€åŒ–åæ ‡ (0-1) æˆ–åƒç´ åæ ‡
        if all(coord <= 1.0 for coord in bbox_coords):
            # å½’ä¸€åŒ–åæ ‡ï¼Œè½¬æ¢ä¸ºåƒç´ åæ ‡
            x1 = int(bbox_coords[0] * img_width)
            y1 = int(bbox_coords[1] * img_height)
            x2 = int(bbox_coords[2] * img_width)
            y2 = int(bbox_coords[3] * img_height)
        else:
            # å·²ç»æ˜¯åƒç´ åæ ‡
            x1, y1, x2, y2 = map(int, bbox_coords)

        # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
        x1 = max(0, min(x1, img_width))
        y1 = max(0, min(y1, img_height))
        x2 = max(0, min(x2, img_width))
        y2 = max(0, min(y2, img_height))

        # ç¡®ä¿x2 > x1, y2 > y1
        if x2 <= x1 or y2 <= y1:
            return None

        # è£å‰ªå›¾åƒ
        cropped_image = image.crop((x1, y1, x2, y2))

        # å¦‚æœè£å‰ªåŒºåŸŸå¤ªå°ï¼Œé€‚å½“æ”¾å¤§
        if cropped_image.size[0] < 50 or cropped_image.size[1] < 50:
            # æ”¾å¤§åˆ°è‡³å°‘100x100
            new_size = (max(100, cropped_image.size[0]), max(100, cropped_image.size[1]))
            cropped_image = cropped_image.resize(new_size, Image.Resampling.LANCZOS)

        return cropped_image

    except Exception as e:
        print(f"âŒ è£å‰ªbboxå¤±è´¥: {e}")
        return None

def extract_question_keywords(question):
    """æå–é—®é¢˜ä¸­çš„å…³é”®è¯ä½œä¸ºæ¨ç†é“¾çš„èµ·ç‚¹"""
    import re

    question_lower = question.lower()

    # ç§»é™¤å¸¸è§çš„ç–‘é—®è¯å’Œåœç”¨è¯
    stop_words = {'what', 'which', 'how', 'when', 'where', 'why', 'who', 'is', 'are', 'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}

    # æå–å…³é”®è¯
    words = re.findall(r'\b\w+\b', question_lower)
    keywords = [word for word in words if word not in stop_words and len(word) > 2]

    # æå–æ•°å­—å’Œå¹´ä»½
    numbers = re.findall(r'\b\d{4}\b|\b\d+\b', question)

    # æå–å¼•å·ä¸­çš„å†…å®¹
    quoted = re.findall(r'"([^"]*)"', question)

    all_keywords = keywords + numbers + quoted

    return {
        "keywords": keywords[:5],  # é™åˆ¶å…³é”®è¯æ•°é‡
        "numbers": numbers,
        "quoted_terms": quoted,
        "all_terms": all_keywords
    }


def analyze_spatial_relationships(bbox_list, used_regions, question_type="parallel"):
    """åˆ†æbboxçš„ç©ºé—´å…³ç³»ï¼Œä¸ºparallelæ¨ç†æä¾›æç¤º"""
    if not bbox_list or len(bbox_list) < 2:
        return ""

    # è·å–å·²ä½¿ç”¨çš„bboxåæ ‡
    used_coords = []
    for idx in used_regions:
        if idx < len(bbox_list):
            coords = bbox_list[idx].get('bbox_coordinates', bbox_list[idx].get('bbox', []))
            if len(coords) == 4:
                used_coords.append((idx, coords))

    if not used_coords:
        return ""

    # åˆ†æå¯ç”¨çš„bbox
    available_regions = []
    for i, bbox in enumerate(bbox_list):
        if i not in used_regions:
            coords = bbox.get('bbox_coordinates', bbox.get('bbox', []))
            if len(coords) == 4:
                available_regions.append((i, coords))

    if not available_regions:
        return ""

    # åˆ†æç©ºé—´å…³ç³»
    spatial_hints = []
    comparison_hints = []

    for used_idx, used_coord in used_coords:
        used_x1, used_y1, used_x2, used_y2 = used_coord
        used_center_x = (used_x1 + used_x2) / 2
        used_center_y = (used_y1 + used_y2) / 2

        # æ‰¾åˆ°åœ¨ç›¸ä¼¼ä½ç½®çš„region
        horizontal_aligned = []
        vertical_aligned = []
        nearby_regions = []

        for region_idx, coords in available_regions:
            x1, y1, x2, y2 = coords
            center_x = (x1 + x2) / 2
            center_y = (y1 + y2) / 2

            # æ£€æŸ¥æ°´å¹³å¯¹é½ (ç›¸ä¼¼çš„yåæ ‡) - é€‚åˆè¡¨æ ¼è¡Œ
            if abs(center_y - used_center_y) < 0.1:  # 10%çš„å®¹å·®
                horizontal_aligned.append(f"Region {region_idx}")

            # æ£€æŸ¥å‚ç›´å¯¹é½ (ç›¸ä¼¼çš„xåæ ‡) - é€‚åˆè¡¨æ ¼åˆ—
            if abs(center_x - used_center_x) < 0.1:  # 10%çš„å®¹å·®
                vertical_aligned.append(f"Region {region_idx}")

            # æ£€æŸ¥é‚»è¿‘åŒºåŸŸ - é€‚åˆæ¯”è¾ƒ/æ’åº
            distance = ((center_x - used_center_x)**2 + (center_y - used_center_y)**2)**0.5
            if distance < 0.3:  # 30%çš„è·ç¦»å†…
                nearby_regions.append(f"Region {region_idx}")

        if horizontal_aligned:
            spatial_hints.append(f"Same row regions: {', '.join(horizontal_aligned)}")

        if vertical_aligned:
            spatial_hints.append(f"Same column regions: {', '.join(vertical_aligned)}")

        if nearby_regions:
            comparison_hints.append(f"Nearby comparison regions: {', '.join(nearby_regions)}")

    # æ ¹æ®é—®é¢˜ç±»å‹ç”Ÿæˆä¸åŒçš„æç¤º
    hint_text = ""
    if question_type == "parallel":
        if spatial_hints:
            hint_text += f"\nğŸ” Spatial Layout: {'; '.join(spatial_hints)}."
        if comparison_hints:
            hint_text += f"\nğŸ“Š For comparison questions: Consider {'; '.join(comparison_hints)}."

        # æ·»åŠ æ’åº/æ¯”è¾ƒçš„ç‰¹æ®Šæç¤º
        hint_text += f"\nğŸ’¡ Comparison Strategy: For ranking questions (highest/lowest/most/least), look for regions with similar content types (numbers, percentages, names) that can be compared directly."

    return hint_text

def generate_content_based_reasoning(question, bbox_content, description, question_type="parallel"):
    """åŸºäºå®é™…bboxå†…å®¹ç”Ÿæˆæ¨ç†ï¼Œé¿å…å¹»è§‰"""

    # æ¸…ç†å’Œæ ‡å‡†åŒ–å†…å®¹
    content = bbox_content or description or ""
    content = content.strip()

    if not content:
        return "Selected region contains relevant information"

    # æå–é—®é¢˜ä¸­çš„å…³é”®è¯
    question_lower = question.lower()
    content_lower = content.lower()

    # æ£€æŸ¥å†…å®¹æ˜¯å¦ç›´æ¥åŒ…å«ç­”æ¡ˆ
    question_words = question_lower.split()
    content_words = content_lower.split()

    # å¯»æ‰¾å…±åŒè¯æ±‡
    common_words = set(question_words) & set(content_words)

    # é’ˆå¯¹æ’åº/æ¯”è¾ƒé—®é¢˜çš„ç‰¹æ®Šå¤„ç†
    if question_type == "parallel" and any(word in question_lower for word in ['second', 'third', 'highest', 'lowest', 'most', 'least']):
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å€¼ä¿¡æ¯
        import re
        numbers = re.findall(r'\d+\.?\d*%?', content)
        if numbers:
            return f"'{content}' contains numerical value {numbers[0]} which can be compared with other regions to determine ranking"
        elif any(word in content_lower for word in ['first', 'second', 'third', 'top', 'bottom']):
            return f"'{content}' contains ranking information that helps determine the position in comparison"
        else:
            return f"'{content}' represents one option that needs to be compared with others to answer the ranking question"

    if common_words:
        # å¦‚æœæœ‰å…±åŒè¯æ±‡ï¼Œè¯´æ˜å†…å®¹ç›¸å…³
        if any(word in content_lower for word in ['dpi', 'dots', 'inch']):
            return f"'{content}' provides the specific measurement requested in the question"
        elif any(word in content_lower for word in ['font', 'serif', 'sans']):
            return f"'{content}' mentions the font type relevant to the question"
        elif any(word in content_lower for word in ['color', 'blue', 'red', 'green']):
            return f"'{content}' specifies the color information asked about"
        elif any(word in content_lower for word in ['number', 'count', 'total']):
            return f"'{content}' provides numerical information relevant to the question"
        else:
            return f"'{content}' contains keywords relevant to the question"
    else:
        # å¦‚æœæ²¡æœ‰æ˜æ˜¾å…±åŒè¯æ±‡ï¼Œç”Ÿæˆé€šç”¨ä½†å…·ä½“çš„æ¨ç†
        return f"Region contains '{content}' which may provide context for answering the question"


def determine_single_bbox_role(question, bbox_content, suggested_role=None):
    """ä¸ºå•ä¸ªbboxç¡®å®šè§’è‰²"""
    if suggested_role and suggested_role in ["direct_answer", "evidence", "keyword_match"]:
        return suggested_role

    question_lower = question.lower()
    content_lower = bbox_content.lower()

    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›´æ¥ç­”æ¡ˆçš„å…³é”®è¯
    if any(word in content_lower for word in ['yes', 'no', 'true', 'false']):
        return "direct_answer"

    # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜å…³é”®è¯
    question_words = set(question_lower.split())
    content_words = set(content_lower.split())
    if len(question_words.intersection(content_words)) > 0:
        return "keyword_match"

    # é»˜è®¤ä¸ºè¯æ®
    return "evidence"

def analyze_question_type(question):
    """åˆ†æé—®é¢˜ç±»å‹ï¼Œåˆ¤æ–­éœ€è¦é¡ºåºé“¾æ¡è¿˜æ˜¯å¹¶åˆ—é“¾æ¡"""
    question_lower = question.lower()

    # çœŸæ­£çš„é¡ºåºå…³ç³»æŒ‡ç¤ºè¯ï¼ˆæ—¶é—´/æ­¥éª¤é¡ºåºï¼‰
    sequential_indicators = [
        'then', 'next', 'after', 'before', 'step', 'stage', 'process', 'sequence',
        'chronological', 'timeline', 'follow', 'subsequent', 'prior', 'earlier',
        'later', 'initially', 'finally', 'lastly', 'procedure', 'workflow',
        'step by step', 'one by one', 'in order', 'in sequence'
    ]

    # æ’åº/æ¯”è¾ƒå…³ç³»æŒ‡ç¤ºè¯ï¼ˆéœ€è¦å¹¶åˆ—æ¯”è¾ƒï¼‰
    ranking_comparison_indicators = [
        'first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth',
        'highest', 'lowest', 'most', 'least', 'best', 'worst', 'top', 'bottom',
        'popular', 'common', 'frequent', 'rare', 'maximum', 'minimum', 'largest', 'smallest',
        'biggest', 'tiniest', 'greater', 'lesser', 'more', 'fewer', 'rank', 'ranking',
        'compare', 'comparison', 'versus', 'vs', 'better', 'worse', 'superior', 'inferior'
    ]

    # å¹¶åˆ—å…³ç³»æŒ‡ç¤ºè¯
    parallel_indicators = [
        'and', 'also', 'both', 'either', 'or', 'all', 'each', 'every', 'multiple',
        'various', 'different', 'several', 'many', 'list', 'enumerate', 'include',
        'contain', 'comprise', 'as well as', 'in addition', 'together'
    ]

    # è®¡ç®—æŒ‡ç¤ºè¯å‡ºç°æ¬¡æ•°
    sequential_count = sum(1 for word in sequential_indicators if word in question_lower)
    ranking_count = sum(1 for word in ranking_comparison_indicators if word in question_lower)
    parallel_count = sum(1 for word in parallel_indicators if word in question_lower)

    # ç‰¹æ®Šæ¨¡å¼æ£€æµ‹
    has_time_sequence = any(pattern in question_lower for pattern in [
        'what happens', 'how to', 'steps to', 'process of', 'procedure for',
        'how do you', 'what do you do', 'what should you do'
    ])

    has_listing = any(pattern in question_lower for pattern in [
        'what are', 'list all', 'name all', 'which ones', 'what types',
        'what kinds', 'what categories', 'enumerate'
    ])

    has_ranking_comparison = any(pattern in question_lower for pattern in [
        'which is the', 'which has the', 'which country', 'which state',
        'which city', 'what is the most', 'what is the least', 'what percentage',
        'where is the', 'where can', 'how much', 'how many', 'how long'
    ])

    # ç‰¹æ®Šæƒ…å†µï¼šåŒ…å«stepä½†è¯¢é—®å…·ä½“æ­¥éª¤å†…å®¹çš„é—®é¢˜
    has_step_inquiry = any(pattern in question_lower for pattern in [
        'what is the first step', 'what is the next step', 'what is the last step',
        'what follows after', 'what comes after', 'what happens in the'
    ])

    # åˆ¤æ–­é—®é¢˜ç±»å‹
    if has_time_sequence or (sequential_count > 0 and ranking_count == 0 and not has_ranking_comparison):
        # çœŸæ­£çš„æ—¶é—´/æ­¥éª¤é¡ºåº
        return "sequential"
    elif (sequential_count > 0 and 'step' in question_lower and 'process' in question_lower):
        # æ˜ç¡®çš„æ­¥éª¤è¿‡ç¨‹é—®é¢˜
        return "sequential"
    elif has_step_inquiry:
        # è¯¢é—®å…·ä½“æ­¥éª¤çš„é—®é¢˜
        return "sequential"
    elif has_ranking_comparison or ranking_count > 0 or has_listing or parallel_count > sequential_count:
        # æ’åºæ¯”è¾ƒã€åˆ—ä¸¾ã€å¹¶åˆ—å…³ç³»
        return "parallel"
    else:
        # é»˜è®¤æ ¹æ®é—®é¢˜ç»“æ„åˆ¤æ–­
        if 'what' in question_lower and ('are' in question_lower or 'all' in question_lower):
            return "parallel"
        elif 'which' in question_lower or 'what' in question_lower or 'where' in question_lower or 'how' in question_lower:
            # å¤§éƒ¨åˆ†which/what/where/howé—®é¢˜éƒ½æ˜¯æŸ¥è¯¢é€‰æ‹©ï¼Œå±äºå¹¶åˆ—
            return "parallel"
        else:
            return "sequential"

def build_reasoning_chain_with_multi_qwen(image_path, question, bbox_list):
    """ä½¿ç”¨å¤šè½®Qwenæ¨¡å‹é€æ­¥æ„å»ºæ¨ç†é“¾"""
    print("ğŸ”— å¼€å§‹ä½¿ç”¨å¤šè½®Qwenæ„å»ºæ¨ç†é“¾...")

    # ç¬¬ä¸€æ­¥ï¼šæå–é—®é¢˜å…³é”®è¯
    keywords = extract_question_keywords(question)
    print(f"ğŸ”‘ å…³é”®è¯: {keywords['keywords']}")

    # ç¬¬äºŒæ­¥ï¼šæ„å»ºbboxä¿¡æ¯æ‘˜è¦
    bbox_summary = []
    for i, bbox in enumerate(bbox_list):
        content = bbox.get('bbox_description', bbox.get('description', ''))
        bbox_summary.append(f"Region {i}: {content}")

    reasoning_steps = []
    used_regions = set()
    current_context = f"Question: {question}\nKeywords: {', '.join(keywords['keywords'])}"

    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœåªæœ‰ä¸€ä¸ªbboxï¼Œç›´æ¥è¿›è¡Œå•æ­¥æ¨ç†
    if len(bbox_list) == 1:
        print(f"ğŸ“ å•bboxæ¨ç†æ¨¡å¼")

        # åˆ†æé—®é¢˜ç±»å‹ï¼ˆå•bboxæ¨¡å¼ä¹Ÿéœ€è¦ï¼‰
        question_type = analyze_question_type(question)

        try:
            single_bbox = bbox_list[0]
            bbox_content = single_bbox.get('bbox_description', single_bbox.get('description', ''))
            description = single_bbox.get('description', bbox_content)

            # æ„å»ºå•æ­¥æ¨ç†prompt - è®©Qwenç”Ÿæˆç®€æ´çš„æ¨ç†å†…å®¹
            single_step_prompt = f"""{current_context}

Available region:
Region: {bbox_content}

Task: Analyze how this region answers the question. Generate a concise explanation (max 30 words).

IMPORTANT: Base your analysis ONLY on what you can actually see in the bbox image above.

Instructions:
1. Look at the actual content visible in the bbox image
2. Start with the key information from the region (e.g., "100 DPI")
3. Explain how it directly answers the question
4. Use format: "[key info] directly answers/provides [question aspect]"
5. Do NOT make up content that is not visible in the bbox

Example format: "100 DPI directly answers the question 'how many dots' for printed medium"

Output format:
SELECTED_REGION: Region 0
ROLE: direct_answer/evidence
REASONING: [key info] directly answers/provides [question aspect]
RELATIONSHIP: none
"""

            # è°ƒç”¨Qwenè¿›è¡Œå•æ­¥åˆ†æ
            step_result = call_qwen_single_step(image_path, single_step_prompt, bbox_list)

            if step_result:
                print(f"ğŸ¤– å•æ­¥Qwenè¾“å‡º:\n{step_result}")
                selected_region, role, reasoning, relationship = parse_single_step_result(step_result)

                # ğŸ”§ å•æ¡†æ¨ç†ï¼šä¼˜å…ˆä½¿ç”¨Qwençš„reasoningï¼Œå¦‚æœè¡¨ç°ä¸å¥½åˆ™ä½¿ç”¨contentå’Œdescription
                if reasoning and reasoning.strip() and reasoning.strip() not in ["Single region analysis", "Analysis of selected region"]:
                    # ä½¿ç”¨Qwenç”Ÿæˆçš„æ¨ç†
                    generated_reasoning = reasoning.strip()
                    if len(generated_reasoning) > 200:  # é™åˆ¶æœ€å¤§é•¿åº¦
                        generated_reasoning = generated_reasoning[:200] + "..."
                else:
                    # å›é€€åˆ°ä½¿ç”¨contentå’Œdescription
                    generated_reasoning = generate_content_based_reasoning(question, bbox_content, description, question_type)

                # æ™ºèƒ½åˆ¤æ–­å•bboxçš„role
                determined_role = determine_single_bbox_role(question, bbox_content, role)

                reasoning_steps.append({
                    "step": 1,
                    "bbox_index": 0,
                    "bbox_content": bbox_content,  # ä¿ç•™åŸå§‹descriptionä½œä¸ºå‚è€ƒ
                    "description": description,
                    "generated_reasoning": generated_reasoning,  # ä½¿ç”¨æ™ºèƒ½é€‰æ‹©çš„æ¨ç†
                    "role": determined_role,
                    "relationship_to_previous": "none",
                    "qwen_analysis": step_result,
                    "bbox_coordinates": single_bbox.get('bbox_coordinates', single_bbox.get('bbox', []))
                })

                print(f"âœ… å•æ­¥æ¨ç†å®Œæˆ: {bbox_content[:50]}... (è§’è‰²: {role or 'direct_answer'})")
            else:
                print(f"âŒ å•æ­¥æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨åŸºäºå†…å®¹çš„æ¨ç†")
                # æ™ºèƒ½åˆ¤æ–­role
                determined_role = determine_single_bbox_role(question, bbox_content)

                # ä½¿ç”¨åŸºäºå†…å®¹çš„æ¨ç†ä½œä¸ºå›é€€
                generated_reasoning = generate_content_based_reasoning(question, bbox_content, description, question_type)

                reasoning_steps.append({
                    "step": 1,
                    "bbox_index": 0,
                    "bbox_content": bbox_content,
                    "description": description,
                    "generated_reasoning": generated_reasoning,
                    "role": determined_role,
                    "relationship_to_previous": "none",
                    "qwen_analysis": "Single bbox fallback with content-based reasoning",
                    "bbox_coordinates": single_bbox.get('bbox_coordinates', single_bbox.get('bbox', []))
                })

        except Exception as e:
            print(f"âŒ å•æ­¥æ¨ç†å‡ºé”™: {e}")
            # åˆ›å»ºé»˜è®¤çš„å•æ­¥æ¨ç†
            single_bbox = bbox_list[0]
            bbox_content = single_bbox.get('bbox_description', single_bbox.get('description', ''))
            description = single_bbox.get('description', bbox_content)

            # æ™ºèƒ½åˆ¤æ–­role
            determined_role = determine_single_bbox_role(question, bbox_content)

            # å¼‚å¸¸æƒ…å†µä¸‹ä½¿ç”¨åŸºäºå†…å®¹çš„æ¨ç†
            generated_reasoning = generate_content_based_reasoning(question, bbox_content, description, question_type)

            reasoning_steps.append({
                "step": 1,
                "bbox_index": 0,
                "bbox_content": bbox_content,
                "description": description,
                "generated_reasoning": generated_reasoning,
                "role": determined_role,
                "relationship_to_previous": "none",
                "qwen_analysis": "Error fallback with content-based reasoning",
                "bbox_coordinates": single_bbox.get('bbox_coordinates', single_bbox.get('bbox', []))
            })

    else:
        # å¤šè½®æ¨ç†ï¼šæ¯è½®æ‰¾ä¸‹ä¸€ä¸ªæœ€ç›¸å…³çš„bbox
        print(f"ğŸ”„ å¤šæ­¥æ¨ç†æ¨¡å¼ï¼Œå…±{len(bbox_list)}ä¸ªbbox")
        for step_num in range(1, len(bbox_list) + 1):  # å¤„ç†æ‰€æœ‰å¯ç”¨çš„bbox
            print(f"\nğŸ” ç¬¬{step_num}è½®Qwenåˆ†æ...")

            # æ„å»ºå½“å‰å¯ç”¨çš„bboxåˆ—è¡¨ï¼ˆæ’é™¤å·²ä½¿ç”¨çš„ï¼‰
            available_regions = []
            available_bbox_list = []
            for i, bbox in enumerate(bbox_list):
                if i not in used_regions:
                    content = bbox.get('bbox_description', bbox.get('description', ''))
                    available_regions.append(f"Region {i}: {content}")
                    available_bbox_list.append(bbox)

            if not available_regions:
                print("ğŸ“ æ‰€æœ‰åŒºåŸŸå·²ä½¿ç”¨å®Œæ¯•")
                break

            # åˆ†æé—®é¢˜ç±»å‹
            question_type = analyze_question_type(question)

            # æ„å»ºå½“å‰æ­¥éª¤çš„prompt
            if step_num == 1:
                role_instruction = f"Find the region that best matches the question keywords and serves as the starting point. IMPORTANT: This is just the first step - you should explore multiple regions to gather comprehensive information before concluding. Try to use most of the available regions. Question type appears to be: {question_type}"
                expected_role = "keyword_match"
            else:
                if question_type == "parallel":
                    # ä¸ºparallelç±»å‹æ·»åŠ ç©ºé—´ä½ç½®æç¤º
                    spatial_hint = analyze_spatial_relationships(bbox_list, used_regions, question_type)

                    # æ ¹æ®é—®é¢˜ç±»å‹æä¾›æ›´å…·ä½“çš„æŒ‡å¯¼
                    question_lower = question.lower()
                    if any(word in question_lower for word in ['second', 'third', 'highest', 'lowest', 'most', 'least']):
                        role_instruction = f"Continue exploring regions containing comparable values/options for ranking comparison. You should examine multiple regions to gather all relevant options before concluding. Look for: 1) Numbers/percentages that can be compared, 2) Items in the same category (countries, products, etc.), 3) Regions with similar formatting/layout.{spatial_hint}"
                    elif any(word in question_lower for word in ['what are', 'list', 'which ones', 'all']):
                        role_instruction = f"Continue finding regions that contain additional items/options to complete the list. Explore multiple regions to ensure you capture all relevant items. Look for regions with similar content structure or formatting.{spatial_hint}"
                    else:
                        role_instruction = f"Continue exploring regions that provide additional information to answer the question comprehensively. Don't conclude too early - examine multiple regions to gather complete evidence. For ranking/comparison questions, look for regions with comparable content (numbers, percentages, names) that can be directly compared.{spatial_hint}"
                else:
                    role_instruction = f"Based on the previous steps, find the next region that logically follows in sequence to answer the question."
                expected_role = "next_step"

            # è®¡ç®—ä½¿ç”¨è¿›åº¦
            used_count = len(used_regions)
            total_count = len(bbox_list)
            progress_info = f"Progress: Used {used_count}/{total_count} regions. Try to explore most regions before concluding."

            step_prompt = f"""{current_context}

{progress_info}

Question Type Analysis: {question_type}
- Sequential questions need step-by-step reasoning (A->B->C)
- Parallel questions need multiple independent evidence (A->B; A->C)
- For parallel questions: Look for regions in similar spatial positions (same row/column) as they often contain related information

Previous reasoning steps:
{chr(10).join([f"Step {s['step']}: Region {s['bbox_index']} - {s.get('generated_reasoning', s['bbox_content'][:50])}..." for s in reasoning_steps])}

Available regions for this step:
{chr(10).join(available_regions)}

Task: {role_instruction}

IMPORTANT: Only use ROLE: conclusion when you have explored most regions and gathered comprehensive information.

Output format:
SELECTED_REGION: [Region X] (X must be 0-based integer: 0, 1, 2, etc.)
ROLE: [{expected_role}/evidence/conclusion]
REASONING: [Why this region is selected and how it contributes to answering the question]
RELATIONSHIP: [How this region relates to previous steps: sequential/parallel/none]
"""

            try:
                # è°ƒç”¨Qwenè¿›è¡Œå•æ­¥åˆ†æ - ä¼ é€’å¯ç”¨çš„bboxåˆ—è¡¨
                step_result = call_qwen_single_step(image_path, step_prompt, available_bbox_list)

                if step_result:
                    print(f"ğŸ¤– Qwenç¬¬{step_num}è½®è¾“å‡º:\n{step_result}")
                    # è§£æå•æ­¥ç»“æœ - ä½¿ç”¨åŸæ¥çš„æ ¼å¼
                    selected_region, role, reasoning, relationship = parse_single_step_result(step_result)

                    # ğŸ”§ ä¿®æ”¹åœæ­¢æ¡ä»¶ï¼šå°½é‡ä½¿ç”¨æ‰€æœ‰bbox
                    # è®¡ç®—å·²ä½¿ç”¨çš„åŒºåŸŸæ•°é‡
                    used_regions_count = len(used_regions)
                    total_regions = len(bbox_list)

                    # åªæœ‰åœ¨æ»¡è¶³ä»¥ä¸‹æ¡ä»¶æ—¶æ‰å…è®¸åœæ­¢ï¼š
                    # 1. è§’è‰²æ˜¯conclusionï¼Œå¹¶ä¸”
                    # 2. å·²ç»ä½¿ç”¨äº†è‡³å°‘80%çš„åŒºåŸŸï¼Œæˆ–è€…è‡³å°‘3ä¸ªåŒºåŸŸ
                    min_required_regions = max(3, int(total_regions * 0.8))

                    if role == "conclusion":
                        if used_regions_count >= min_required_regions:
                            print(f"ğŸ¯ æ£€æµ‹åˆ°conclusionè§’è‰²ä¸”å·²ä½¿ç”¨{used_regions_count}/{total_regions}ä¸ªåŒºåŸŸï¼Œç»“æŸæ¨ç†é“¾")
                            break
                        else:
                            print(f"âš ï¸ æ£€æµ‹åˆ°conclusionä½†åªä½¿ç”¨äº†{used_regions_count}/{total_regions}ä¸ªåŒºåŸŸï¼Œç»§ç»­æ¨ç†")
                            print(f"   éœ€è¦è‡³å°‘ä½¿ç”¨{min_required_regions}ä¸ªåŒºåŸŸ")
                            # å¼ºåˆ¶æ”¹ä¸ºevidenceè§’è‰²ç»§ç»­æ¨ç†
                            role = "evidence"

                    # æ£€æŸ¥åŒºåŸŸé€‰æ‹©çš„æœ‰æ•ˆæ€§
                    if selected_region is None or selected_region >= len(bbox_list):
                        print(f"âŒ æ— æœ‰æ•ˆåŒºåŸŸé€‰æ‹©: {selected_region}")
                        break

                    # ğŸ”§ é˜²æ­¢é‡å¤é€‰æ‹©åŒä¸€åŒºåŸŸ
                    if selected_region in used_regions:
                        print(f"âš ï¸ åŒºåŸŸ{selected_region}å·²è¢«ä½¿ç”¨ï¼Œå¯»æ‰¾å…¶ä»–åŒºåŸŸ")
                        # å¯»æ‰¾æœªä½¿ç”¨çš„åŒºåŸŸ
                        available_region_indices = [i for i in range(len(bbox_list)) if i not in used_regions]
                        if available_region_indices:
                            selected_region = available_region_indices[0]
                            print(f"ğŸ”§ è‡ªåŠ¨é€‰æ‹©æœªä½¿ç”¨çš„åŒºåŸŸ: {selected_region}")
                        else:
                            print("âŒ æ‰€æœ‰åŒºåŸŸéƒ½å·²ä½¿ç”¨ï¼Œç»“æŸæ¨ç†")
                            break

                    bbox_element = bbox_list[selected_region]
                    bbox_content = bbox_element.get('bbox_description', bbox_element.get('description', ''))
                    description = bbox_element.get('description', bbox_content)

                    # ğŸ”§ å¤šæ¡†æ¨ç†ï¼šä¼˜å…ˆä½¿ç”¨Qwençš„reasoningï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨contentå’Œdescription
                    if reasoning and reasoning.strip() and reasoning.strip() not in ["Analysis of selected region", "Single region analysis"]:
                        # ä½¿ç”¨Qwenç”Ÿæˆçš„æ¨ç†
                        generated_reasoning = reasoning.strip()
                        if len(generated_reasoning) > 200:  # é™åˆ¶æœ€å¤§é•¿åº¦
                            generated_reasoning = generated_reasoning[:200] + "..."
                    else:
                        # å›é€€åˆ°ä½¿ç”¨contentå’Œdescription
                        generated_reasoning = generate_content_based_reasoning(question, bbox_content, description, question_type)

                    reasoning_steps.append({
                        "step": step_num,
                        "bbox_index": selected_region,
                        "bbox_content": bbox_content,
                        "description": description,
                        "generated_reasoning": generated_reasoning,  # ä½¿ç”¨æ£€æŸ¥åçš„æ¨ç†
                        "role": role,
                        "relationship_to_previous": relationship,
                        "qwen_analysis": step_result,
                        "qwen_continue_decision": True,  # é»˜è®¤ç»§ç»­
                        "qwen_explanation": "ç»§ç»­æ¨ç†",
                        "bbox_coordinates": bbox_element.get('bbox_coordinates', bbox_element.get('bbox', []))
                    })

                    used_regions.add(selected_region)
                    current_context += f"\nStep {step_num}: Region {selected_region} - {bbox_content}"

                    print(f"âœ… é€‰æ‹©Region {selected_region}: {bbox_content} (è§’è‰²: {role})")
                    print(f"ğŸ“ æ¨ç†å†…å®¹: {generated_reasoning[:80]}...")

                    # ä¿ç•™åŸæœ‰çš„conclusionè§’è‰²åˆ¤æ–­ä½œä¸ºå¤‡ç”¨
                    if role == "conclusion":
                        print("ğŸ¯ è§’è‰²ä¸ºconclusionï¼Œç»“æŸæ¨ç†é“¾")
                        break
                else:
                    print(f"âŒ ç¬¬{step_num}è½®Qwenåˆ†æå¤±è´¥")
                    break

            except Exception as e:
                print(f"âŒ ç¬¬{step_num}è½®åˆ†æå‡ºé”™: {e}")
                break



    # ä½¿ç”¨åŸæ¥çš„æœ€ç»ˆç­”æ¡ˆï¼Œä¸é‡æ–°ç”Ÿæˆ
    if reasoning_steps:
        print(f"\nâœ… æ¨ç†é“¾ç”Ÿæˆå®Œæˆï¼Œä¿ç•™åŸæœ‰æœ€ç»ˆç­”æ¡ˆ")
        # è¿™é‡Œfinal_answerä¼šåœ¨è°ƒç”¨å‡½æ•°ä¸­ä»åŸå§‹æ•°æ®è·å–
        final_answer = None  # æ ‡è®°ä½¿ç”¨åŸæœ‰ç­”æ¡ˆ
    else:
        final_answer = "æ— æ³•ç”Ÿæˆç­”æ¡ˆ"

    # åˆ†æé—®é¢˜ç±»å‹å¹¶ç”Ÿæˆæ¨ç†é“¾æ–‡æœ¬
    question_type = analyze_question_type(question)
    chain_text_info = generate_reasoning_chain_text(reasoning_steps, question, question_type)

    # ç¡®å®šé“¾ç±»å‹
    if len(reasoning_steps) == 1:
        chain_type = "single_step"
    elif question_type == "parallel":
        chain_type = "parallel"
    elif any(step.get('relationship_to_previous') == 'sequential' for step in reasoning_steps):
        chain_type = "sequential"
    else:
        chain_type = "linear"

    return {
        "chain_type": chain_type,
        "reasoning_steps": reasoning_steps,
        "total_steps": len(reasoning_steps),
        "final_answer": final_answer,
        "keywords_used": keywords,
        "multi_round_analysis": True,
        "question_type": question_type,
        "chain_text": chain_text_info.get("chain_text", ""),
        "chain_format": chain_text_info.get("chain_format", ""),
        "reasoning_chain_description": f"Question type: {question_type}, Chain: {chain_text_info.get('chain_text', '')}"
    }

def call_qwen_single_step(image_path, prompt, available_bbox_list=None):
    """è°ƒç”¨Qwenè¿›è¡Œå•æ­¥åˆ†æ - åŒ…å«åŸå›¾å’Œbboxè£å‰ªå›¾"""
    try:
        original_image = Image.open(image_path).convert('RGB')

        # æ„å»ºæ¶ˆæ¯å†…å®¹
        content = [
            {"type": "image", "image": original_image},
            {"type": "text", "text": "Original image above. "}
        ]

        # å¦‚æœæœ‰å¯ç”¨çš„bboxåˆ—è¡¨ï¼Œæ·»åŠ è£å‰ªå›¾
        if available_bbox_list:
            content.append({"type": "text", "text": "Available regions shown below:"})

            for i, bbox in enumerate(available_bbox_list):
                # è£å‰ªbboxåŒºåŸŸ
                crop_image = crop_bbox_from_image(original_image, bbox)
                if crop_image:
                    content.append({"type": "image", "image": crop_image})
                    bbox_desc = bbox.get('bbox_description', bbox.get('description', ''))
                    content.append({"type": "text", "text": f"Region {i+1}: {bbox_desc}"})

        content.append({"type": "text", "text": f"\n{prompt}"})

        messages = [
            {
                "role": "user",
                "content": content
            }
        ]

        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )
        inputs = inputs.to(model.device)

        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=processor.tokenizer.eos_token_id
            )

        generated_ids_trimmed = []
        for in_ids, out_ids in zip(inputs.input_ids, generated_ids):
            if len(out_ids) > len(in_ids):
                generated_ids_trimmed.append(out_ids[len(in_ids):])
            else:
                generated_ids_trimmed.append(out_ids)

        if generated_ids_trimmed:
            output_text = processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )

            if output_text and len(output_text) > 0:
                return output_text[0].strip()

    except Exception as e:
        print(f"âŒ Qwenå•æ­¥è°ƒç”¨å¤±è´¥: {e}")

    return None

def parse_single_step_result(qwen_output):
    """è§£æå•æ­¥Qwenè¾“å‡º - æ”¯æŒæ–°çš„CONTINUEæ ¼å¼"""
    import re

    selected_region = None
    role = "reasoning_step"
    reasoning = ""
    relationship = "none"
    should_continue = True  # é»˜è®¤ç»§ç»­
    explanation = ""

    lines = qwen_output.split('\n')

    for line in lines:
        line = line.strip()

        # è§£ææ˜¯å¦ç»§ç»­ - æ–°å¢
        continue_patterns = [
            r'CONTINUE:\s*(YES|NO)',                  # CONTINUE: YES/NO
            r'ç»§ç»­:\s*(æ˜¯|å¦)',                       # ç»§ç»­: æ˜¯/å¦
        ]

        for pattern in continue_patterns:
            continue_match = re.search(pattern, line, re.IGNORECASE)
            if continue_match:
                continue_value = continue_match.group(1).upper()
                should_continue = continue_value in ['YES', 'æ˜¯']
                break

        # è§£æè§£é‡Š - æ–°å¢
        explanation_patterns = [
            r'EXPLANATION:\s*(.+)',                   # EXPLANATION: ...
            r'è§£é‡Š:\s*(.+)',                          # è§£é‡Š: ...
        ]

        for pattern in explanation_patterns:
            explanation_match = re.search(pattern, line, re.IGNORECASE)
            if explanation_match:
                explanation = explanation_match.group(1).strip()
                break

        # è§£æé€‰æ‹©çš„åŒºåŸŸ - æ”¯æŒNONEå’Œå„ç§æ ¼å¼
        region_patterns = [
            r'SELECTED_REGION:\s*NONE',               # SELECTED_REGION: NONE
            r'SELECTED_REGION:\s*.*?Region\s*(\d+)',  # SELECTED_REGION: Region 1
            r'SELECTED_REGION:\s*(\d+)',              # SELECTED_REGION: 1
            r'Region\s*(\d+)',                        # Region 1
            r'é€‰æ‹©.*?åŒºåŸŸ\s*(\d+)',                    # é€‰æ‹©åŒºåŸŸ1
            r'I\s+choose\s+Region\s*(\d+)',          # I choose Region 1
            r'The\s+selected\s+region\s+is\s+(\d+)', # The selected region is 1
        ]

        for pattern in region_patterns:
            if 'NONE' in pattern:
                none_match = re.search(pattern, line, re.IGNORECASE)
                if none_match:
                    selected_region = None
                    break
            else:
                region_match = re.search(pattern, line, re.IGNORECASE)
                if region_match:
                    selected_region = int(region_match.group(1))
                    break

        # è§£æè§’è‰² - æ”¯æŒå¤šç§æ ¼å¼
        role_patterns = [
            r'ROLE:\s*(.+)',                          # ROLE: keyword_match
            r'è§’è‰²:\s*(.+)',                          # è§’è‰²: å…³é”®è¯åŒ¹é…
            r'This\s+region\s+serves\s+as\s+(.+)',   # This region serves as evidence
        ]

        for pattern in role_patterns:
            role_match = re.search(pattern, line, re.IGNORECASE)
            if role_match:
                role = role_match.group(1).strip().lower()
                break

        # è§£ææ¨ç† - æ”¯æŒå¤šç§æ ¼å¼ï¼ŒåŒ…æ‹¬æ‹¼å†™é”™è¯¯
        reasoning_patterns = [
            r'REASONING:\s*(.+)',                     # REASONING: This region...
            r'REASONon:\s*(.+)',                      # REASONon: (å¤„ç†æ‹¼å†™é”™è¯¯)
            r'æ¨ç†:\s*(.+)',                          # æ¨ç†: è¿™ä¸ªåŒºåŸŸ...
            r'Because\s+(.+)',                       # Because this region contains...
            r'This\s+region\s+(.+)',                 # This region contains the keyword
        ]

        for pattern in reasoning_patterns:
            reasoning_match = re.search(pattern, line, re.IGNORECASE)
            if reasoning_match:
                reasoning = reasoning_match.group(1).strip()
                break

        # è§£æå…³ç³»
        relationship_match = re.search(r'RELATIONSHIP:\s*.*?(sequential|parallel|none)', line, re.IGNORECASE)
        if relationship_match:
            relationship = relationship_match.group(1).lower()

    # å¦‚æœæ²¡æœ‰è§£æåˆ°åŒºåŸŸä¸”åº”è¯¥ç»§ç»­ï¼Œå°è¯•ä»æ•´ä¸ªè¾“å‡ºä¸­æå–æ•°å­—
    if selected_region is None and should_continue:
        # æŸ¥æ‰¾ä»»ä½•æ•°å­—ï¼Œå¯èƒ½æ˜¯åŒºåŸŸç¼–å·
        numbers = re.findall(r'\b(\d+)\b', qwen_output)
        if numbers:
            # å–ç¬¬ä¸€ä¸ªæ•°å­—ä½œä¸ºåŒºåŸŸç¼–å·
            try:
                selected_region = int(numbers[0])
                print(f"ğŸ” ä»è¾“å‡ºä¸­æå–åˆ°åŒºåŸŸç¼–å·: {numbers[0]}")
            except:
                pass

    print(f"ğŸ” è§£æç»“æœ: åŒºåŸŸ={selected_region}, è§’è‰²={role}")
    print(f"ğŸ“ æ¨ç†: {reasoning[:80]}...")

    return selected_region, role, reasoning, relationship



def clean_text_content(text):
    """æ¸…ç†æ–‡æœ¬å†…å®¹ï¼Œå»é™¤é‡å¤å’Œé”™è¯¯çš„éƒ¨åˆ†"""
    if not text:
        return ""

    # å»é™¤å¸¸è§çš„é‡å¤æ¨¡å¼
    import re

    # å»é™¤ "This 2. This 3. This" è¿™æ ·çš„é‡å¤æ¨¡å¼
    text = re.sub(r'\s+This\s+\d+\.\s+This\s*', ' ', text)
    text = re.sub(r'\s+This\s+\d+\.\s*$', '', text)

    # å»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
    text = ' '.join(text.split())

    # å»é™¤é‡å¤çš„å¥å­ç‰‡æ®µ
    sentences = text.split('.')
    unique_sentences = []
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and sentence not in unique_sentences:
            unique_sentences.append(sentence)

    cleaned_text = '. '.join(unique_sentences)
    if cleaned_text and not cleaned_text.endswith('.'):
        cleaned_text += '.'

    return cleaned_text

def generate_reasoning_chain_text(reasoning_steps, question, chain_type="auto"):
    """ç”Ÿæˆæ¨ç†é“¾çš„æ–‡æœ¬è¡¨ç¤ºï¼Œæ”¯æŒé¡ºåºé“¾æ¡å’Œå¹¶åˆ—é“¾æ¡"""

    if not reasoning_steps:
        return {
            "chain_text": "",
            "chain_format": "empty",
            "step_count": 0,
            "question_type": chain_type
        }

    # å¦‚æœæ²¡æœ‰æŒ‡å®šé“¾ç±»å‹ï¼Œè‡ªåŠ¨åˆ¤æ–­
    if chain_type == "auto":
        chain_type = analyze_question_type(question)

    # ä¸ºæ¯ä¸ªæ­¥éª¤ç”Ÿæˆæè¿°æ–‡æœ¬
    step_descriptions = []
    for step in reasoning_steps:
        bbox_index = step.get('bbox_index', 0)

        # ä¼˜å…ˆä½¿ç”¨Qwenç”Ÿæˆçš„æ¨ç†å†…å®¹ï¼Œè€Œä¸æ˜¯åŸå§‹description
        generated_reasoning = step.get('generated_reasoning', '')
        # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¹Ÿæ£€æŸ¥æ—§çš„reasoningå­—æ®µ
        if not generated_reasoning:
            generated_reasoning = step.get('reasoning', '')

        original_description = step.get('description', '')
        bbox_content = step.get('bbox_content', '')

        # æ¸…ç†æ–‡æœ¬å†…å®¹
        if generated_reasoning and generated_reasoning not in ["Single region analysis", "Analysis of selected region"]:
            # ä½¿ç”¨Qwenç”Ÿæˆçš„æ¨ç†å†…å®¹ï¼ˆæ’é™¤é»˜è®¤çš„å ä½ç¬¦ï¼‰
            step_text = clean_text_content(generated_reasoning)
        elif original_description:
            # å›é€€åˆ°åŸå§‹æè¿°
            step_text = clean_text_content(original_description)
        elif bbox_content:
            # æœ€åå›é€€åˆ°bboxå†…å®¹
            step_text = clean_text_content(bbox_content)
        else:
            step_text = f"Region_{bbox_index}"

        # ç¡®ä¿æ–‡æœ¬ä¸ä¼šå¤ªé•¿
        if len(step_text) > 150:
            step_text = step_text[:150] + "..."

        step_descriptions.append({
            'index': bbox_index,
            'text': step_text,
            'role': step.get('role', 'reasoning_step'),
            'relationship': step.get('relationship_to_previous', 'none')
        })

    # æ ¹æ®é“¾ç±»å‹ç”Ÿæˆä¸åŒæ ¼å¼çš„æ¨ç†é“¾
    if chain_type == "sequential":
        # é¡ºåºé“¾æ¡ï¼šbbox0->bbox1->bbox2->...
        chain_text = " -> ".join([step['text'] for step in step_descriptions])
        chain_format = "sequential"

    elif chain_type == "parallel":
        # å¹¶åˆ—é“¾æ¡ï¼šbbox0->bbox1; bbox0->bbox2; ...
        if len(step_descriptions) > 1:
            root_step = step_descriptions[0]
            parallel_steps = step_descriptions[1:]

            # ç”Ÿæˆå¹¶åˆ—å…³ç³» - å¹³è¡Œé—®é¢˜ä½¿ç”¨åˆ†å·åˆ†å‰²ï¼Œä¸ä½¿ç”¨ç®­å¤´
            parallel_chains = [root_step['text']]
            for step in parallel_steps:
                parallel_chains.append(step['text'])

            chain_text = "; ".join(parallel_chains)
            chain_format = "parallel"
        else:
            chain_text = step_descriptions[0]['text']
            chain_format = "single"

    else:
        # æ··åˆæˆ–å…¶ä»–ç±»å‹ï¼Œæ ¹æ®å®é™…å…³ç³»ç”Ÿæˆ
        chain_parts = []
        current_chain = [step_descriptions[0]['text']]

        for i in range(1, len(step_descriptions)):
            step = step_descriptions[i]
            relationship = step['relationship']

            if relationship == 'sequential':
                current_chain.append(step['text'])
            elif relationship == 'parallel':
                # å¼€å§‹æ–°çš„å¹¶åˆ—åˆ†æ”¯
                if len(current_chain) > 1:
                    chain_parts.append(" -> ".join(current_chain))
                    current_chain = [step_descriptions[0]['text'], step['text']]
                else:
                    current_chain.append(step['text'])
            else:
                current_chain.append(step['text'])

        # æ·»åŠ æœ€åçš„é“¾æ¡
        if current_chain:
            chain_parts.append(" -> ".join(current_chain))

        chain_text = "; ".join(chain_parts) if len(chain_parts) > 1 else chain_parts[0] if chain_parts else ""
        chain_format = "mixed"

    return {
        "chain_text": chain_text,
        "chain_format": chain_format,
        "step_count": len(step_descriptions),
        "question_type": chain_type
    }


def select_bbox_generation_mode():
    """é€‰æ‹©bboxç”Ÿæˆæ¨¡å¼"""
    print("\nğŸ¯ é€‰æ‹©bboxç”Ÿæˆæ¨¡å¼:")
    print("  1. ä»…ç”Ÿæˆå•bboxæ¨ç†é“¾ (bbox_count == 1)")
    print("     - é€‚ç”¨äºç®€å•çš„ç›´æ¥å›ç­”é—®é¢˜")
    print("     - åªå¤„ç†åŒ…å«1ä¸ªç›¸å…³åŒºåŸŸçš„é—®é¢˜")
    print("  2. ä»…ç”Ÿæˆå¤šbboxæ¨ç†é“¾ (bbox_count > 1)")
    print("     - é€‚ç”¨äºå¤æ‚çš„å¤šæ­¥æ¨ç†é—®é¢˜")
    print("     - åªå¤„ç†åŒ…å«2ä¸ªæˆ–æ›´å¤šç›¸å…³åŒºåŸŸçš„é—®é¢˜")
    print("  3. è‡ªåŠ¨æ¨¡å¼ (å¤„ç†æ‰€æœ‰bboxæ•°é‡)")
    print("     - å¤„ç†æ‰€æœ‰ç±»å‹çš„é—®é¢˜ï¼Œä¸é™åˆ¶bboxæ•°é‡")

    while True:
        try:
            choice = input("è¯·é€‰æ‹©æ¨¡å¼ (1/2/3): ").strip()
            if choice == "1":
                print("âœ… å·²é€‰æ‹©ï¼šä»…ç”Ÿæˆå•bboxæ¨ç†é“¾")
                print("   å°†åªå¤„ç†bbox_count == 1çš„é—®é¢˜")
                return "single"
            elif choice == "2":
                print("âœ… å·²é€‰æ‹©ï¼šä»…ç”Ÿæˆå¤šbboxæ¨ç†é“¾")
                print("   å°†åªå¤„ç†bbox_count > 1çš„é—®é¢˜")
                return "multi"
            elif choice == "3":
                print("âœ… å·²é€‰æ‹©ï¼šè‡ªåŠ¨æ¨¡å¼ (å¤„ç†æ‰€æœ‰bboxæ•°é‡)")
                print("   å°†å¤„ç†æ‰€æœ‰bboxæ•°é‡çš„é—®é¢˜")
                return "auto"
            else:
                print("âŒ è¯·è¾“å…¥ 1ã€2 æˆ– 3")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            exit(0)
        except Exception as e:
            print(f"âŒ è¾“å…¥é”™è¯¯: {e}")



def get_sample_info(item, dataset_config):
    """æ ¹æ®æ•°æ®é›†é…ç½®è·å–æ ·æœ¬ä¿¡æ¯"""
    image_id_field = dataset_config['image_id_field']
    question_id_field = dataset_config['question_id_field']
    image_folder = dataset_config['image_folder']

    image_name = item[image_id_field]
    image_path = find_image_file(image_folder, image_name)

    if not image_path:
        return None

    return {
        'question_id': item[question_id_field],
        'image_name': image_name,
        'image_path': image_path,
        'question': item['question']
    }

def find_image_file(image_folder, image_name):
    """æŸ¥æ‰¾å›¾åƒæ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹æ‰©å±•åå’Œç‰¹æ®Šæ ¼å¼"""
    # å¸¸è§çš„å›¾åƒæ‰©å±•å
    extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']

    # å¦‚æœå·²ç»æœ‰æ‰©å±•åï¼Œç›´æ¥æ£€æŸ¥
    if '.' in image_name:
        image_path = os.path.join(image_folder, image_name)
        if os.path.exists(image_path):
            return image_path

    # å°è¯•ä¸åŒçš„æ‰©å±•å
    for ext in extensions:
        image_path = os.path.join(image_folder, image_name + ext)
        if os.path.exists(image_path):
            return image_path

    # ç‰¹æ®Šå¤„ç†ï¼šVQAv2/COCOæ ¼å¼ (çº¯æ•°å­—ID -> COCO_train2014_000000xxxxxx.jpg)
    if image_name.isdigit() and 'coco' in image_folder.lower():
        # è¡¥é½åˆ°12ä½æ•°å­—
        padded_id = image_name.zfill(12)
        coco_formats = [
            f"COCO_train2014_{padded_id}",
            f"COCO_val2014_{padded_id}",
            f"COCO_test2015_{padded_id}"
        ]

        for coco_name in coco_formats:
            for ext in extensions:
                image_path = os.path.join(image_folder, coco_name + ext)
                if os.path.exists(image_path):
                    return image_path

    return None

def initialize_qwen_model():
    """åˆå§‹åŒ–Qwen2-VLæ¨¡å‹"""
    global model, processor

    if model is None:
        print("ğŸš€ æ­£åœ¨åŠ è½½Qwen2-VLæ¨¡å‹...")

        # åŠ è½½å¤„ç†å™¨
        processor = AutoProcessor.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            min_pixels=224 * 28 * 28,
            max_pixels=1024 * 28 * 28
        )

        # åŠ è½½æ¨¡å‹
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.bfloat16,
            attn_implementation="eager",
            device_map="auto"  # è‡ªåŠ¨åˆ†é…åˆ°å¤šä¸ªGPU
        )

        print("âœ… Qwen2-VLæ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"ğŸ“Š æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ: {model.hf_device_map}")





def check_bbox_duplicates(bbox_list, similarity_threshold=0.9):
    """æ£€æŸ¥å¹¶å»é™¤é‡å¤çš„bbox"""
    if not bbox_list:
        return bbox_list, []

    print(f"ğŸ” æ£€æŸ¥bboxé‡å¤æ€§...")
    print(f"   åŸå§‹bboxæ•°é‡: {len(bbox_list)}")

    def calculate_bbox_similarity(bbox1, bbox2):
        """è®¡ç®—ä¸¤ä¸ªbboxçš„ç›¸ä¼¼åº¦"""
        # 1. åæ ‡ç›¸ä¼¼åº¦
        coords1 = bbox1.get('bbox_coordinates', bbox1.get('bbox', []))
        coords2 = bbox2.get('bbox_coordinates', bbox2.get('bbox', []))

        coord_similarity = 0.0
        if coords1 and coords2 and len(coords1) == 4 and len(coords2) == 4:
            # è®¡ç®—åæ ‡å·®å¼‚
            coord_diff = sum(abs(c1 - c2) for c1, c2 in zip(coords1, coords2))
            coord_similarity = max(0, 1 - coord_diff / 4.0)  # å½’ä¸€åŒ–åˆ°0-1

        # 2. å†…å®¹ç›¸ä¼¼åº¦
        content1 = bbox1.get('bbox_description', bbox1.get('description', '')).lower()
        content2 = bbox2.get('bbox_description', bbox2.get('description', '')).lower()

        content_similarity = 0.0
        if content1 and content2:
            import difflib
            content_similarity = difflib.SequenceMatcher(None, content1, content2).ratio()

        # ç»¼åˆç›¸ä¼¼åº¦ (åæ ‡æƒé‡0.6ï¼Œå†…å®¹æƒé‡0.4)
        overall_similarity = coord_similarity * 0.6 + content_similarity * 0.4
        return overall_similarity, coord_similarity, content_similarity

    unique_bbox_list = []
    duplicate_info = []

    for i, bbox in enumerate(bbox_list):
        is_duplicate = False

        for j, unique_bbox in enumerate(unique_bbox_list):
            similarity, coord_sim, content_sim = calculate_bbox_similarity(bbox, unique_bbox)

            if similarity >= similarity_threshold:
                duplicate_info.append({
                    'original_index': i,
                    'duplicate_of': j,
                    'similarity': similarity,
                    'coord_similarity': coord_sim,
                    'content_similarity': content_sim,
                    'original_content': bbox.get('bbox_description', bbox.get('description', '')),
                    'duplicate_content': unique_bbox.get('bbox_description', unique_bbox.get('description', ''))
                })
                is_duplicate = True
                break

        if not is_duplicate:
            unique_bbox_list.append(bbox)

    # æ‰“å°å»é‡ç»“æœ
    removed_count = len(bbox_list) - len(unique_bbox_list)
    print(f"   å»é‡åbboxæ•°é‡: {len(unique_bbox_list)}")
    print(f"   ç§»é™¤é‡å¤bbox: {removed_count} ä¸ª")

    if duplicate_info:
        print(f"   é‡å¤bboxè¯¦æƒ…:")
        for i, dup in enumerate(duplicate_info[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"     {i+1}. ç´¢å¼•{dup['original_index']} ä¸ ç´¢å¼•{dup['duplicate_of']} é‡å¤")
            print(f"        ç›¸ä¼¼åº¦: {dup['similarity']:.2f} (åæ ‡:{dup['coord_similarity']:.2f}, å†…å®¹:{dup['content_similarity']:.2f})")
            print(f"        å†…å®¹: '{dup['original_content'][:50]}...'")

        if len(duplicate_info) > 5:
            print(f"     ... è¿˜æœ‰ {len(duplicate_info) - 5} ä¸ªé‡å¤é¡¹")

    return unique_bbox_list, duplicate_info

def extract_bbox_content(bbox):
    """ä»bboxä¸­æå–å¯ç”¨çš„å†…å®¹æè¿°"""
    # å°è¯•å¤šä¸ªå¯èƒ½çš„æè¿°å­—æ®µ
    description_fields = [
        'bbox_description',
        'description',
        'content',
        'text',
        'ocr_text',
        'extracted_text'
    ]

    for field in description_fields:
        content = bbox.get(field, '')
        if content and len(content.strip()) >= 3:
            return content.strip()

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æè¿°ï¼Œå°è¯•ç”ŸæˆåŸºäºåæ ‡çš„æè¿°
    coords = bbox.get('bbox_coordinates', bbox.get('bbox', []))
    if coords and len(coords) == 4:
        x1, y1, x2, y2 = coords
        # ç”ŸæˆåŸºäºä½ç½®çš„æè¿°
        width = x2 - x1
        height = y2 - y1
        area = width * height

        if area > 0.5:  # å¤§åŒºåŸŸ
            return f"Large region at coordinates ({x1:.2f}, {y1:.2f}) to ({x2:.2f}, {y2:.2f})"
        elif area > 0.1:  # ä¸­ç­‰åŒºåŸŸ
            return f"Medium region at coordinates ({x1:.2f}, {y1:.2f}) to ({x2:.2f}, {y2:.2f})"
        else:  # å°åŒºåŸŸ
            return f"Small region at coordinates ({x1:.2f}, {y1:.2f}) to ({x2:.2f}, {y2:.2f})"

    return ""

def validate_bbox_data(bbox_list):
    """éªŒè¯bboxæ•°æ®çš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§ï¼Œå°è¯•ä¿®å¤ç¼ºå¤±çš„æè¿°"""
    print(f"ğŸ” éªŒè¯bboxæ•°æ®...")

    valid_bbox_list = []
    invalid_count = 0
    fixed_count = 0

    for i, bbox in enumerate(bbox_list):
        issues = []
        fixed_bbox = bbox.copy()  # åˆ›å»ºå‰¯æœ¬ä»¥ä¾¿ä¿®æ”¹

        # æ£€æŸ¥åæ ‡
        coords = bbox.get('bbox_coordinates', bbox.get('bbox', []))
        if not coords:
            issues.append("ç¼ºå°‘åæ ‡")
        elif len(coords) != 4:
            issues.append(f"åæ ‡æ ¼å¼é”™è¯¯(é•¿åº¦{len(coords)})")
        elif not all(isinstance(x, (int, float)) for x in coords):
            issues.append("åæ ‡ç±»å‹é”™è¯¯")

        # æ£€æŸ¥å¹¶å°è¯•ä¿®å¤æè¿°
        original_description = bbox.get('bbox_description', bbox.get('description', ''))

        if not original_description or len(original_description.strip()) < 3:
            # å°è¯•æå–å…¶ä»–å†…å®¹
            extracted_content = extract_bbox_content(bbox)

            if extracted_content:
                # ä¿®å¤æè¿°
                fixed_bbox['bbox_description'] = extracted_content
                fixed_bbox['description'] = extracted_content
                fixed_count += 1
                print(f"   ğŸ”§ ä¿®å¤bbox {i}æè¿°: '{extracted_content[:50]}...'")
            else:
                issues.append("æè¿°è¿‡çŸ­æˆ–ç¼ºå¤±ä¸”æ— æ³•ä¿®å¤")

        # æ£€æŸ¥åæ ‡æœ‰æ•ˆæ€§
        if coords and len(coords) == 4:
            x1, y1, x2, y2 = coords
            if x1 >= x2 or y1 >= y2:
                issues.append("åæ ‡é¡ºåºé”™è¯¯")
            if any(coord < 0 for coord in coords):
                issues.append("åæ ‡ä¸ºè´Ÿå€¼")

        # å¦‚æœåªæ˜¯æè¿°é—®é¢˜ä¸”å·²ä¿®å¤ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„
        critical_issues = [issue for issue in issues if "æè¿°" not in issue]

        if critical_issues:
            invalid_count += 1
            if invalid_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ— æ•ˆé¡¹
                print(f"   âŒ æ— æ•ˆbbox {i}: {', '.join(critical_issues)}")
                print(f"      åæ ‡: {coords}")
                print(f"      æè¿°: '{original_description[:50]}...'")
        else:
            valid_bbox_list.append(fixed_bbox)

    print(f"   æœ‰æ•ˆbbox: {len(valid_bbox_list)} ä¸ª")
    print(f"   æ— æ•ˆbbox: {invalid_count} ä¸ª")
    print(f"   ä¿®å¤æè¿°: {fixed_count} ä¸ª")

    return valid_bbox_list

def print_reasoning_chain_examples():
    """æ‰“å°æ¨ç†é“¾æ ¼å¼ç¤ºä¾‹"""
    print("\nğŸ“‹ æ¨ç†é“¾æ ¼å¼ç¤ºä¾‹:")
    print("=" * 60)

    print("\nğŸ”„ é¡ºåºé“¾æ¡ (Sequential Chain):")
    print("   é€‚ç”¨äº: æ­¥éª¤æ€§é—®é¢˜ã€æ—¶é—´é¡ºåºé—®é¢˜ã€æµç¨‹é—®é¢˜")
    print("   æ ¼å¼: [Description1]: Content1 -> [Description2]: Content2 -> [Description3]: Content3")
    print("   ç¤ºä¾‹: [Title Region]: Company Annual Report -> [Date Section]: Year 2023 -> [Financial Data]: Revenue $100M")

    print("\nğŸ”€ å¹¶åˆ—é“¾æ¡ (Parallel Chain):")
    print("   é€‚ç”¨äº: åˆ—ä¸¾é—®é¢˜ã€å¤šé€‰é¡¹é—®é¢˜ã€å¹¶åˆ—å…³ç³»é—®é¢˜")
    print("   æ ¼å¼: [Root]: Content -> [Branch1]: Content1; [Root]: Content -> [Branch2]: Content2")
    print("   ç¤ºä¾‹: [Question]: What products? -> [Product A]: Laptop; [Question]: What products? -> [Product B]: Phone")

    print("\nğŸ”— æ··åˆé“¾æ¡ (Mixed Chain):")
    print("   é€‚ç”¨äº: å¤æ‚é—®é¢˜ï¼Œæ—¢æœ‰é¡ºåºåˆæœ‰å¹¶åˆ—å…³ç³»")
    print("   æ ¼å¼: æ ¹æ®å®é™…å…³ç³»åŠ¨æ€ç”Ÿæˆ")
    print("   ç¤ºä¾‹: [Start]: Process -> [Step1]: Input; [Start]: Process -> [Step2]: Output")

    print("\nğŸ’¡ å…³é”®æ”¹è¿›:")
    print("   âœ… ç”¨ description å’Œ content æ›¿ä»£ bbox ç´¢å¼•")
    print("   âœ… æ ¹æ®é—®é¢˜ç±»å‹è‡ªåŠ¨é€‰æ‹©é“¾æ¡æ ¼å¼")
    print("   âœ… æ”¯æŒé¡ºåºæ¨ç†å’Œå¹¶åˆ—æ¨ç†")
    print("   âœ… æ›´ç›´è§‚çš„æ¨ç†é“¾è¡¨ç¤º")
    print("=" * 60)


def load_existing_results(dataset_config):
    """åŠ è½½å·²æœ‰çš„ç»“æœï¼Œå¹¶åˆ›å»ºIDåˆ°ç»“æœçš„æ˜ å°„ï¼Œè¯†åˆ«éœ€è¦é‡æ–°ç”Ÿæˆçš„å¤±è´¥æ•°æ®"""
    output_file = dataset_config['output_file']
    if os.path.exists(output_file):
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                results_list = json.load(f)

            # åˆ›å»ºIDåˆ°ç»“æœçš„æ˜ å°„ï¼Œä¾¿äºå¿«é€ŸæŸ¥æ‰¾å’Œæ›´æ–°
            results_map = {}
            failed_ids = []
            success_count = 0

            for result in results_list:
                result_id = result.get('id')
                if result_id:
                    results_map[result_id] = result

                    # æ£€æŸ¥æ˜¯å¦ä¸ºå¤±è´¥çš„æ•°æ®ï¼ˆæ¨ç†æ­¥æ•°ä¸º0æˆ–é”™è¯¯çŠ¶æ€ï¼‰
                    reasoning_chain = result.get('reasoning_chain', {})
                    reasoning_steps = reasoning_chain.get('reasoning_steps', [])
                    chain_type = reasoning_chain.get('chain_type', '')

                    if (len(reasoning_steps) == 0 or
                        chain_type == 'error' or
                        reasoning_chain.get('error')):
                        failed_ids.append(result_id)
                    else:
                        success_count += 1

            print(f"ğŸ“‚ åŠ è½½äº† {len(results_map)} ä¸ªå·²æœ‰ç»“æœ")
            print(f"   âœ… æˆåŠŸç”Ÿæˆ: {success_count} ä¸ª")
            print(f"   âŒ éœ€è¦é‡æ–°ç”Ÿæˆ: {len(failed_ids)} ä¸ª")

            if failed_ids:
                print(f"   ğŸ”„ å¤±è´¥çš„IDç¤ºä¾‹: {failed_ids[:5]}...")

            # å°†å¤±è´¥çš„IDåˆ—è¡¨æ·»åŠ åˆ°dataset_configä¸­ï¼Œä¾›åç»­ä½¿ç”¨
            dataset_config['failed_ids'] = set(failed_ids)

            return results_list, results_map
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç°æœ‰ç»“æœå¤±è´¥: {e}")
            return [], {}

    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ç©ºçš„å¤±è´¥IDé›†åˆ
    dataset_config['failed_ids'] = set()
    return [], {}


def save_results(results, dataset_config):
    """ä¿å­˜å…³ç³»åˆ†æç»“æœ"""
    output_file = dataset_config['output_file']
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

def create_ordered_results_list(bbox_data, results_map, dataset_config):
    """æ ¹æ®è¾“å…¥æ•°æ®çš„é¡ºåºåˆ›å»ºæœ‰åºçš„ç»“æœåˆ—è¡¨"""
    ordered_results = []
    for item in bbox_data:
        question_id = item.get(dataset_config['question_id_field'])
        if question_id and question_id in results_map:
            ordered_results.append(results_map[question_id])
    return ordered_results

def generate_reasoning_chains_with_bbox(dataset_config):
    """ä½¿ç”¨bboxä¿¡æ¯ç”Ÿæˆæ¨ç†é“¾"""
    print("ğŸš€ å¼€å§‹åŸºäºbboxçš„æ¨ç†é“¾æ„å»º...")

    # åˆå§‹åŒ–Qwenæ¨¡å‹
    print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–Qwenæ¨¡å‹...")
    initialize_qwen_model()

    # åŠ è½½bboxæ•°æ®
    print("ğŸ“‚ åŠ è½½bboxæ•°æ®...")
    bbox_file = dataset_config['bbox_file']

    with open(bbox_file, 'r', encoding='utf-8') as f:
        bbox_data = json.load(f)

    # å¤„ç†æ ·æœ¬æ•°é‡è®¾ç½®
    max_samples = dataset_config.get('default_max_samples', None)  # é»˜è®¤å¤„ç†æ‰€æœ‰æ ·æœ¬
    total_samples = len(bbox_data)

    if max_samples is not None and total_samples > max_samples:
        bbox_data = bbox_data[:max_samples]
        print(f"ğŸ“Š é™åˆ¶å¤„ç†å‰ {max_samples} ä¸ªé—®é¢˜ï¼ˆæ€»å…± {total_samples} ä¸ªï¼‰")
    else:
        print(f"ğŸ“Š å¤„ç†æ‰€æœ‰ {total_samples} ä¸ªé—®é¢˜")

    # åŠ è½½å·²æœ‰ç»“æœ
    _, existing_results_map = load_existing_results(dataset_config)
    processed_ids = set(existing_results_map.keys())

    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦è·³è¿‡å·²å¤„ç†çš„é—®é¢˜
    if REGENERATION_CONFIG['skip_existing'] and not REGENERATION_CONFIG['regenerate_all']:
        print(f"ğŸ“‹ å·²å¤„ç† {len(processed_ids)} ä¸ªé—®é¢˜ï¼Œè·³è¿‡é‡å¤å¤„ç†")
    else:
        print(f"ğŸ“‹ å·²æœ‰ {len(processed_ids)} ä¸ªç»“æœï¼Œæ ¹æ®é…ç½®å¯èƒ½ä¼šé‡æ–°ç”Ÿæˆ")
        if REGENERATION_CONFIG['regenerate_all']:
            print("ğŸ”„ é…ç½®ä¸ºé‡æ–°ç”Ÿæˆæ‰€æœ‰é—®é¢˜")
            processed_ids = set()  # æ¸…ç©ºå·²å¤„ç†IDï¼Œé‡æ–°ç”Ÿæˆæ‰€æœ‰
        elif REGENERATION_CONFIG['regenerate_multi_bbox'] or REGENERATION_CONFIG['regenerate_single_bbox']:
            print("ğŸ”„ é…ç½®ä¸ºé‡æ–°ç”Ÿæˆç‰¹å®šç±»å‹çš„é—®é¢˜")

    # å¤„ç†æ¯ä¸ªé—®é¢˜
    for idx, item in enumerate(bbox_data):
        question_id = item.get(dataset_config['question_id_field'])

        print(f"\n{'='*60}")
        print(f"ğŸ”„ å¤„ç†é—®é¢˜ {idx+1}/{len(bbox_data)}: {question_id}")

        # è·å–æ ·æœ¬ä¿¡æ¯
        sample_info = get_sample_info(item, dataset_config)
        if not sample_info:
            print(f"âŒ æ— æ³•è·å–æ ·æœ¬ä¿¡æ¯ï¼Œè·³è¿‡")
            continue

        print(f"â“ é—®é¢˜: {sample_info['question']}")

        # è·å–bboxåˆ†æç»“æœ
        bbox_analysis = item.get('bbox_analysis', {})
        relevant_elements = bbox_analysis.get('relevant_elements', [])

        if len(relevant_elements) < 1:
            print(f"â­ï¸ æ²¡æœ‰ç›¸å…³bboxï¼Œè·³è¿‡æ¨ç†é“¾æ„å»º")
            continue

        bbox_count = len(relevant_elements)
        print(f"ğŸ“¦ åŸå§‹ç›¸å…³bboxæ•°é‡: {bbox_count}")

        # ==================== ç”Ÿæˆæ§åˆ¶é€»è¾‘ ====================
        should_process = True
        skip_reason = ""

        # 1. æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ä¸”éœ€è¦è·³è¿‡
        is_failed_data = question_id in dataset_config.get('failed_ids', set())

        if question_id in processed_ids and REGENERATION_CONFIG['skip_existing'] and not REGENERATION_CONFIG['regenerate_all']:
            # å¦‚æœæ˜¯å¤±è´¥çš„æ•°æ®ä¸”é…ç½®ä¸ºé‡æ–°ç”Ÿæˆå¤±è´¥æ•°æ®ï¼Œå¼ºåˆ¶é‡æ–°ç”Ÿæˆ
            if is_failed_data and REGENERATION_CONFIG['regenerate_failed']:
                should_process = True
                print(f"ğŸ”„ é‡æ–°ç”Ÿæˆå¤±è´¥çš„æ•°æ®: {question_id}")
            elif not REGENERATION_CONFIG['regenerate_multi_bbox'] and not REGENERATION_CONFIG['regenerate_single_bbox']:
                should_process = False
                skip_reason = "å·²å¤„ç†ä¸”é…ç½®ä¸ºè·³è¿‡ç°æœ‰ç»“æœ"
            elif REGENERATION_CONFIG['regenerate_multi_bbox'] and bbox_count > 1:
                should_process = True
                print(f"ğŸ”„ é‡æ–°ç”Ÿæˆå¤šbboxé—®é¢˜ (bboxæ•°é‡: {bbox_count})")
            elif REGENERATION_CONFIG['regenerate_single_bbox'] and bbox_count == 1:
                should_process = True
                print(f"ğŸ”„ é‡æ–°ç”Ÿæˆå•bboxé—®é¢˜ (bboxæ•°é‡: {bbox_count})")
            else:
                should_process = False
                skip_reason = f"å·²å¤„ç†ï¼Œä¸ç¬¦åˆé‡æ–°ç”Ÿæˆæ¡ä»¶ (bboxæ•°é‡: {bbox_count})"

        # 2. æ£€æŸ¥bboxæ•°é‡é™åˆ¶
        if should_process:
            if REGENERATION_CONFIG['min_bbox_count'] and bbox_count < REGENERATION_CONFIG['min_bbox_count']:
                should_process = False
                skip_reason = f"bboxæ•°é‡ {bbox_count} å°äºæœ€å°é˜ˆå€¼ {REGENERATION_CONFIG['min_bbox_count']}"
            elif REGENERATION_CONFIG['max_bbox_count'] and bbox_count > REGENERATION_CONFIG['max_bbox_count']:
                should_process = False
                skip_reason = f"bboxæ•°é‡ {bbox_count} å¤§äºæœ€å¤§é˜ˆå€¼ {REGENERATION_CONFIG['max_bbox_count']}"
            elif REGENERATION_CONFIG['target_bbox_count'] and bbox_count != REGENERATION_CONFIG['target_bbox_count']:
                should_process = False
                skip_reason = f"bboxæ•°é‡ {bbox_count} ä¸ç­‰äºç›®æ ‡æ•°é‡ {REGENERATION_CONFIG['target_bbox_count']}"

        # 3. æ£€æŸ¥bboxç”Ÿæˆæ¨¡å¼
        if should_process:
            bbox_mode = REGENERATION_CONFIG.get('bbox_generation_mode', 'auto')
            if bbox_mode == 'single' and bbox_count != 1:
                should_process = False
                skip_reason = f"bboxç”Ÿæˆæ¨¡å¼ä¸º'single'ï¼Œä½†å½“å‰bboxæ•°é‡ä¸º {bbox_count}"
            elif bbox_mode == 'multi' and bbox_count <= 1:
                should_process = False
                skip_reason = f"bboxç”Ÿæˆæ¨¡å¼ä¸º'multi'ï¼Œä½†å½“å‰bboxæ•°é‡ä¸º {bbox_count}"
            # 'auto'æ¨¡å¼å¤„ç†æ‰€æœ‰bboxæ•°é‡

        # 4. æ£€æŸ¥ç‰¹å®šç±»å‹é‡æ–°ç”Ÿæˆ
        if should_process and not REGENERATION_CONFIG['regenerate_all']:
            if REGENERATION_CONFIG['regenerate_multi_bbox'] and bbox_count <= 1:
                should_process = False
                skip_reason = f"é…ç½®ä¸ºåªé‡æ–°ç”Ÿæˆå¤šbboxé—®é¢˜ï¼Œä½†å½“å‰bboxæ•°é‡ä¸º {bbox_count}"
            elif REGENERATION_CONFIG['regenerate_single_bbox'] and bbox_count != 1:
                should_process = False
                skip_reason = f"é…ç½®ä¸ºåªé‡æ–°ç”Ÿæˆå•bboxé—®é¢˜ï¼Œä½†å½“å‰bboxæ•°é‡ä¸º {bbox_count}"

        # å†³å®šæ˜¯å¦è·³è¿‡
        if not should_process:
            if REGENERATION_CONFIG['verbose_logging']:
                print(f"â­ï¸ è·³è¿‡: {skip_reason}")
            else:
                print(f"â­ï¸ è·³è¿‡é—®é¢˜ {question_id}")
            continue

        # å¦‚æœæ˜¯é‡æ–°ç”Ÿæˆï¼Œä»ç°æœ‰ç»“æœæ˜ å°„ä¸­ç§»é™¤
        if question_id in processed_ids:
            if question_id in existing_results_map:
                del existing_results_map[question_id]
                print(f"ğŸ—‘ï¸ ç§»é™¤ç°æœ‰ç»“æœï¼Œå‡†å¤‡é‡æ–°ç”Ÿæˆ")
        # ==================== ç”Ÿæˆæ§åˆ¶é€»è¾‘ç»“æŸ ====================

        # ğŸ” æ•°æ®æ£€æŸ¥å’Œæ¸…ç†
        print(f"ğŸ”§ å¼€å§‹æ•°æ®æ£€æŸ¥å’Œæ¸…ç†...")

        # 1. éªŒè¯bboxæ•°æ®æœ‰æ•ˆæ€§
        valid_elements = validate_bbox_data(relevant_elements)

        if len(valid_elements) < 1:
            print(f"â­ï¸ æ²¡æœ‰æœ‰æ•ˆbboxï¼Œè·³è¿‡æ¨ç†é“¾æ„å»º")
            continue

        # 2. å»é™¤é‡å¤bbox
        unique_elements, _ = check_bbox_duplicates(valid_elements, similarity_threshold=0.85)

        if len(unique_elements) < 1:
            print(f"â­ï¸ å»é‡åæ²¡æœ‰æœ‰æ•ˆbboxï¼Œè·³è¿‡æ¨ç†é“¾æ„å»º")
            continue

        print(f"âœ… æœ€ç»ˆå¯ç”¨bboxæ•°é‡: {len(unique_elements)}")

        # ç‰¹æ®Šæç¤ºï¼šå•bboxæƒ…å†µ
        if len(unique_elements) == 1:
            print(f"ğŸ“ å•bboxæ¨ç†æ¨¡å¼ - å°†è¿›è¡Œç›´æ¥åˆ†æ")

        # æ›´æ–°ç›¸å…³å…ƒç´ ä¸ºæ¸…ç†åçš„æ•°æ®
        relevant_elements = unique_elements

        # æ˜¾ç¤ºæ•°æ®æ¸…ç†ç»Ÿè®¡
        original_count = len(bbox_analysis.get('relevant_elements', []))
        final_count = len(relevant_elements)
        removed_count = original_count - final_count

        if removed_count > 0:
            print(f"ğŸ“Š æ•°æ®æ¸…ç†ç»Ÿè®¡: åŸå§‹{original_count}ä¸ª â†’ æœ€ç»ˆ{final_count}ä¸ª (ç§»é™¤{removed_count}ä¸ª)")

        # ğŸ”¥ æ ¸å¿ƒï¼šä½¿ç”¨Qwenæ„å»ºæ¨ç†é“¾
        try:
            reasoning_chain = build_reasoning_chain_with_multi_qwen(sample_info['image_path'], sample_info['question'], relevant_elements)
            print(f"âœ… æ¨ç†é“¾æ„å»ºå®Œæˆ")
            print(f"ğŸ”— é“¾ç±»å‹: {reasoning_chain['chain_type']}")
            print(f"ğŸ“Š æ¨ç†æ­¥æ•°: {reasoning_chain['total_steps']}")
            print(f"â“ é—®é¢˜ç±»å‹: {reasoning_chain.get('question_type', 'unknown')}")
            print(f"ğŸ”— æ¨ç†é“¾: {reasoning_chain.get('chain_text', '')}")

            # æ˜¾ç¤ºæ¨ç†é“¾
            for step in reasoning_chain['reasoning_steps']:
                generated_reasoning = step.get('generated_reasoning', '')
                original_description = step.get('description', step.get('bbox_content', ''))

                if generated_reasoning:
                    display_text = generated_reasoning[:60] + "..." if len(generated_reasoning) > 60 else generated_reasoning
                    print(f"   æ­¥éª¤{step['step']}: {display_text} (è§’è‰²: {step['role']})")
                else:
                    print(f"   æ­¥éª¤{step['step']}: [{original_description[:30]}...] (è§’è‰²: {step['role']})")

            if reasoning_chain.get('parallel_bbox'):
                print(f"ğŸ”€ å¹¶åˆ—å…³ç³»: {len(reasoning_chain['parallel_bbox'])} ä¸ª")

        except Exception as e:
            print(f"âŒ æ¨ç†é“¾æ„å»ºå¤±è´¥: {e}")
            reasoning_chain = {
                "chain_type": "error",
                "error": str(e),
                "reasoning_steps": []
            }

        # ä¿å­˜ç»“æœ - ç²¾ç®€ç‰ˆæœ¬ï¼Œå»é™¤é‡å¤å­—æ®µ
        result = {
            "id": question_id,
            "image": [sample_info['image_name']],
            "question": sample_info['question'],
            "reasoning_chain": reasoning_chain,
            "bbox_elements": relevant_elements,
            "ground_truth_answers": item.get('answers', []),
            # ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œç”¨äºåˆ†æï¼‰
            "stats": {
                "bbox_count": len(relevant_elements),
                "original_bbox_count": len(bbox_analysis.get('relevant_elements', [])),
                "removed_bbox_count": len(bbox_analysis.get('relevant_elements', [])) - len(relevant_elements),
                "data_cleaning_applied": True
            }
        }

        # å°†æ–°ç»“æœæ·»åŠ åˆ°æ˜ å°„ä¸­
        existing_results_map[question_id] = result

        # æ¯5ä¸ªé—®é¢˜ä¿å­˜ä¸€æ¬¡
        if len(existing_results_map) % 5 == 0:
            # åˆ›å»ºæŒ‰è¾“å…¥é¡ºåºæ’åˆ—çš„ç»“æœåˆ—è¡¨
            ordered_results = create_ordered_results_list(bbox_data, existing_results_map, dataset_config)
            save_results(ordered_results, dataset_config)
            print(f"ğŸ’¾ å·²ä¿å­˜ {len(ordered_results)} ä¸ªç»“æœ")

    # æœ€ç»ˆä¿å­˜ - ç¡®ä¿ç»“æœæŒ‰è¾“å…¥æ•°æ®é¡ºåºæ’åˆ—
    final_ordered_results = create_ordered_results_list(bbox_data, existing_results_map, dataset_config)
    save_results(final_ordered_results, dataset_config)

    # ç»Ÿè®¡æ•°æ®æ¸…ç†æ•ˆæœ
    total_original_bbox = sum(r.get('original_bbox_count', 0) for r in final_ordered_results)
    total_final_bbox = sum(r.get('bbox_count', 0) for r in final_ordered_results)
    total_removed_bbox = sum(r.get('removed_bbox_count', 0) for r in final_ordered_results)

    print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼æ€»å…±ç”Ÿæˆäº† {len(final_ordered_results)} ä¸ªæ¨ç†é“¾")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {dataset_config['output_file']}")
    print(f"\nğŸ“Š æ•°æ®æ¸…ç†æ€»ä½“ç»Ÿè®¡:")
    print(f"   åŸå§‹bboxæ€»æ•°: {total_original_bbox}")
    print(f"   æœ€ç»ˆbboxæ€»æ•°: {total_final_bbox}")
    print(f"   ç§»é™¤bboxæ€»æ•°: {total_removed_bbox}")
    if total_original_bbox > 0:
        removal_rate = (total_removed_bbox / total_original_bbox) * 100
        print(f"   ç§»é™¤ç‡: {removal_rate:.1f}%")

    return final_ordered_results

if __name__ == "__main__":
    # éªŒè¯CUDAè®¾ç½®
    print("\nğŸ”§ éªŒè¯GPUç¯å¢ƒ:")
    if torch.cuda.is_available():
        print(f"âœ… CUDAå¯ç”¨")
        print(f"ğŸ”§ å½“å‰å¯è§GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"ğŸ”§ GPU {i}: {torch.cuda.get_device_name(i)}")
            # æ˜¾ç¤ºGPUå†…å­˜ä¿¡æ¯
            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            allocated_memory = torch.cuda.memory_allocated(i) / 1024**3
            cached_memory = torch.cuda.memory_reserved(i) / 1024**3
            print(f"      æ€»å†…å­˜: {total_memory:.1f}GB, å·²åˆ†é…: {allocated_memory:.1f}GB, ç¼“å­˜: {cached_memory:.1f}GB")
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        exit(1)

    # éªŒè¯ç¯å¢ƒå˜é‡è®¾ç½®
    print("\nğŸ”§ ç¯å¢ƒå˜é‡è®¾ç½®:")
    for key in ['CUDA_LAUNCH_BLOCKING', 'TORCH_USE_CUDA_DSA', 'PYTORCH_CUDA_ALLOC_CONF', 'CUDA_VISIBLE_DEVICES', 'TOKENIZERS_PARALLELISM']:
        print(f"  {key}: {os.environ.get(key, 'æœªè®¾ç½®')}")

    # æ£€æŸ¥å¯ç”¨çš„æ•°æ®é›†æ–‡ä»¶
    print("\nğŸ“‚ æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶...")
    available_datasets = []
    for name, config in DATASETS.items():
        if os.path.exists(config['bbox_file']):
            available_datasets.append(name)

    if not available_datasets:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„bboxæ•°æ®æ–‡ä»¶")
        print("è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:")
        for name, config in DATASETS.items():
            print(f"  - {config['bbox_file']}")
        exit(1)

    print("ğŸ“‹ å¯ç”¨çš„æ•°æ®é›†:")
    for i, name in enumerate(available_datasets):
        config = DATASETS[name]
        print(f"  {i+1}. {config['name']} ({name}) - {config['bbox_file']}")

    # è®©ç”¨æˆ·é€‰æ‹©æ•°æ®é›†
    try:
        choice = input(f"\nè¯·é€‰æ‹©æ•°æ®é›† (1-{len(available_datasets)}): ").strip()
        choice_idx = int(choice) - 1

        if 0 <= choice_idx < len(available_datasets):
            dataset_name = available_datasets[choice_idx]
        else:
            print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®é›†")
            dataset_name = available_datasets[0]
    except (ValueError, KeyboardInterrupt):
        print("âŒ æ— æ•ˆè¾“å…¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®é›†")
        dataset_name = available_datasets[0]

    # é€‰æ‹©bboxç”Ÿæˆæ¨¡å¼
    bbox_mode = select_bbox_generation_mode()
    REGENERATION_CONFIG['bbox_generation_mode'] = bbox_mode

    # é…ç½®æ•°æ®é›†
    dataset_config = DATASETS[dataset_name].copy()
    # ä¿®æ”¹è¾“å‡ºæ–‡ä»¶åä¸ºæ¨ç†é“¾
    dataset_config['output_file'] = dataset_config['output_file'].replace('relations', 'reasoning_chains')
    # dataset_config['default_max_samples'] = None  # å¤„ç†æ‰€æœ‰æ ·æœ¬

    print(f"\nğŸš€ å¼€å§‹ä¸º {dataset_config['name']} æ•°æ®é›†æ„å»ºæ¨ç†é“¾...")
    print(f"ğŸ“‚ è¾“å…¥æ–‡ä»¶: {dataset_config['bbox_file']}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {dataset_config['output_file']}")

    # æ˜¾ç¤ºå½“å‰ç”Ÿæˆé…ç½®
    print(f"\nâš™ï¸ å½“å‰ç”Ÿæˆé…ç½®:")
    print(f"   ğŸ¯ bboxç”Ÿæˆæ¨¡å¼: {REGENERATION_CONFIG['bbox_generation_mode']}")
    print(f"   ğŸ”„ é‡æ–°ç”Ÿæˆæ‰€æœ‰é—®é¢˜: {REGENERATION_CONFIG['regenerate_all']}")
    print(f"   ğŸ“¦ é‡æ–°ç”Ÿæˆå¤šbboxé—®é¢˜: {REGENERATION_CONFIG['regenerate_multi_bbox']}")
    print(f"   ğŸ“ é‡æ–°ç”Ÿæˆå•bboxé—®é¢˜: {REGENERATION_CONFIG['regenerate_single_bbox']}")
    print(f"   ğŸ”§ é‡æ–°ç”Ÿæˆå¤±è´¥æ•°æ®: {REGENERATION_CONFIG['regenerate_failed']}")
    print(f"   ğŸ¯ ç›®æ ‡bboxæ•°é‡: {REGENERATION_CONFIG['target_bbox_count'] or 'æ‰€æœ‰'}")
    print(f"   ğŸ“Š bboxæ•°é‡èŒƒå›´: {REGENERATION_CONFIG['min_bbox_count']} - {REGENERATION_CONFIG['max_bbox_count'] or 'æ— é™åˆ¶'}")
    print(f"   â­ï¸ è·³è¿‡å·²æœ‰ç»“æœ: {REGENERATION_CONFIG['skip_existing']}")

    # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦ä¿®æ”¹é…ç½®
    try:
        modify_config = input(f"\næ˜¯å¦è¦ä¿®æ”¹ç”Ÿæˆé…ç½®? (y/N): ").strip().lower()
        if modify_config in ['y', 'yes']:
            print(f"\nğŸ“ é…ç½®é€‰é¡¹:")
            print(f"  1. ä¿®æ”¹bboxç”Ÿæˆæ¨¡å¼")
            print(f"  2. é‡æ–°ç”Ÿæˆæ‰€æœ‰é—®é¢˜")
            print(f"  3. é‡æ–°ç”Ÿæˆå¤šbboxé—®é¢˜ (bbox_count > 1)")
            print(f"  4. é‡æ–°ç”Ÿæˆå•bboxé—®é¢˜ (bbox_count == 1)")
            print(f"  5. åªå¤„ç†ç‰¹å®šbboxæ•°é‡çš„é—®é¢˜")
            print(f"  6. è®¾ç½®bboxæ•°é‡èŒƒå›´")
            print(f"  7. ä½¿ç”¨å½“å‰é…ç½®")

            choice = input(f"è¯·é€‰æ‹© (1-7): ").strip()

            if choice == '1':
                # ä¿®æ”¹bboxç”Ÿæˆæ¨¡å¼
                new_mode = select_bbox_generation_mode()
                REGENERATION_CONFIG['bbox_generation_mode'] = new_mode
                print(f"âœ… å·²æ›´æ–°bboxç”Ÿæˆæ¨¡å¼ä¸º: {new_mode}")
            elif choice == '2':
                REGENERATION_CONFIG['regenerate_all'] = True
                REGENERATION_CONFIG['regenerate_multi_bbox'] = False
                REGENERATION_CONFIG['regenerate_single_bbox'] = False
                REGENERATION_CONFIG['skip_existing'] = False
                print(f"âœ… é…ç½®ä¸ºé‡æ–°ç”Ÿæˆæ‰€æœ‰é—®é¢˜")
            elif choice == '3':
                REGENERATION_CONFIG['regenerate_multi_bbox'] = True
                REGENERATION_CONFIG['regenerate_single_bbox'] = False
                REGENERATION_CONFIG['regenerate_all'] = False
                print(f"âœ… é…ç½®ä¸ºé‡æ–°ç”Ÿæˆå¤šbboxé—®é¢˜")
            elif choice == '4':
                REGENERATION_CONFIG['regenerate_single_bbox'] = True
                REGENERATION_CONFIG['regenerate_multi_bbox'] = False
                REGENERATION_CONFIG['regenerate_all'] = False
                print(f"âœ… é…ç½®ä¸ºé‡æ–°ç”Ÿæˆå•bboxé—®é¢˜")
            elif choice == '5':
                target_count = input(f"è¯·è¾“å…¥ç›®æ ‡bboxæ•°é‡: ").strip()
                try:
                    REGENERATION_CONFIG['target_bbox_count'] = int(target_count)
                    print(f"âœ… é…ç½®ä¸ºåªå¤„ç†bboxæ•°é‡ä¸º {target_count} çš„é—®é¢˜")
                except ValueError:
                    print(f"âŒ æ— æ•ˆè¾“å…¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            elif choice == '6':
                min_count = input(f"è¯·è¾“å…¥æœ€å°bboxæ•°é‡ (å½“å‰: {REGENERATION_CONFIG['min_bbox_count']}): ").strip()
                max_count = input(f"è¯·è¾“å…¥æœ€å¤§bboxæ•°é‡ (å½“å‰: {REGENERATION_CONFIG['max_bbox_count'] or 'æ— é™åˆ¶'}): ").strip()
                try:
                    if min_count:
                        REGENERATION_CONFIG['min_bbox_count'] = int(min_count)
                    if max_count:
                        REGENERATION_CONFIG['max_bbox_count'] = int(max_count)
                    print(f"âœ… é…ç½®bboxæ•°é‡èŒƒå›´: {REGENERATION_CONFIG['min_bbox_count']} - {REGENERATION_CONFIG['max_bbox_count'] or 'æ— é™åˆ¶'}")
                except ValueError:
                    print(f"âŒ æ— æ•ˆè¾“å…¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            else:
                print(f"âœ… ä½¿ç”¨å½“å‰é…ç½®")
    except KeyboardInterrupt:
        print(f"\nâœ… ä½¿ç”¨å½“å‰é…ç½®")

    # æ˜¾ç¤ºæœ€ç»ˆé…ç½®
    print(f"\nğŸ¯ æœ€ç»ˆç”Ÿæˆé…ç½®:")
    print(f"   ğŸ¯ bboxç”Ÿæˆæ¨¡å¼: {REGENERATION_CONFIG['bbox_generation_mode']}")
    print(f"   ğŸ”„ é‡æ–°ç”Ÿæˆæ‰€æœ‰é—®é¢˜: {REGENERATION_CONFIG['regenerate_all']}")
    print(f"   ğŸ“¦ é‡æ–°ç”Ÿæˆå¤šbboxé—®é¢˜: {REGENERATION_CONFIG['regenerate_multi_bbox']}")
    print(f"   ğŸ“ é‡æ–°ç”Ÿæˆå•bboxé—®é¢˜: {REGENERATION_CONFIG['regenerate_single_bbox']}")
    print(f"   ğŸ”§ é‡æ–°ç”Ÿæˆå¤±è´¥æ•°æ®: {REGENERATION_CONFIG['regenerate_failed']}")
    print(f"   ğŸ¯ ç›®æ ‡bboxæ•°é‡: {REGENERATION_CONFIG['target_bbox_count'] or 'æ‰€æœ‰'}")
    print(f"   ğŸ“Š bboxæ•°é‡èŒƒå›´: {REGENERATION_CONFIG['min_bbox_count']} - {REGENERATION_CONFIG['max_bbox_count'] or 'æ— é™åˆ¶'}")

    # æ˜¾ç¤ºæ–°åŠŸèƒ½è¯´æ˜
    print_reasoning_chain_examples()

    # è¿è¡Œæ¨ç†é“¾æ„å»º
    results = generate_reasoning_chains_with_bbox(dataset_config)

    print(f"\nğŸ‰ æ¨ç†é“¾æ„å»ºå®Œæˆï¼")
    print(f"ğŸ“Š æ€»å…±å¤„ç†äº† {len(results)} ä¸ªé—®é¢˜")


