"""
ç›´æ¥ä½¿ç”¨Qwen2-VLç”ŸæˆBbox
========================
è®©Qwen2-VLç›´æ¥åˆ†æå›¾ç‰‡å’Œé—®é¢˜ï¼Œç”Ÿæˆbboxå’Œdescription
å†è®©OCRç”Ÿæˆbbox
"""

import os
import sys

def select_gpu_before_torch():
    """åœ¨å¯¼å…¥torchä¹‹å‰é€‰æ‹©GPU"""
    print("ğŸš€ Qwen2-VL Bboxç”Ÿæˆå™¨")
    print("=" * 50)

    # ä¸´æ—¶å¯¼å…¥torchæ¥æ£€æµ‹GPU
    import subprocess

    try:
        # ä½¿ç”¨nvidia-smiè·å–GPUä¿¡æ¯
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used,memory.free',
                               '--format=csv,noheader,nounits'],
                              capture_output=True, text=True, timeout=10)

        if result.returncode != 0:
            print("âŒ æ— æ³•è·å–GPUä¿¡æ¯ï¼Œå°†ä½¿ç”¨é»˜è®¤è®¾ç½®")
            return '0'

        gpu_info = result.stdout.strip().split('\n')
        gpu_count = len(gpu_info)

        if gpu_count == 1:
            print(f"ğŸ¯ åªæœ‰1ä¸ªGPUå¯ç”¨ï¼Œè‡ªåŠ¨é€‰æ‹©GPU 0")
            return '0'

        print(f"\nğŸ¯ æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUï¼Œé€‰æ‹©ä½¿ç”¨æ–¹å¼:")

        for i, info in enumerate(gpu_info):
            parts = info.split(', ')
            if len(parts) >= 4:
                name = parts[0]
                total_mb = float(parts[1])
                used_mb = float(parts[2])
                free_mb = float(parts[3])

                total_gb = total_mb / 1024
                used_gb = used_mb / 1024
                free_gb = free_mb / 1024

                print(f"   GPU {i}: {name}")
                print(f"      æ€»å†…å­˜: {total_gb:.1f}GB")
                print(f"      å·²ä½¿ç”¨: {used_gb:.1f}GB")
                print(f"      å¯ç”¨: {free_gb:.1f}GB")

                # å¦‚æœGPUä½¿ç”¨ç‡å¾ˆé«˜ï¼Œç»™å‡ºæç¤º
                usage_percent = (used_gb / total_gb) * 100
                if usage_percent > 50:
                    print(f"      âš ï¸ ä½¿ç”¨ç‡: {usage_percent:.1f}% (è¾ƒé«˜)")
                elif usage_percent > 10:
                    print(f"      ğŸ“Š ä½¿ç”¨ç‡: {usage_percent:.1f}%")
                print()

        print("é€‰æ‹©ä½¿ç”¨æ–¹å¼:")
        print("   0. ä½¿ç”¨æ‰€æœ‰GPU")
        print("   1. ä½¿ç”¨ä¸¤ä¸ªGPU (æ¨èï¼Œæ›´å¿«)")
        print("   2. ä½¿ç”¨å•ä¸ªGPU")

        while True:
            try:
                choice = input("è¯·é€‰æ‹© (0/1/2): ").strip()
                if choice == "0":
                    print(f"âœ… å·²é€‰æ‹©ä½¿ç”¨æ‰€æœ‰ {gpu_count} ä¸ªGPU")
                    return ','.join(str(i) for i in range(gpu_count))
                elif choice == "1":
                    # ä½¿ç”¨ä¸¤ä¸ªGPU
                    if gpu_count >= 2:
                        print("è¯·é€‰æ‹©è¦ä½¿ç”¨çš„ä¸¤ä¸ªGPU:")
                        selected_gpus = []

                        # é€‰æ‹©ç¬¬ä¸€ä¸ªGPU
                        while True:
                            try:
                                gpu1 = input(f"è¯·é€‰æ‹©ç¬¬ä¸€ä¸ªGPU (0-{gpu_count-1}): ").strip()
                                gpu1_id = int(gpu1)
                                if 0 <= gpu1_id < gpu_count:
                                    selected_gpus.append(gpu1_id)
                                    break
                                else:
                                    print(f"âŒ è¯·è¾“å…¥0åˆ°{gpu_count-1}ä¹‹é—´çš„æ•°å­—")
                            except ValueError:
                                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

                        # é€‰æ‹©ç¬¬äºŒä¸ªGPU
                        while True:
                            try:
                                gpu2 = input(f"è¯·é€‰æ‹©ç¬¬äºŒä¸ªGPU (0-{gpu_count-1}ï¼Œä¸èƒ½ä¸ç¬¬ä¸€ä¸ªç›¸åŒ): ").strip()
                                gpu2_id = int(gpu2)
                                if 0 <= gpu2_id < gpu_count:
                                    if gpu2_id != selected_gpus[0]:
                                        selected_gpus.append(gpu2_id)
                                        break
                                    else:
                                        print("âŒ ç¬¬äºŒä¸ªGPUä¸èƒ½ä¸ç¬¬ä¸€ä¸ªç›¸åŒ")
                                else:
                                    print(f"âŒ è¯·è¾“å…¥0åˆ°{gpu_count-1}ä¹‹é—´çš„æ•°å­—")
                            except ValueError:
                                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

                        gpu_str = ','.join(str(gpu) for gpu in selected_gpus)
                        print(f"âœ… å·²é€‰æ‹©ä½¿ç”¨GPU {selected_gpus[0]}å’Œ{selected_gpus[1]}")
                        return gpu_str
                    else:
                        print("âŒ å¯ç”¨GPUæ•°é‡ä¸è¶³2ä¸ªï¼Œè¯·é€‰æ‹©å…¶ä»–é€‰é¡¹")
                elif choice == "2":
                    while True:
                        try:
                            gpu_choice = input(f"è¯·é€‰æ‹©å•ä¸ªGPU (0-{gpu_count-1}): ").strip()
                            gpu_id = int(gpu_choice)
                            if 0 <= gpu_id < gpu_count:
                                print(f"âœ… å·²é€‰æ‹©GPU {gpu_id}")
                                return str(gpu_id)
                            else:
                                print(f"âŒ è¯·è¾“å…¥0åˆ°{gpu_count-1}ä¹‹é—´çš„æ•°å­—")
                        except ValueError:
                            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                else:
                    print("âŒ è¯·è¾“å…¥0ã€1æˆ–2")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            except KeyboardInterrupt:
                print("\nâŒ ç”¨æˆ·å–æ¶ˆ")
                sys.exit(1)

    except Exception as e:
        print(f"âŒ è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
        print("å°†ä½¿ç”¨é»˜è®¤GPU 0")
        return '0'

# åœ¨å¯¼å…¥torchä¹‹å‰é€‰æ‹©GPUå¹¶è®¾ç½®ç¯å¢ƒå˜é‡
selected_gpu = select_gpu_before_torch()

# ï¿½ğŸš¨ é‡è¦ï¼šå¿…é¡»åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®CUDAç¯å¢ƒå˜é‡
os.environ['CUDA_LAUNCH_BLOCKING'] = '0' #cuda å†…æ ¸å¼‚æ­¥å¯åŠ¨
os.environ['TORCH_USE_CUDA_DSA'] = '0' #ç¦ç”¨ CUDA çš„ Device-Side Assertions (DSA) é™ä½è¿è¡Œå¼€é”€
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True' #å…è®¸æ˜¾å­˜åŠ¨æ€æ‰©å±•ï¼Œæå‡åˆ©ç”¨ç‡
os.environ['CUDA_VISIBLE_DEVICES'] = selected_gpu  # è®¾ç½®é€‰æ‹©çš„GPU
os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # é¿å…tokenizerså¹¶è¡Œå¤„ç†è­¦å‘Š

print(f"ğŸ”§ è®¾ç½®CUDA_VISIBLE_DEVICES = {selected_gpu}")

import json
import torch
from time import sleep
from PIL import Image
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

def calculate_iou(bbox1, bbox2):
    """è®¡ç®—ä¸¤ä¸ªbboxçš„IoU (Intersection over Union)"""
    x1_1, y1_1, x2_1, y2_1 = bbox1
    x1_2, y1_2, x2_2, y2_2 = bbox2

    # è®¡ç®—äº¤é›†
    x1_inter = max(x1_1, x1_2)
    y1_inter = max(y1_1, y1_2)
    x2_inter = min(x2_1, x2_2)
    y2_inter = min(y2_1, y2_2)

    if x2_inter <= x1_inter or y2_inter <= y1_inter:
        return 0.0

    inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)

    # è®¡ç®—å¹¶é›†
    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
    union_area = area1 + area2 - inter_area

    return inter_area / union_area if union_area > 0 else 0.0

def remove_duplicate_bboxes(elements, iou_threshold=0.5):
    """å»é™¤é‡å¤çš„bbox"""
    if not elements:
        return elements

    # æŒ‰bboxé¢ç§¯æ’åºï¼Œä¿ç•™è¾ƒå¤§çš„bbox
    elements_with_area = []
    for elem in elements:
        bbox = elem.get('bbox', [])
        if len(bbox) == 4:
            area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
            elements_with_area.append((elem, area))

    # æŒ‰é¢ç§¯é™åºæ’åº
    elements_with_area.sort(key=lambda x: x[1], reverse=True)

    filtered_elements = []
    for elem, area in elements_with_area:
        bbox = elem['bbox']
        is_duplicate = False

        # æ£€æŸ¥æ˜¯å¦ä¸å·²ä¿ç•™çš„bboxé‡å¤
        for kept_elem in filtered_elements:
            kept_bbox = kept_elem['bbox']
            iou = calculate_iou(bbox, kept_bbox)
            if iou > iou_threshold:
                is_duplicate = True
                break

        if not is_duplicate:
            filtered_elements.append(elem)

    return filtered_elements

# å¸¸é‡å®šä¹‰
DISTANCE_THRESHOLD = 200  # åƒç´ è·ç¦»é˜ˆå€¼
CONFIDENCE_THRESHOLD = 0.5  # OCRç½®ä¿¡åº¦é˜ˆå€¼
MIN_WORD_LENGTH = 2  # æœ€å°è¯é•¿åº¦
TEXT_MATCH_THRESHOLD = 0.5  # æ–‡æœ¬åŒ¹é…é˜ˆå€¼
MARGIN_RATIO = 1  # åŒºåŸŸæ‰©å±•æ¯”ä¾‹



# éªŒè¯ç¯å¢ƒå˜é‡è®¾ç½®
print("ğŸ”§ ç¯å¢ƒå˜é‡è®¾ç½®:")
for key in ['CUDA_LAUNCH_BLOCKING', 'TORCH_USE_CUDA_DSA', 'PYTORCH_CUDA_ALLOC_CONF', 'TOKENIZERS_PARALLELISM']:
    print(f"  {key}: {os.environ.get(key, 'æœªè®¾ç½®')}")

# GPUé€‰æ‹©å·²åœ¨å¯¼å…¥torchä¹‹å‰å®Œæˆ

# é…ç½®è·¯å¾„ - ä¿®å¤ç›¸å¯¹è·¯å¾„é—®é¢˜
MODEL_PATH = "Qwen2-VL-7B-Instruct"

# æ•°æ®é›†é…ç½®
DATASETS = {
    # ===== Visual CoT æŒ‰æ•°æ®é›†åˆ†åˆ«å¤„ç† =====
    "viscot_flickr30k": { #2å· å…¨æ–° æš‚åœ
        "name": "Visual-CoT-Flickr30k",
        "image_folder": "playground/data/cot/flickr30k",
        "data_file": "playground/data/viscot_363k.json",
        "output_file": "images_bbox/VisCoT_flickr30k_one_agent.json",
        "image_id_field": "image",
        "question_id_field": "id",
        "dataset_filter": "flickr30k",
        "default_max_samples": None,  # 135,735æ¡
        "total_samples": 135735
    },

    "viscot_gqa": { #3å· å…¨æ–° æš‚åœ
        "name": "Visual-CoT-GQA",
        "image_folder": "playground/data/cot/gqa",
        "data_file": "playground/data/viscot_363k.json",
        "output_file": "images_bbox/VisCoT_gqa_one_agent.json",
        "image_id_field": "image",
        "question_id_field": "id",
        "dataset_filter": "gqa",
        "default_max_samples": None,  # 88,294æ¡
        "total_samples": 88294
    },

    "viscot_openimages": { # æŠ¥é”™
        "name": "Visual-CoT-OpenImages",
        "image_folder": "playground/data/cot/openimages",
        "data_file": "playground/data/viscot_363k.json",
        "output_file": "images_bbox/VisCoT_openimages_one_agent.json",
        "image_id_field": "image",
        "question_id_field": "id",
        "dataset_filter": "openimages",
        "default_max_samples": None,  # 43,053æ¡
        "total_samples": 43053
    },

    "viscot_docvqa": { # 1å· å…¨æ–° å¼€å§‹
        "name": "Visual-CoT-DocVQA",
        "image_folder": "playground/data/cot/docvqa",
        "data_file": "playground/data/viscot_363k.json",
        "output_file": "images_bbox/VisCoT_docvqa_one_agent.json",
        "image_id_field": "image",
        "question_id_field": "id",
        "dataset_filter": "docvqa",
        "default_max_samples": None,  # 33,453æ¡
        "total_samples": 33453
    },

    "viscot_textcap": {
        "name": "Visual-CoT-TextCap",
        "image_folder": "playground/data/cot/textcap",
        "data_file": "playground/data/viscot_363k.json",
        "output_file": "images_bbox/VisCoT_textcap_one_agent.json",
        "image_id_field": "image",
        "question_id_field": "id",
        "dataset_filter": "textcap",
        "default_max_samples": None,  # 32,152æ¡
        "total_samples": 32152
    },

    "viscot_v7w": { #0å· å…¨æ–° è¿è¡Œ
        "name": "Visual-CoT-Visual7W",
        "image_folder": "playground/data/cot/v7w",
        "data_file": "playground/data/viscot_363k.json",
        "output_file": "images_bbox/VisCoT_v7w_one_agent.json",
        "image_id_field": "image",
        "question_id_field": "id",
        "dataset_filter": "v7w",
        "default_max_samples": None,  # 30,491æ¡
        "total_samples": 30491
    },

    "viscot_textvqa": { #ok æ—§ç‰ˆæœ¬
        "name": "Visual-CoT-TextVQA",
        "image_folder": "playground/data/cot/textvqa",
        "data_file": "playground/data/viscot_363k.json",
        "output_file": "images_bbox/VisCoT_textvqa_one_agent.json",
        "image_id_field": "image",
        "question_id_field": "id",
        "dataset_filter": "textvqa",
        "default_max_samples": None,  # 18,524æ¡
        "total_samples": 18524
    },

    "viscot_infographicsvqa": {  #å·è·‘
        "name": "Visual-CoT-InfographicsVQA",
        "image_folder": "playground/data/cot/infographicsvqa",
        "data_file": "playground/data/viscot_363k.json",
        "output_file": "images_bbox/VisCoT_infographicsvqa_one_agent.json",
        "image_id_field": "image",
        "question_id_field": "id",
        "dataset_filter": "infographicsvqa",
        "default_max_samples": None,  # 15,055æ¡ åˆ°13090
        "total_samples": 15055
    },

    "viscot_cub": {
        "name": "Visual-CoT-CUB",
        "image_folder": "playground/data/cot/cub",
        "data_file": "playground/data/viscot_363k.json",
        "output_file": "images_bbox/VisCoT_cub_one_agent.json",
        "image_id_field": "image",
        "question_id_field": "id",
        "dataset_filter": "cub",
        "default_max_samples": None,  # 3,987æ¡
        "total_samples": 3987
    },

    "viscot_vsr": {
        "name": "Visual-CoT-VSR",
        "image_folder": "playground/data/cot/vsr",
        "data_file": "playground/data/viscot_363k.json",
        "output_file": "images_bbox/VisCoT_vsr_one_agent.json",
        "image_id_field": "image",
        "question_id_field": "id",
        "dataset_filter": "vsr",
        "default_max_samples": None,  # 3,376æ¡
        "total_samples": 3376
    },

    # ===== dataset_with_GT å¤æ‚é—®é¢˜æ•°æ®é›† =====
    "gqa_complex": { # 37593
    
        "name": "GQA-Complex",
        "image_folder": "playground/data/cot/gqa",
        "data_file": "dataset_with_GT/GQA/GQA_merged_complex_6plus.json",
        "output_file": "images_bbox/GQA_complex_one_agent.json",
        "image_id_field": "imageId",
        "question_id_field": "question_id",
        "default_max_samples": None,  # å¤„ç†å…¨éƒ¨æ•°æ®
        "total_samples": 153272,  # 153,272ä¸ªå¤æ‚é—®é¢˜
        "data_format": "gqa_complex"
    },

    "docvqa_complex": { # 13 complete
        "name": "DocVQA-Complex",
        "image_folder": "playground/data/cot/docvqa",
        "data_file": "dataset_with_GT/Docvqa/DocVQA_complex_4plus.json",
        "output_file": "images_bbox/DocVQA_complex_one_agent.json",
        "image_id_field": "imageId",
        "question_id_field": "question_id",
        "default_max_samples": None,  # å¤„ç†å…¨éƒ¨æ•°æ®
        "total_samples": 11995,  # çº¦12Kä¸ªå¤æ‚é—®é¢˜
        "data_format": "docvqa_complex"
    },

    "infovqa_complex": { # 13 # 21668
        "name": "InfoVQA-Complex",
        "image_folder": "playground/data/cot/infographicsvqa",
        "data_file": "dataset_with_GT/InfoVQA/InfoVQA_complex_4plus_parallel.json",
        "output_file": "images_bbox/InfoVQA_complex_one_agent.json",
        "image_id_field": "imageId",
        "question_id_field": "question_id",
        "default_max_samples": None,  # å¤„ç†å…¨éƒ¨æ•°æ®
        "total_samples": 22331,  # 22,331ä¸ªå¤æ‚é—®é¢˜
        "data_format": "infovqa_complex"
    },

    "textvqa_complex": { # complete
        "name": "TextVQA-Complex",
        "image_folder": "playground/data/cot/textvqa",
        "data_file": "dataset_with_GT/TextVQA/TextVQA_complex_3plus_parallel.json",
        "output_file": "images_bbox/TextVQA_complex_one_agent.json",
        "image_id_field": "imageId",
        "question_id_field": "question_id",
        "default_max_samples": None,  # å¤„ç†å…¨éƒ¨æ•°æ®
        "total_samples": 12508,  # 12,508ä¸ªå¤æ‚é—®é¢˜
        "data_format": "textvqa_complex"
    },

    "visual7w_complex": {# complete
        "name": "Visual7W-Complex",
        "image_folder": "playground/data/cot/v7w",
        "data_file": "dataset_with_GT/Visual7W/Visual7W_complex_3plus_parallel.json",
        "output_file": "images_bbox/Visual7W_complex_one_agent.json",
        "image_id_field": "imageId",
        "question_id_field": "question_id",
        "default_max_samples": None,  # å¤„ç†å…¨éƒ¨æ•°æ®
        "total_samples": 17954,  # 17,954ä¸ªå¤æ‚é—®é¢˜
        "data_format": "visual7w_complex"
    },

    "vqav2_complex": { # complete 
        "name": "VQAv2-Complex",
        "image_folder": "playground/data/cot/coco",  # VQAv2ä½¿ç”¨COCOå›¾åƒ
        "data_file": "dataset_with_GT/VQAv2/VQAv2_complex_5plus_parallel.json",
        "output_file": "images_bbox/VQAv2_complex_one_agent.json",
        "image_id_field": "imageId",
        "question_id_field": "question_id",
        "default_max_samples": None,  # å¤„ç†å…¨éƒ¨æ•°æ®
        "total_samples": 35383,  # 35,383ä¸ªå¤æ‚é—®é¢˜
        "data_format": "vqav2_complex"
    }

}
# å…¨å±€å˜é‡
model = None
processor = None

# åˆå§‹åŒ–qwen
def initialize_qwen_model():
    """åˆå§‹åŒ–Qwen2-VLæ¨¡å‹"""
    global model, processor

    if model is None:
        print("ğŸš€ æ­£åœ¨åŠ è½½Qwen2-VLæ¨¡å‹...")

        # åŠ è½½å¤„ç†å™¨ - ä½¿ç”¨ä¸debug_qwen.pyç›¸åŒçš„ç®€å•é…ç½®
        processor = AutoProcessor.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True
        )

        # åŠ è½½æ¨¡å‹ - ä½¿ç”¨å¤šGPUåŠ é€Ÿ
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,  # åŠç²¾åº¦æµ®ç‚¹ï¼Œfloat16
            device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°å¤šä¸ªGPU
            trust_remote_code=True, #å…è®¸åŠ è½½è‡ªå®šä¹‰ä»£ç 
            low_cpu_mem_usage=True #ä¼˜åŒ–CPUçš„å†…å­˜ä½¿ç”¨
        )

        print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU {os.environ.get('CUDA_VISIBLE_DEVICES', 'unknown')}")
        # æ˜¾ç¤ºæ¨¡å‹åˆ†å¸ƒæƒ…å†µ
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ŒGPUåˆ†å¸ƒæƒ…å†µ:")
        device_map = {}
        dtype_info = {}
        for _, param in model.named_parameters():
            device = str(param.device)
            dtype = str(param.dtype)

            #ç»Ÿè®¡å„è®¾å¤‡ä¸Šçš„å‚æ•°æ•°é‡
            if device not in device_map:
                device_map[device] = 0
            device_map[device] += param.numel()

            #ç»Ÿè®¡å„ç²¾åº¦çš„å‚æ•°æ•°é‡
            if dtype not in dtype_info:
                dtype_info[dtype] = 0
            dtype_info[dtype] += param.numel()

        for device, param_count in device_map.items():
            print(f"   {device}: {param_count:,} å‚æ•°")

        print("ğŸ“Š æ¨¡å‹ç²¾åº¦åˆ†å¸ƒ:")
        for dtype, param_count in dtype_info.items():
            print(f"   {dtype}: {param_count:,} å‚æ•°")

        # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            for i in range(min(2, torch.cuda.device_count())):  # æ˜¾ç¤º2ä¸ªGPU
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                cached = torch.cuda.memory_reserved(i) / 1024**3
                print(f"   GPU {i}: å·²åˆ†é… {allocated:.1f}GB, ç¼“å­˜ {cached:.1f}GB")

# ç§»é™¤è¯¦ç»†çš„å†…å­˜ä¿¡æ¯æ‰“å°ä»¥æé«˜é€Ÿåº¦

# æ¸…ç†GPU
def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if torch.cuda.is_available():
        for i in range(min(2, torch.cuda.device_count())):  # æ¸…ç†2ä¸ªGPU
            torch.cuda.empty_cache()
            torch.cuda.synchronize(device=i)

# é‡ç½®æ¨¡å‹çŠ¶æ€
def reset_model_state():
    """é‡ç½®æ¨¡å‹çŠ¶æ€"""
    global model, processor
    try:
        print("ğŸ”„ é‡ç½®æ¨¡å‹çŠ¶æ€...")
        # å½»åº•æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()
        for i in range(min(2, torch.cuda.device_count())):
            torch.cuda.synchronize(device=i)

        # é‡ç½®æ¨¡å‹çŠ¶æ€
        if hasattr(model, 'eval'):
            model.eval()

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½
        try:
            # å°è¯•ç®€å•æ“ä½œæ£€æŸ¥æ¨¡å‹çŠ¶æ€
            next(model.parameters()).device
            print("âœ… æ¨¡å‹çŠ¶æ€é‡ç½®å®Œæˆ")
        except Exception:
            print("âš ï¸ æ¨¡å‹çŠ¶æ€å¼‚å¸¸ï¼Œå°è¯•é‡æ–°åŠ è½½...")
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹
            model = None
            processor = None
            initialize_qwen_model()
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹çŠ¶æ€é‡ç½®å¤±è´¥: {e}")

# è·å–GPUå¯ç”¨memeory
def get_available_gpu_memory():
    """è·å–å¯ç”¨GPUå†…å­˜ï¼ˆGBï¼‰"""
    if not torch.cuda.is_available():
        return 0

    min_free_memory = float('inf')
    for i in range(min(2, torch.cuda.device_count())):
        total = torch.cuda.get_device_properties(i).total_memory / 1024**3
        cached = torch.cuda.memory_reserved(i) / 1024**3
        free = total - cached
        min_free_memory = min(min_free_memory, free)

    return min_free_memory

# æ ¹æ®GPUå†…å­˜è‡ªé€‚åº”è°ƒæ•´å›¾åƒå°ºå¯¸é™åˆ¶
def adaptive_image_size(_, base_max_size=1500):
    """æ ¹æ®GPUå†…å­˜è‡ªé€‚åº”è°ƒæ•´å›¾åƒå°ºå¯¸é™åˆ¶"""
    available_memory = get_available_gpu_memory()

    if available_memory > 10:  # å……è¶³å†…å­˜ï¼ˆè¿˜æœ‰10Gï¼‰
        max_size = base_max_size
    elif available_memory > 8:  # ä¸­ç­‰å†…å­˜
        max_size = int(base_max_size * 0.8)  # 1200
    elif available_memory > 6:  # è¾ƒå°‘å†…å­˜
        max_size = int(base_max_size * 0.6)  # 900
    else:  # å†…å­˜ç´§å¼ 
        max_size = int(base_max_size * 0.4)  # 600

    print(f"ğŸ§  å¯ç”¨GPUå†…å­˜: {available_memory:.1f}GB, å›¾åƒå°ºå¯¸é™åˆ¶: {max_size}px")
    return max_size

# sub-è¾“å…¥å¯¹è¯+å›¾ç‰‡ï¼Œå¦‚æœæ— æ³•æ­£å¸¸è¿è¡Œåˆ™ç¼©å‡token
def generate_qwen_response(messages, max_tokens=512):
    """ç”ŸæˆQwen2-VLå“åº” - å¿«é€Ÿç‰ˆæœ¬ï¼Œä¸æ¸…ç†å†…å­˜"""
    global model, processor

    try:
        # å‡†å¤‡è¾“å…¥
        text = processor.apply_chat_template(
            messages, # è¾“å…¥
            tokenize=False, # æ˜¯å¦åœ¨åº”ç”¨èŠå¤©æ¨¡æ¿åç«‹å³å¯¹æ–‡æœ¬è¿›è¡Œ åˆ†è¯ï¼ˆtokenizationï¼‰
            add_generation_prompt=True # æ·»åŠ ç”Ÿæˆæç¤ºç¬¦ï¼ˆå¦‚ "<|assistant|>"ï¼‰
        )
        image_inputs,video_inputs = process_vision_info(messages)

        inputs = processor(
            text=[text], #æ–‡æœ¬
            images=image_inputs, #å›¾åƒè¾“å…¥
            video=video_inputs,
            padding=True, #è‡ªåŠ¨å¡«å……åˆ°ç›¸åŒé•¿åº¦
            return_tensors="pt", #è¿”å›pytorchå¼ é‡
        )

        # æŠŠè¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
        model_device = next(model.parameters()).device
        inputs = inputs.to(model_device)

        # ç”Ÿæˆå“åº” - ä½¿ç”¨è‡ªé€‚åº”tokenç­–ç•¥
        with torch.no_grad():
            # æ¸…ç†GPUç¼“å­˜é¿å…æ•°å€¼é—®é¢˜
            torch.cuda.empty_cache()

            # è‡ªé€‚åº”è°ƒæ•´tokenæ•°é‡
            available_memory = get_available_gpu_memory()
            if available_memory < 5:  # å†…å­˜ä¸è¶³æ—¶å‡å°‘token
                actual_tokens = min(max_tokens, 512)
                print(f"âš ï¸ GPUå†…å­˜ä¸è¶³({available_memory:.1f}GB)ï¼Œå‡å°‘tokenåˆ°{actual_tokens}")
            else:
                actual_tokens = max_tokens

            print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆï¼Œmax_tokens={actual_tokens}")

        # ç”Ÿæˆå“åº” 
        with torch.no_grad():
            # æ¸…ç†GPUç¼“å­˜é¿å…æ•°å€¼é—®é¢˜ï¼ˆcopyç‰ˆæœ¬çš„å…³é”®æ­¥éª¤ï¼‰
            torch.cuda.empty_cache()

            # ä½¿ç”¨æœ€ä¿å®ˆçš„ç”Ÿæˆå‚æ•°é¿å…æ•°å€¼ç¨³å®šæ€§é—®é¢˜
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=actual_tokens,  # ä½¿ç”¨ä¸Šé¢è°ƒæ•´åçš„token
                do_sample=False,  # ä½¿ç”¨è´ªå¿ƒè§£ç ï¼ˆæ¯æ¬¡é€‰æ¦‚ç‡æœ€é«˜çš„tokenï¼‰ï¼Œä¿è¯ç¨³å®šç”Ÿæˆ
                temperature=1.0,  # ä¸ç¼©æ”¾logitsï¼ŒAIå®Œå…¨æŒ‰å­¦ä¹ åˆ°çš„æ¦‚ç‡é€‰æ‹©ä¸‹ä¸€ä¸ªè¯ï¼ˆæœ€æ¥è¿‘è®­ç»ƒæ•°æ®é£æ ¼ï¼‰
                pad_token_id=processor.tokenizer.eos_token_id, # ç©ºæ ¼
                eos_token_id=processor.tokenizer.eos_token_id, # ç»“æŸç¬¦
                repetition_penalty=1.0,  # ä½¿ç”¨é»˜è®¤å€¼ï¼Œä¸ä¼šä¸»åŠ¨é¿å…é‡å¤ 
                use_cache=True,  # å¯ç”¨ç¼“å­˜æé«˜ç¨³å®šæ€§
                output_scores=False,  # ç¦ç”¨åˆ†æ•°è¾“å‡º
                output_attentions=False,  # ç¦ç”¨æ³¨æ„åŠ›è¾“å‡º
                output_hidden_states=False  # ç¦ç”¨éšè—çŠ¶æ€è¾“å‡º
            )
            print(f"âœ… ç”Ÿæˆå®Œæˆ")

        # è§£ç è¾“å‡º
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )

        return output_text[0].strip() if output_text else None

    except torch.cuda.OutOfMemoryError as e:
        print(f"âŒ GPUå†…å­˜ä¸è¶³: {e}")
        torch.cuda.empty_cache()
        return None
    except RuntimeError as e:
        # å¦‚æœåˆå§‹è®¾ç½®æ— æ³•ç”Ÿæˆï¼Œå°è¯•ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°é‡æ–°ç”Ÿæˆ
        if "probability tensor contains either `inf`, `nan` or element < 0" in str(e):
            print(f"âŒ æ•°å€¼ç¨³å®šæ€§é”™è¯¯: {e}")
            print("ğŸ”„ å°è¯•ä½¿ç”¨æ›´ä¿å®ˆçš„ç”Ÿæˆå‚æ•°...")

            # å°è¯•ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°é‡æ–°ç”Ÿæˆ
            try:
                torch.cuda.empty_cache()  # æ¸…ç†ç¼“å­˜

                # é‡æ–°å‡†å¤‡è¾“å…¥ï¼ˆå¯èƒ½æœ‰åŠ©äºè§£å†³æ•°å€¼é—®é¢˜ï¼‰
                inputs = processor(
                    text=[messages],
                    images=[messages[0]["content"][0]["image"]],
                    padding=True,
                    return_tensors="pt"
                ).to(model.device)

                with torch.no_grad():
                    generated_ids = model.generate(
                        **inputs,
                        max_new_tokens=64,  # å¤§å¹…å‡å°‘tokenæ•°é‡
                        do_sample=False,  # ç¡®å®šæ€§ç”Ÿæˆ
                        temperature=1.0,
                        top_k=50,  # é™åˆ¶é‡‡æ ·èŒƒå›´
                        pad_token_id=processor.tokenizer.eos_token_id,
                        eos_token_id=processor.tokenizer.eos_token_id,
                        repetition_penalty=1.0,
                        use_cache=False,  # ç¦ç”¨ç¼“å­˜
                        output_attentions=False,
                        output_hidden_states=False
                    )

                # è§£ç è¾“å‡º
                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                output_text = processor.batch_decode(
                    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                )

                print("âœ… ä¿å®ˆå‚æ•°ç”ŸæˆæˆåŠŸ")
                return output_text[0].strip() if output_text else None

            except Exception as e2:
                print(f"âŒ ä¿å®ˆå‚æ•°ç”Ÿæˆä¹Ÿå¤±è´¥: {e2}")
                return None
        else:
            print(f"âŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
            return None
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {e}")
        print(f"ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        print(f"ğŸ” è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None

# layer1æˆ–2-ç®€åŒ–ç‰ˆæœ¬ï¼šç»Ÿä¸€æç¤ºè¯ï¼Œæ ¹æ®OCRå¢å¼ºç»“æœåˆ†å±‚
# ========================================
# ğŸ¯ ç®€åŒ–åçš„æ–°æ¶æ„è¯´æ˜
# ========================================
# Layer 1/2: ç»Ÿä¸€çš„Qwen2-VLæ–¹æ³•ï¼ˆç®€åŒ–ä¸º2ç§ç­–ç•¥ï¼‰
#   1 - ç»Ÿä¸€æç¤ºè¯ + OCRå¢å¼ºæˆåŠŸ
#   2 - ç»Ÿä¸€æç¤ºè¯ + OCRå¢å¼ºå¤±è´¥
# Layer 3: OCR + å…³é”®è¯åŒ¹é…
# Layer 4: çº¯ç®—æ³•ä¿åº•
# ========================================
def generate_bboxes_for_question(image_path, question):
    """ç»Ÿä¸€çš„bboxç”Ÿæˆæ–¹æ³•ï¼šæ™ºèƒ½é€‰æ‹©OCRå¢å¼ºï¼ŒåŒ…å«å¤šç§Qwen2-VLç­–ç•¥"""
    try:
        image = Image.open(image_path).convert('RGB')
        original_size = image.size
        print(f"ğŸ” åŸå§‹å›¾åƒå°ºå¯¸: {image.size}")

        # æ£€æŸ¥å›¾åƒå°ºå¯¸ï¼Œå¦‚æœä»»ä¸€è¾¹è¶…è¿‡3000åˆ™ç¼©æ”¾
        max_dimension = max(image.size)
        if max_dimension > 2000:
            scale_ratio = round(2000 / max_dimension, 3)
            new_width = int(image.size[0] * scale_ratio)
            new_height = int(image.size[1] * scale_ratio)
            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            print(f"ğŸ“ å›¾åƒç¼©æ”¾: {original_size} -> {image.size} (scale: {scale_ratio:.3f})")
        else:
            print(f"ğŸ“ ä½¿ç”¨åŸå§‹å›¾åƒå°ºå¯¸: {image.size}")
            scale_ratio = 1.0

        # ä¿å­˜å›¾åƒå°ºå¯¸ä¿¡æ¯ç”¨äºå½’ä¸€åŒ–åæ ‡
        image._original_size = original_size
        image._scale_ratio = scale_ratio

        # ğŸ¯ ç®€åŒ–ç­–ç•¥ï¼šç»Ÿä¸€çš„æç¤ºè¯ï¼Œæ ¹æ®OCRå¢å¼ºç»“æœåˆ†å±‚
        prompt = f"""Question: {question}


        
Analyze the image carefully and identify ALL visual elements that could help answer this question. Look for multiple relevant items including text, objects, numbers, signs, labels, etc.

For each relevant element you find, provide:
1. The reason for selecting this bbox
2. How its content relates to the question
3. Bounding box coordinates
4. A brief description (1-3 words) of what the element contains

Return ONLY this JSON format:

{{
  "relevant_elements": [
    {{
      "description": "brief description (1-3 words)",
      "selection_reason": "why this bbox was selected",
      "content_relation": "how the content in this bbox relates to the question",
      "bbox": [x1, y1, x2, y2]
    }},
    ...,
    {{
      "description": "brief description (1-3 words)",
      "selection_reason": "why this bbox was selected",
      "content_relation": "how the content in this bbox relates to the question",
      "bbox": [x1, y1, x2, y2]
    }}
  ]
}}

IMPORTANT:
- Each region contains only ONE element (text, number, icon, object, etc.)
- Look for multiple pieces of evidence that support the answer
- Include both primary and supporting visual elements
- Coordinates are normalized 0-1 format
Return JSON only."""

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ],
            }
        ]

        print("ğŸ”„ ä½¿ç”¨ç»Ÿä¸€æç¤ºè¯ç”Ÿæˆbbox...")
        response = generate_qwen_response(messages, max_tokens=512)
        sleep(2)

        if response:
            print(f"ğŸ” æ¨¡å‹åŸå§‹å“åº”é•¿åº¦: {len(response)}")
            print(f"ğŸ” æ¨¡å‹åŸå§‹å“åº”: {response}")

            # æ¸…ç†å“åº”æå–JSON
            response = response.strip()
            if response.startswith('```json'):
                response = response[7:]
            elif response.startswith('```'):
                response = response[3:]
            if response.endswith('```'):
                response = response[:-3]
            response = response.strip()

            print(f"ğŸ” æ¸…ç†åå“åº”é•¿åº¦: {len(response)}")
            print(f"ğŸ” æ¸…ç†åå“åº”: {response}")

            # å°è¯•è§£æJSON
            result = None
            try:
                result = json.loads(response)
                print(f"âœ… JSONè§£ææˆåŠŸ")
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                # å°è¯•ä¿®å¤JSONæ ¼å¼
                print("ğŸ”§ å°è¯•ä¿®å¤JSONæ ¼å¼...")
                try:
                    fixed_response = fix_json_format(response)
                    result = json.loads(fixed_response)
                    print(f"âœ… JSONä¿®å¤æˆåŠŸ")
                except json.JSONDecodeError:
                    print(f"âŒ JSONä¿®å¤ä¹Ÿå¤±è´¥ï¼Œé™çº§åˆ°OCRæ–¹æ¡ˆ...")
                    return generate_emergency_bboxes(question, image_path, scale_ratio)

            if result:
                # å¤„ç†ç»“æœæ ¼å¼ï¼Œä¼ å…¥å›¾åƒå°ºå¯¸ç”¨äºåæ ‡å½’ä¸€åŒ–å’Œé—®é¢˜ç”¨äºå…³é”®è¯æ£€æŸ¥
                result = process_qwen_result(result, "bbox", image.size[0], image.size[1], question)

                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„å…ƒç´ 
                if not result.get('relevant_elements'):
                    print("âŒ Qwen2-VLæ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„bboxï¼Œé™çº§åˆ°Layer 3...")
                    return generate_emergency_bboxes(question, image_path, scale_ratio)

                # ğŸ¯ å…³é”®ï¼šå¯¹æ‰€æœ‰æˆåŠŸçš„ç»“æœéƒ½å°è¯•OCRå¢å¼º
                print("ğŸ”§ å¯ç”¨æ··åˆæ–¹æ¡ˆï¼šä½¿ç”¨OCRç²¾ç¡®åŒ–bboxä½ç½®...")

                # ä¸ºæ¯ä¸ªå…ƒç´ æ·»åŠ rough_bboxä¿¡æ¯ï¼ˆä½¿ç”¨åŸå§‹çš„bboxä½œä¸ºç²—ç•¥åŒºåŸŸï¼‰
                elements_with_rough_bbox = []
                for element in result['relevant_elements']:
                    if isinstance(element, dict) and 'bbox' in element:
                        element_copy = element.copy()
                        element_copy['rough_bbox'] = element['bbox']  # ä½¿ç”¨åŸå§‹bboxä½œä¸ºç²—ç•¥åŒºåŸŸ
                        elements_with_rough_bbox.append(element_copy)
                    else:
                        elements_with_rough_bbox.append(element)

                enhanced_elements = match_content_with_ocr(elements_with_rough_bbox, image_path, question)

                # æ£€æŸ¥OCRå¢å¼ºæ˜¯å¦æˆåŠŸï¼šçœ‹ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦æœ‰match_info
                ocr_enhanced_success = False
                if enhanced_elements:
                    for element in enhanced_elements:
                        if isinstance(element, dict) and 'match_info' in element:
                            ocr_enhanced_success = True
                            break

                if ocr_enhanced_success:
                    # OCRå¢å¼ºæˆåŠŸ - Layer 1 (æœ€é«˜è´¨é‡)
                    # å»é™¤é‡å¤çš„bboxï¼Œä½†ä¿ç•™rough_bboxå­—æ®µ
                    enhanced_elements = remove_duplicate_bboxes(enhanced_elements, iou_threshold=0.5)
                    result['relevant_elements'] = enhanced_elements
                    result["generation_method"] = "hybrid_qwen2vl_ocr"
                    result["generation_layer"] = 1
                    result["generation_description"] = "Generated by hybrid method: Qwen2-VL + OCR precise localization"
                    print(f"âœ… OCRå¢å¼ºæˆåŠŸï¼š{len(enhanced_elements)} ä¸ªç²¾ç¡®bbox (Layer 1, å»é‡å)")
                    return result
                else:
                    # OCRå¢å¼ºå¤±è´¥ï¼Œä½†Qwen2-VLç»“æœå¯ç”¨ - Layer 2 (ä¸­ç­‰è´¨é‡)
                    # ä¿ç•™rough_bboxå­—æ®µï¼Œä½¿ç”¨åŸå§‹ç»“æœ
                    clean_elements = []
                    for element in result['relevant_elements']:
                        if isinstance(element, dict):
                            # ä¿ç•™rough_bboxå­—æ®µï¼Œä¸åˆ é™¤
                            clean_elements.append(element)
                        else:
                            clean_elements.append(element)

                    # å»é™¤é‡å¤çš„bbox
                    clean_elements = remove_duplicate_bboxes(clean_elements, iou_threshold=0.5)
                    result['relevant_elements'] = clean_elements
                    result["generation_method"] = "qwen2vl_only"
                    result["generation_layer"] = 2
                    result["generation_description"] = "Generated by Qwen2-VL only (OCR enhancement failed)"
                    print(f"âœ… Qwen2-VLæˆåŠŸï¼ˆOCRå¢å¼ºå¤±è´¥ï¼‰ï¼š{len(result['relevant_elements'])} ä¸ªbbox (Layer 2, å»é‡å)")
                    return result
        else:
            print("âŒ Qwen2-VLæ— å“åº”ï¼Œé™çº§åˆ°OCRæ–¹æ¡ˆ...")
            return generate_emergency_bboxes(question, image_path, scale_ratio)

        # å¦‚æœåˆ°è¿™é‡Œè¯´æ˜å‡ºç°äº†æ„å¤–æƒ…å†µï¼Œä½¿ç”¨ä¿åº•æ–¹æ¡ˆ
        print("ğŸ”„ æ„å¤–æƒ…å†µï¼Œä½¿ç”¨ä¿åº•æ–¹æ¡ˆ...")
        return generate_emergency_bboxes(question, image_path, scale_ratio)

    except Exception as e:
        print(f"âŒ ç”Ÿæˆbboxæ—¶å‡ºé”™: {e}")
        print("ğŸ”„ ä½¿ç”¨æœ€åä¿åº•æ–¹æ¡ˆ...")
        return generate_emergency_bboxes(question, image_path, 1.0)

def extract_question_keywords(question):
    """ä»é—®é¢˜ä¸­æå–å…³é”®è¯"""
    import re

    # è½¬æ¢ä¸ºå°å†™
    question = question.lower()

    # ç§»é™¤å¸¸è§çš„ç–‘é—®è¯å’Œåœç”¨è¯
    stop_words = {
        'what', 'where', 'when', 'why', 'how', 'who', 'which', 'is', 'are', 'was', 'were',
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
        'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after',
        'above', 'below', 'between', 'among', 'this', 'that', 'these', 'those', 'can',
        'could', 'should', 'would', 'will', 'shall', 'may', 'might', 'must', 'do', 'does',
        'did', 'have', 'has', 'had', 'be', 'been', 'being', 'you', 'i', 'we', 'they', 'it'
    }

    # æå–å•è¯
    words = re.findall(r'\b[a-zA-Z]+\b', question)

    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
    keywords = [word for word in words if word not in stop_words and len(word) > 2]

    return keywords

def check_content_relevance(content_relation, question_keywords, min_keyword_match=1):
    """æ£€æŸ¥content_relationæ˜¯å¦åŒ…å«é—®é¢˜çš„å…³é”®è¯"""
    if not content_relation or not question_keywords:
        return False

    content_lower = content_relation.lower()

    # è®¡ç®—åŒ¹é…çš„å…³é”®è¯æ•°é‡
    matched_keywords = 0
    for keyword in question_keywords:
        if keyword in content_lower:
            matched_keywords += 1

    return matched_keywords >= min_keyword_match

def process_qwen_result(result, bbox_field, image_width=None, image_height=None, question=None):
    """å¤„ç†Qwen2-VLçš„ç»“æœæ ¼å¼"""
    # æ£€æŸ¥ç»“æœæ ¼å¼
    if isinstance(result, list):
        # å¦‚æœè¿”å›çš„æ˜¯æ•°ç»„ï¼Œè½¬æ¢ä¸ºæœŸæœ›çš„å­—å…¸æ ¼å¼
        print("ğŸ”§ æ£€æµ‹åˆ°æ•°ç»„æ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼")
        result = {"relevant_elements": result}
    elif not isinstance(result, dict):
        # å¦‚æœæ—¢ä¸æ˜¯å­—å…¸ä¹Ÿä¸æ˜¯æ•°ç»„ï¼Œåˆ›å»ºç©ºç»“æœ
        print("âš ï¸ æ£€æµ‹åˆ°éé¢„æœŸæ ¼å¼ï¼Œåˆ›å»ºç©ºç»“æœ")
        result = {"relevant_elements": []}

    # éªŒè¯å’Œæ¸…ç†ç»“æœ
    if result.get('relevant_elements'):
        print(f"ğŸ” åŸå§‹å…ƒç´ æ•°é‡: {len(result['relevant_elements'])}")
        valid_elements = []
        for i, element in enumerate(result['relevant_elements']):
            print(f"ğŸ” æ£€æŸ¥å…ƒç´  {i}: {element}")

            if isinstance(element, dict) and ('description' in element or 'selection_reason' in element or 'content_relation' in element) and bbox_field in element:
                bbox = element[bbox_field]
                print(f"   ğŸ“¦ åŸå§‹bbox: {bbox}")

                if isinstance(bbox, list) and len(bbox) == 4:
                    # æ£€æŸ¥æ¯ä¸ªåæ ‡
                    coord_valid = all(isinstance(x, (int, float)) for x in bbox)

                    if coord_valid:
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦å½’ä¸€åŒ–ï¼ˆå¦‚æœåæ ‡>1ï¼Œè¯´æ˜æ˜¯åƒç´ åæ ‡ï¼‰
                        if any(x > 1 for x in bbox) and image_width and image_height:
                            # è½¬æ¢åƒç´ åæ ‡ä¸ºå½’ä¸€åŒ–åæ ‡
                            normalized_bbox = [
                                bbox[0] / image_width,   # x1
                                bbox[1] / image_height,  # y1
                                bbox[2] / image_width,   # x2
                                bbox[3] / image_height   # y2
                            ]
                            # ç¡®ä¿åæ ‡åœ¨0-1èŒƒå›´å†…
                            normalized_bbox = [max(0, min(1, coord)) for coord in normalized_bbox]
                            element[bbox_field] = normalized_bbox
                            print(f"   ğŸ”„ å½’ä¸€åŒ–åbbox: {normalized_bbox}")
                            bbox = normalized_bbox

                        range_valid = all(0 <= x <= 1 for x in bbox)

                        # æ£€æŸ¥å®½åº¦å’Œé«˜åº¦æ˜¯å¦ä¸º0
                        width = bbox[2] - bbox[0]  # x2 - x1
                        height = bbox[3] - bbox[1]  # y2 - y1
                        size_valid = width > 0 and height > 0

                        print(f"   âœ“ åæ ‡ç±»å‹æœ‰æ•ˆ: {coord_valid}")
                        print(f"   âœ“ åæ ‡èŒƒå›´æœ‰æ•ˆ (0-1): {range_valid}")
                        print(f"   âœ“ å°ºå¯¸æœ‰æ•ˆ (å®½åº¦={width:.4f}, é«˜åº¦={height:.4f}): {size_valid}")

                        if range_valid and size_valid:
                            # ç»Ÿä¸€bboxå­—æ®µåä¸º'bbox'
                            if bbox_field != 'bbox':
                                element['bbox'] = element.pop(bbox_field)

                            # ç›´æ¥æ·»åŠ æ‰€æœ‰æœ‰æ•ˆçš„bboxï¼Œä¸è¿›è¡Œå…³é”®è¯ç­›é€‰
                            valid_elements.append(element)
                        else:
                            if not range_valid:
                                print(f"   âŒ å…ƒç´  {i} åæ ‡èŒƒå›´æ— æ•ˆ (ä¸åœ¨0-1èŒƒå›´å†…): {bbox}")
                            if not size_valid:
                                print(f"   âŒ å…ƒç´  {i} å°ºå¯¸æ— æ•ˆ (å®½åº¦={width:.4f}, é«˜åº¦={height:.4f}): {bbox}")
                    else:
                        print(f"   âŒ å…ƒç´  {i} åæ ‡ç±»å‹æ— æ•ˆ: {bbox}")
                else:
                    print(f"   âŒ å…ƒç´  {i} bboxæ ¼å¼æ— æ•ˆ: {bbox}")
            else:
                missing_fields = []
                if not isinstance(element, dict):
                    missing_fields.append("ä¸æ˜¯å­—å…¸")
                elif 'content' not in element:
                    missing_fields.append("ç¼ºå°‘content")
                elif bbox_field not in element:
                    missing_fields.append(f"ç¼ºå°‘{bbox_field}")
                print(f"   âŒ å…ƒç´  {i} éªŒè¯å¤±è´¥: {', '.join(missing_fields)}")

        # å»é™¤é‡å¤çš„bbox
        valid_elements = remove_duplicate_bboxes(valid_elements, iou_threshold=0.5)
        result['relevant_elements'] = valid_elements
        print(f"âœ… éªŒè¯åä¿ç•™ {len(valid_elements)} ä¸ªæœ‰æ•ˆå…ƒç´  (å»é‡å)")
    else:
        # ç¡®ä¿æœ‰relevant_elementså­—æ®µ
        result['relevant_elements'] = []
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„relevant_elements")

    return result

def fix_json_format(json_str):
    """å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜"""
    # ç§»é™¤å¯èƒ½çš„å¤šä½™å­—ç¬¦
    json_str = json_str.strip()

    # ä¿®å¤åŒé€—å·é—®é¢˜ (å¦‚ "0.5,," -> "0.5,")
    import re
    json_str = re.sub(r',+', ',', json_str)  # å°†å¤šä¸ªè¿ç»­é€—å·æ›¿æ¢ä¸ºå•ä¸ªé€—å·

    # å¦‚æœJSONè¢«æˆªæ–­ï¼Œå°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡
    if not json_str.endswith('}') and not json_str.endswith(']'):
        # æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„å…ƒç´ 
        last_complete_brace = json_str.rfind('}')
        if last_complete_brace > 0:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ ç»“æŸç¬¦
            temp_str = json_str[:last_complete_brace + 1]
            # è®¡ç®—å¤§æ‹¬å·å¹³è¡¡
            open_braces = temp_str.count('{')
            close_braces = temp_str.count('}')
            open_brackets = temp_str.count('[')
            close_brackets = temp_str.count(']')

            # æ·»åŠ ç¼ºå¤±çš„ç»“æŸç¬¦
            if open_brackets > close_brackets:
                temp_str += ']' * (open_brackets - close_brackets)
            if open_braces > close_braces:
                temp_str += '}' * (open_braces - close_braces)

            json_str = temp_str

    return json_str


# layer3-ç´§æ€¥ç”Ÿæˆbboxï¼ŒåŸºäºé—®é¢˜é‡Œçš„å…³é”®è¯ï¼Œç”¨OCRæ ‡è®°bboxï¼ŒOCR + å…³é”®è¯åŒ¹é… (3.0)
def generate_emergency_bboxes(question, image_path, _=1.0):
    """æœ€åä¿åº•æ–¹æ¡ˆï¼šåŸºäºå…³é”®è¯ç”Ÿæˆbbox"""
    print("ğŸš¨ ä½¿ç”¨ç´§æ€¥ä¿åº•æ–¹æ¡ˆï¼šåŸºäºå…³é”®è¯ç”Ÿæˆbbox")

    try:
        # åˆå§‹åŒ–PaddleOCRä½œä¸ºä¿åº• - ç›´æ¥åŠ è½½
        try:
            from paddleocr import PaddleOCR
            ocr = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=False, show_log=False)
            print("âœ… PaddleOCRåˆå§‹åŒ–æˆåŠŸ (ä¿åº•æ¨¡å¼)")
        except Exception as e:
            print(f"âŒ PaddleOCRåˆå§‹åŒ–å¤±è´¥: {e}")
            return generate_basic_fallback_bboxes(question)

        # ä»é—®é¢˜ä¸­æå–å…³é”®è¯
        keywords = extract_keywords_from_question(question)
        print(f"ğŸ” æå–çš„å…³é”®è¯: {keywords}")

        # æ‰§è¡ŒOCR
        if hasattr(ocr, 'predict'):
            ocr_results = ocr.predict(image_path, cls=True)
        else:
            ocr_results = ocr.ocr(image_path, cls=True)

        if not ocr_results or not ocr_results[0]:
            print("âŒ OCRæœªæ£€æµ‹åˆ°æ–‡æœ¬ï¼Œä½¿ç”¨åŸºæœ¬ä¿åº•æ–¹æ¡ˆ")
            return generate_basic_fallback_bboxes(question)

        # è·å–å›¾åƒå°ºå¯¸ç”¨äºåæ ‡å½’ä¸€åŒ–
        from PIL import Image
        image = Image.open(image_path)
        image_width, image_height = image.size

        # æŸ¥æ‰¾å…³é”®è¯å¯¹åº”çš„bbox
        emergency_elements = []
        found_keywords = set()

        for keyword in keywords:
            keyword_lower = keyword.lower()
            for line in ocr_results[0]:
                text = line[1][0].strip().lower()
                if keyword_lower in text or any(word in text for word in keyword_lower.split()):
                    # è½¬æ¢PaddleOCRçš„bboxæ ¼å¼ä¸ºç»Ÿä¸€çš„[x1, y1, x2, y2]æ ¼å¼
                    bbox_points = line[0]
                    bbox_coords = normalize_bbox_format(bbox_points)
                    if bbox_coords:
                        # å½’ä¸€åŒ–åæ ‡åˆ°0-1èŒƒå›´
                        x1, y1, x2, y2 = bbox_coords
                        normalized_bbox = [
                            round(x1 / image_width, 3),
                            round(y1 / image_height, 3),
                            round(x2 / image_width, 3),
                            round(y2 / image_height, 3)
                        ]
                        emergency_elements.append({
                            "type": "text",
                            "content": line[1][0],
                            "bbox": normalized_bbox,
                            "relevance": f"Contains keyword '{keyword}' relevant to the question"
                        })
                        found_keywords.add(keyword)
                        break

        # å¦‚æœæ²¡æ‰¾åˆ°å…³é”®è¯ï¼Œè‡³å°‘è¿”å›ä¸€äº›OCRæ–‡æœ¬
        if not emergency_elements:
            print("âš ï¸ æœªæ‰¾åˆ°å…³é”®è¯åŒ¹é…ï¼Œè¿”å›å‰3ä¸ªOCRæ–‡æœ¬")
            for line in ocr_results[0][:3]:
                bbox_points = line[0]
                bbox_coords = normalize_bbox_format(bbox_points)
                if bbox_coords:
                    # å½’ä¸€åŒ–åæ ‡åˆ°0-1èŒƒå›´
                    x1, y1, x2, y2 = bbox_coords
                    normalized_bbox = [
                        round(x1 / image_width, 3),
                        round(y1 / image_height, 3),
                        round(x2 / image_width, 3),
                        round(y2 / image_height, 3)
                    ]
                    emergency_elements.append({
                        "type": "text",
                        "content": line[1][0],
                        "bbox": normalized_bbox,
                        "relevance": f"Prominent text that might be relevant to: {question}"
                    })

        # åæ ‡å·²ç»å½’ä¸€åŒ–ï¼Œæ— éœ€è¿›ä¸€æ­¥å¤„ç†

        print(f"âœ… ä¿åº•æ–¹æ¡ˆç”Ÿæˆäº† {len(emergency_elements)} ä¸ªbbox")
        return {
            "question_analysis": f"Emergency keyword-based analysis for: {question}",
            "relevant_elements": emergency_elements,
            "answer_reasoning": f"Found text elements related to keywords: {list(found_keywords)}",
            "generation_method": "emergency_ocr",
            "generation_layer": 3,  # Layer 3: OCR + å…³é”®è¯åŒ¹é…
            "generation_description": "Generated by emergency OCR-based keyword matching when all other methods failed"
        }

    except Exception as e:
        print(f"âŒ ä¿åº•æ–¹æ¡ˆå‡ºé”™: {e}")
        return generate_basic_fallback_bboxes(question)

# layer4-æœ€åä¿åº•-ç»™å·¦è¾¹çš„ä¸‰ä¸ªæ¡†ï¼Œçº¯ç®—æ³•ä¿åº• (4.0)
def generate_basic_fallback_bboxes(question):
    """æœ€åŸºæœ¬çš„ä¿åº•æ–¹æ¡ˆ"""
    print("ğŸš¨ ä½¿ç”¨æœ€åŸºæœ¬çš„ä¿åº•æ–¹æ¡ˆ")

    # æå–å…³é”®è¯
    keywords = extract_keywords_from_question(question)

    # ä¸ºæ¯ä¸ªå…³é”®è¯ç”Ÿæˆä¸€ä¸ªå‡è®¾çš„bbox
    emergency_elements = []
    for i, keyword in enumerate(keywords[:3]):
        y_offset = i * 120 + 50
        emergency_elements.append({
            "type": "text",
            "content": f"Text containing '{keyword}'",
            "bbox": [50, y_offset, 300, y_offset + 50],
            "relevance": f"Assumed location for keyword '{keyword}' from question"
        })

    return {
        "question_analysis": f"Basic fallback analysis for: {question}",
        "relevant_elements": emergency_elements,
        "answer_reasoning": f"Generated basic bboxes for keywords: {keywords[:3]}",
        "generation_method": "basic_fallback",
        "generation_layer": 4,  # Layer 4: çº¯ç®—æ³•ä¿åº•
        "generation_description": "Generated by basic fallback with assumed bbox positions when all other methods failed"
    }

# ç»Ÿä¸€bboxæ ¼å¼
def normalize_bbox_format(bbox):
    """
    å°†ä¸åŒæ ¼å¼çš„bboxç»Ÿä¸€è½¬æ¢ä¸º [x1, y1, x2, y2] æ ¼å¼
    æ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š
    1. [x1, y1, x2, y2] - ç›´æ¥è¿”å›
    2. [[x1, y1, x2, y2]] - åµŒå¥—åˆ—è¡¨æ ¼å¼
    3. [[x1, y1], [x2, y2], [x3, y3], [x4, y4]] - 4ä¸ªè§’ç‚¹æ ¼å¼
    """
    if not bbox or not isinstance(bbox, list):
        return None

    try:
        # æ ¼å¼1: [x1, y1, x2, y2] - ç›´æ¥è¿”å›
        if len(bbox) == 4 and all(isinstance(x, (int, float)) for x in bbox):
            return bbox

        # æ ¼å¼2: [[x1, y1, x2, y2]] - åµŒå¥—åˆ—è¡¨æ ¼å¼
        if len(bbox) == 1 and isinstance(bbox[0], list) and len(bbox[0]) == 4:
            return bbox[0]

        # æ ¼å¼3: [[x1, y1], [x2, y2], [x3, y3], [x4, y4]] - 4ä¸ªè§’ç‚¹æ ¼å¼
        if len(bbox) == 4 and all(isinstance(point, list) and len(point) == 2 for point in bbox):
            x_coords = [point[0] for point in bbox]
            y_coords = [point[1] for point in bbox]
            x1, y1, x2, y2 = min(x_coords), min(y_coords), max(x_coords), max(y_coords)
            return [x1, y1, x2, y2]

        # å…¶ä»–æ ¼å¼æš‚ä¸æ”¯æŒ
        return None

    except Exception:
        return None

# ä»é—®é¢˜ä¸­æå–å…³é”®è¯
def extract_keywords_from_question(question):
    """ä»é—®é¢˜ä¸­æå–å…³é”®è¯"""
    # ç®€å•çš„å…³é”®è¯æå–
    import re

    # ç§»é™¤å¸¸è§çš„åœç”¨è¯
    stop_words = {'what', 'which', 'how', 'where', 'when', 'why', 'who', 'is', 'are', 'was', 'were',
                  'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
                  'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after',
                  'above', 'below', 'between', 'among', 'this', 'that', 'these', 'those', 'has', 'have'}

    # æå–å•è¯
    words = re.findall(r'\b[a-zA-Z]+\b', question.lower())

    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
    keywords = [word for word in words if word not in stop_words and len(word) > 2]

    # è¿”å›å‰5ä¸ªå…³é”®è¯
    return keywords[:5]


# é€‰æ‹©æ•°æ®é›†
def select_dataset():
    """é€‰æ‹©è¦å¤„ç†çš„æ•°æ®é›†"""
    print("\nğŸ“Š è¯·é€‰æ‹©è¦å¤„ç†çš„æ•°æ®é›†:")
    for i, (key, config) in enumerate(DATASETS.items(), 1):
        # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        max_samples_str = "å…¨éƒ¨" if config['default_max_samples'] is None else str(config['default_max_samples'])
        total_samples_str = f" (æ€»å…± {config['total_samples']:,} æ¡)" if 'total_samples' in config else ""
        print(f"   {i}. {config['name']} ({key}) - é»˜è®¤å¤„ç† {max_samples_str} æ¡{total_samples_str}")

        # å¤„ç†å•æ–‡ä»¶æˆ–å¤šæ–‡ä»¶é…ç½®
        if 'data_files' in config:
            data_files_str = ', '.join(config['data_files'])
            print(f"      æ•°æ®æ–‡ä»¶: {data_files_str}")
        elif 'data_file' in config:
            print(f"      æ•°æ®æ–‡ä»¶: {config['data_file']}")

        print(f"      å›¾åƒç›®å½•: {config['image_folder']}")
        print(f"      è¾“å‡ºæ–‡ä»¶: {config['output_file']}")

        # æ˜¾ç¤ºviscotæ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯
        if 'total_samples' in config:
            print(f"      æ€»æ•°æ®é‡: {config['total_samples']:,} æ¡")

        if 'datasets_breakdown' in config:
            print(f"      æ•°æ®é›†æ„æˆ:")
            for dataset, count in config['datasets_breakdown'].items():
                print(f"        - {dataset}: {count:,} æ¡")

        if 'dataset_filter' in config:
            print(f"      è¿‡æ»¤æ¡ä»¶: ä»…å¤„ç† {config['dataset_filter']} æ•°æ®")

        print()

    while True:
        try:
            choice = input(f"è¯·è¾“å…¥é€‰æ‹© (1-{len(DATASETS)}): ").strip()
            choice_num = int(choice)
            if 1 <= choice_num <= len(DATASETS):
                dataset_key = list(DATASETS.keys())[choice_num - 1]
                selected_config = DATASETS[dataset_key]
                print(f"âœ… å·²é€‰æ‹©: {selected_config['name']}")
                # æ•°æ®é›†é”®åå’Œå®Œæ•´é…ç½®å­—å…¸
                return dataset_key, selected_config
            else:
                print(f"âŒ è¯·è¾“å…¥ 1 åˆ° {len(DATASETS)} ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

# æ£€æŸ¥ç›®æ ‡æ–‡ä»¶ä¸­çš„ç»“æœï¼Œè¿”å›éœ€è¦å¤„ç†çš„æ ·æœ¬å’Œå·²å­˜åœ¨çš„ç»“æœ
def check_existing_results(output_file, samples):
    """æ£€æŸ¥å·²å­˜åœ¨çš„ç»“æœï¼Œåªåšç»Ÿè®¡ï¼Œä¸è¿‡æ»¤æ ·æœ¬"""
    if not os.path.exists(output_file):
        return samples, []

    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            existing_results = json.load(f)

        # ç»Ÿè®¡å„ç§ç±»å‹çš„ç»“æœ
        good_results_count = 0  # layer 1/2
        backup_results_count = 0  # layer 3/4

        for result in existing_results:
            bbox_analysis = result.get('bbox_analysis')
            if bbox_analysis and bbox_analysis.get('generation_layer', 1) >= 3:
                backup_results_count += 1
            else:
                good_results_count += 1

        new_samples_count = len(samples) - len(existing_results)

        print(f"ğŸ“Š æ£€æŸ¥å·²å­˜åœ¨ç»“æœ:")
        print(f"   å·²å­˜åœ¨: {len(existing_results)} æ¡ç»“æœ")
        print(f"   å…¶ä¸­å¥½ç»“æœ: {good_results_count} æ¡ (layer 1/2)")
        if backup_results_count > 0:
            print(f"   å…¶ä¸­å¤‡ç”¨æ–¹æ¡ˆ: {backup_results_count} æ¡ (generation_layer >= 3)")
        if new_samples_count > 0:
            print(f"   æ–°æ ·æœ¬: {new_samples_count} æ¡")

        return samples, existing_results

    except Exception as e:
        print(f"âš ï¸ è¯»å–å·²å­˜åœ¨æ–‡ä»¶å¤±è´¥: {e}")
        return samples, []

# æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶
def find_image_file(image_name, base_path, data_format=None):
    """æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶"""
    # å¦‚æœ image_name å·²ç»åŒ…å«æ‰©å±•åï¼Œå…ˆå»æ‰
    if '.' in image_name:
        image_name = image_name.split('.')[0]

    # æ ¹æ®æ•°æ®é›†ç±»å‹ç¡®å®šå¯èƒ½çš„æ‰©å±•å
    if data_format == 'gqa_complex':
        extensions = ['.jpg', '.jpeg']
    elif data_format == 'docvqa_complex':
        extensions = ['.png', '.jpg', '.jpeg']
    elif data_format == 'infovqa_complex':
        extensions = ['.jpeg', '.jpg', '.png']
    elif data_format == 'textvqa_complex':
        extensions = ['.jpg', '.jpeg', '.png']
    elif data_format == 'visual7w_complex':
        extensions = ['.jpg', '.jpeg', '.png']
    elif data_format == 'vqav2_complex':
        extensions = ['.jpg', '.jpeg', '.png']
    else:
        extensions = ['.jpg', '.jpeg', '.png']

    # ç›´æ¥åœ¨base_pathä¸­æŸ¥æ‰¾å›¾ç‰‡
    for ext in extensions:
        image_path = os.path.join(base_path, image_name + ext)
        if os.path.isfile(image_path):
            return image_path

    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯æ•°å­—IDï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ ¼å¼
    if image_name.isdigit():
        # æ ¹æ®æ•°æ®é›†ç±»å‹ç”Ÿæˆå¯èƒ½çš„æ–‡ä»¶åæ ¼å¼
        possible_names = [image_name]  # åŸå§‹åç§°

        if data_format == 'gqa_complex':
            # GQA: é€šå¸¸æ˜¯æ•°å­—ID
            possible_names.extend([
                f"{int(image_name):012d}",  # 12ä½è¡¥é›¶
                f"n{image_name:08d}",  # nå¼€å¤´8ä½è¡¥é›¶
            ])
        elif data_format == 'vqav2_complex':
            # VQAv2: COCOæ ¼å¼
            possible_names.extend([
                f"COCO_train2014_{int(image_name):012d}",
                f"COCO_val2014_{int(image_name):012d}",
                f"{int(image_name):012d}",
            ])
        else:
            # é€šç”¨æ ¼å¼
            possible_names.extend([
                f"{int(image_name):012d}",  # 12ä½è¡¥é›¶
                f"COCO_train2014_{int(image_name):012d}",  # COCOæ ¼å¼
                f"flickr30k_{image_name}",  # Flickr30kæ ¼å¼
            ])

        for name in possible_names:
            for ext in extensions:
                image_path = os.path.join(base_path, name + ext)
                if os.path.isfile(image_path):
                    return image_path

    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆåŒ…å«image_nameçš„æ–‡ä»¶ï¼‰
    try:
        if os.path.exists(base_path):
            for filename in os.listdir(base_path):
                if filename.lower().endswith(tuple(extensions)):
                    # å»æ‰æ‰©å±•åè¿›è¡Œæ¯”è¾ƒ
                    file_base = filename.rsplit('.', 1)[0]
                    if image_name in file_base or file_base in image_name:
                        return os.path.join(base_path, filename)
    except Exception as e:
        print(f"âš ï¸ æœç´¢å›¾ç‰‡æ—¶å‡ºé”™: {e}")

    return None

# è¯»å– viscot363ç­‰ç±»ä¼¼çš„GTæ•°æ®é›†
def load_dataset_data(dataset_config):
    """åŠ è½½æ•°æ®é›†æ•°æ®"""
    data_file = dataset_config['data_file']
    data_format = dataset_config.get('data_format', '')

    if not os.path.exists(data_file):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return None

    try:
        print(f"ğŸ“– åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        dataset_name = dataset_config['name']

        if dataset_name.startswith('Visual-CoT'):
            # Visual CoTæ•°æ®å¤„ç†
            if isinstance(data, list):
                samples = data
            else:
                samples = data.get('data', data)

            print(f"âœ… æˆåŠŸåŠ è½½ {len(samples)} æ¡æ•°æ®")

            # å¦‚æœæŒ‡å®šäº†æ•°æ®é›†è¿‡æ»¤å™¨ï¼Œåªä¿ç•™ç‰¹å®šæ•°æ®é›†çš„æ•°æ®
            if 'dataset_filter' in dataset_config:
                filter_dataset = dataset_config['dataset_filter']
                original_count = len(samples)
                samples = [s for s in samples if s.get('dataset') == filter_dataset]
                print(f"âœ… è¿‡æ»¤æ•°æ®é›† '{filter_dataset}': {len(samples)}/{original_count} æ¡æ•°æ®")

            return samples

        elif data_format.endswith('_complex'):
            # dataset_with_GT å¤æ‚é—®é¢˜æ•°æ®é›†å¤„ç†
            # è¿™äº›æ•°æ®é›†çš„æ ¼å¼æ˜¯å­—å…¸ï¼Œkeyæ˜¯é—®é¢˜IDï¼Œvalueæ˜¯é—®é¢˜ä¿¡æ¯
            if isinstance(data, dict):
                # è½¬æ¢ä¸º (key, value) å…ƒç»„åˆ—è¡¨ï¼Œæ–¹ä¾¿å¤„ç†
                samples = [(key, value) for key, value in data.items()]
                print(f"âœ… æˆåŠŸåŠ è½½ {len(samples)} æ¡å¤æ‚é—®é¢˜æ•°æ®")
                return samples
            elif isinstance(data, list):
                # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œç›´æ¥è¿”å›
                samples = data
                print(f"âœ… æˆåŠŸåŠ è½½ {len(samples)} æ¡æ•°æ®")
                return samples
            else:
                print(f"âŒ ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {type(data)}")
                return None
        else:
            # é€šç”¨å¤„ç†
            if isinstance(data, list):
                samples = data
            elif 'data' in data:
                samples = data['data']
            elif 'annotations' in data:
                samples = data['annotations']
            else:
                samples = list(data.values()) if isinstance(data, dict) else data

            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†ï¼Œå…± {len(samples)} ä¸ªæ ·æœ¬")
            return samples

    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None

# ä» viscot363ké‡Œé¢æå–é—®é¢˜ã€GT bboxã€GT answerã€image info
def get_sample_info(sample, dataset_config):
    """ä»æ ·æœ¬ä¸­æå–ä¿¡æ¯"""
    dataset_name = dataset_config['name']
    data_format = dataset_config.get('data_format', '')

    if dataset_name.startswith('Visual-CoT'):
        # Visual CoTæ•°æ®æ ¼å¼å¤„ç†
        conversations = sample.get('conversations', [])
        if len(conversations) < 2:
            return None

        # æå–é—®é¢˜ï¼ˆç¬¬ä¸€ä¸ªconversationï¼Œå»æ‰bboxæŒ‡ä»¤ï¼‰
        question_raw = conversations[0].get('value', '').replace('<image>', '').strip()
        question = question_raw.replace('Please provide the bounding box coordinate of the region that can help you answer the question better.', '').strip()
        question = question.rstrip('.?!').strip()

        # æå–bboxåæ ‡ï¼ˆç¬¬äºŒä¸ªconversationï¼‰
        bbox_str = conversations[1].get('value', '').strip()
        bbox_coords = None
        if bbox_str.startswith('[') and bbox_str.endswith(']'):
            try:
                bbox_coords = eval(bbox_str)  # è§£æbboxåæ ‡
            except:
                bbox_coords = None

        # æå–çœŸæ­£çš„ç­”æ¡ˆï¼ˆæœ€åä¸€ä¸ªconversationï¼‰
        answer = conversations[-1].get('value', '').strip() if len(conversations) > 2 else ''

        # æå–å›¾ç‰‡ä¿¡æ¯å’ŒGT bbox
        images = sample.get('image', [])
        if not images:
            return None

        # å¤„ç†å›¾ç‰‡è·¯å¾„å’ŒGT bbox
        if isinstance(images, list) and len(images) > 1:
            # ç¬¬äºŒä¸ªå…ƒç´ å¯èƒ½åŒ…å«GT bboxä¿¡æ¯ï¼šcot/v7w/v7w_276.jpg###[446, 246, 502, 345]
            image_with_bbox = images[1]
            if '###' in image_with_bbox:
                image_path, bbox_str = image_with_bbox.split('###', 1)
                try:
                    gt_bbox = eval(bbox_str.strip())  # è§£æGT bbox
                except:
                    gt_bbox = None
            else:
                image_path = image_with_bbox
                gt_bbox = None
        else:
            image_path = images[0] if isinstance(images, list) else images
            gt_bbox = None

        # ä»è·¯å¾„ä¸­æå–å›¾ç‰‡åï¼šcot/docvqa/abc.jpg -> abc
        image_name = os.path.basename(image_path).split('.')[0] if image_path else ''

        # æå–æ•°æ®é›†ä¿¡æ¯
        dataset_type = sample.get('dataset', 'unknown')

        # ç”Ÿæˆå”¯ä¸€çš„question_id
        question_id = f"{dataset_type}_{image_name}_{hash(question) % 100000}"

        return {
            'question_id': question_id,
            'question': question,
            'image_name': image_name,
            'image_path': image_path,
            'answers': [answer] if answer else [],
            'GT_bbox': gt_bbox,  # çœŸæ­£çš„GT bboxï¼ˆåƒç´ åæ ‡ï¼‰
            'viscot_bbox': bbox_coords,  # Visual-CoTç”Ÿæˆçš„bboxï¼ˆå½’ä¸€åŒ–åæ ‡ï¼‰
            # ä¸ä¿å­˜Visual-CoTçš„åŸå§‹æ•°æ®ï¼šdataset_type, conversations, dataset_name
        }
    elif data_format.endswith('_complex'):
        # dataset_with_GT å¤æ‚é—®é¢˜æ•°æ®é›†æ ¼å¼å¤„ç†
        # è¿™äº›æ•°æ®é›†çš„æ ¼å¼æ˜¯å­—å…¸ï¼Œkeyæ˜¯é—®é¢˜IDï¼Œvalueæ˜¯é—®é¢˜ä¿¡æ¯

        # å¦‚æœsampleæ˜¯å­—å…¸çš„ä¸€ä¸ªæ¡ç›®ï¼Œéœ€è¦æå–keyå’Œvalue
        if isinstance(sample, tuple) and len(sample) == 2:
            # (key, value) æ ¼å¼
            sample_key, sample_data = sample
            question_id = sample_key
        else:
            # ç›´æ¥æ˜¯sampleæ•°æ®
            sample_data = sample
            question_id = sample.get('question_id', sample.get('id', ''))

        # æå–åŸºæœ¬ä¿¡æ¯
        question = sample_data.get('question', '')
        answer = sample_data.get('answer', '')
        image_id = sample_data.get('imageId', sample_data.get('image_id', ''))

        # å¤„ç†ç­”æ¡ˆæ ¼å¼
        all_answers = sample_data.get('all_answers', [])
        if not all_answers and answer:
            all_answers = [answer]

        # æ ¹æ®ä¸åŒæ•°æ®é›†å¤„ç†å›¾åƒåç§°
        if data_format == 'gqa_complex':
            # GQA: imageId é€šå¸¸æ˜¯æ•°å­—ï¼Œå¯¹åº” playground/data/cot/gqa/xxx.jpg
            image_name = str(image_id)
        elif data_format == 'docvqa_complex':
            # DocVQA: imageId é€šå¸¸æ˜¯æ–‡æ¡£IDï¼Œå¯¹åº” playground/data/cot/docvqa/xxx.png
            image_name = str(image_id)
        elif data_format == 'infovqa_complex':
            # InfoVQA: éœ€è¦ä»åŸå§‹æ•°æ®ä¸­è·å–image_local_name
            # ç”±äºå¤æ‚æ•°æ®é›†ä¸­imageIdä¸ºç©ºï¼Œéœ€è¦ä»sample_keyä¸­æå–åŸå§‹ID
            if hasattr(sample_data, 'get') and sample_data.get('image_local_name'):
                image_name = sample_data['image_local_name'].replace('.jpeg', '').replace('.jpg', '').replace('.png', '')
            else:
                # ä»sample_keyä¸­æå–ï¼šInfoVQA_train_65718 -> 65718
                parts = question_id.split('_')
                if len(parts) >= 3:
                    original_id = parts[-1]  # è·å–æœ€åä¸€éƒ¨åˆ†ä½œä¸ºID
                    # éœ€è¦æŸ¥æ‰¾å¯¹åº”çš„image_local_nameï¼Œè¿™é‡Œå…ˆç”¨IDä½œä¸ºfallback
                    image_name = original_id
                else:
                    image_name = str(image_id)
        elif data_format == 'textvqa_complex':
            # TextVQA: imageId å¯¹åº” playground/data/cot/textvqa/xxx.jpg
            image_name = str(image_id)
        elif data_format == 'visual7w_complex':
            # Visual7W: imageId å¯¹åº” playground/data/cot/v7w/v7w_xxx.jpg
            image_name = f"v7w_{image_id}"
        elif data_format == 'vqav2_complex':
            # VQAv2: imageId å¯¹åº” playground/data/cot/vqav2/xxx.jpg
            image_name = str(image_id)
        else:
            image_name = str(image_id)

        return {
            'question_id': question_id,
            'question': question,
            'image_name': image_name,
            'image_id': image_id,
            'answers': all_answers,
            'data_format': data_format
        }
    else:
        # é€šç”¨å¤„ç†
        question_id_field = dataset_config['question_id_field']
        image_id_field = dataset_config['image_id_field']
        return {
            'question_id': sample.get(question_id_field),
            'question': sample.get('question'),
            'image_name': sample.get(image_id_field, ''),
            'image_id': sample.get(image_id_field, ''),
            'answers': sample.get('answers', [sample.get('answer', '')])
        }

# ä¸»è¦å¤„ç†å‘½ä»¤å’Œé€»è¾‘çš„ç¨‹åº
# ç”Ÿæˆä¹‹å‰å¤±è´¥çš„æ•°æ® + æ–°æ•°æ®
def process_samples_with_config(dataset_config, max_samples=None):
    """å¤„ç†æ•°æ®ç”Ÿæˆbboxï¼ˆæŒ‡å®šé…ç½®ï¼‰"""
    # å¦‚æœæ²¡æœ‰æŒ‡å®šmax_samplesï¼Œä½¿ç”¨æ•°æ®é›†çš„é»˜è®¤å€¼
    if max_samples is None:
        max_samples = dataset_config['default_max_samples']

    # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
    if max_samples is None:
        print(f"ğŸ“Š å°†å¤„ç† {dataset_config['name']} æ•°æ®é›†çš„æ‰€æœ‰æ•°æ®")
    else:
        print(f"ğŸ“Š å°†å¤„ç† {dataset_config['name']} æ•°æ®é›†çš„å‰ {max_samples} æ¡æ•°æ®")

    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_file = dataset_config['output_file']
    os.makedirs(os.path.dirname(output_file), exist_ok=True)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶è¯¢é—®ç”¨æˆ·é€‰æ‹©
    existing_results = []
    start_index = 0

    # å¦‚æœå·²ç»æœ‰è¾“å‡ºæ–‡ä»¶
    if os.path.exists(output_file):
        print(f"ğŸ“ å‘ç°å·²å­˜åœ¨çš„æ–‡ä»¶: {output_file}")
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                existing_results = json.load(f)
            print(f"ğŸ“Š å·²æœ‰ {len(existing_results)} æ¡æ•°æ®")

            # æ£€æŸ¥éœ€è¦é‡æ–°ç”Ÿæˆçš„æ ·æœ¬ï¼ˆgeneration_layer >= 3ï¼‰
            # ä¼˜å…ˆçº§æ˜¯ç”¨AIæ¨¡å‹ç”Ÿæˆçš„bboxï¼ˆLayer 1å’Œ2ï¼‰ï¼ŒLayer 3å’Œ4éœ€è¦é‡æ–°ç”Ÿæˆ
            retry_indices = []
            for idx, result in enumerate(existing_results):
                bbox_analysis = result.get('bbox_analysis')
                if bbox_analysis and bbox_analysis.get('generation_layer', 1) >= 3:
                    retry_indices.append(idx)

            print(f"ğŸ” å‘ç° {len(retry_indices)} ä¸ªä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆçš„æ ·æœ¬ (generation_layer >= 3)")

            # è¯¢é—®æ˜¯å¦å…ˆå¤„ç†å¤‡ç”¨æ–¹æ¡ˆæ ·æœ¬
            process_retry_first = False
            if retry_indices:
                while True:
                    retry_choice = input(f"\nğŸ”„ æ˜¯å¦å…ˆé‡æ–°ç”Ÿæˆ {len(retry_indices)} ä¸ªå¤‡ç”¨æ–¹æ¡ˆæ ·æœ¬ï¼Ÿ\n1. æ˜¯ï¼Œå…ˆå¤„ç†å¤‡ç”¨æ–¹æ¡ˆæ ·æœ¬\n2. å¦ï¼Œè·³è¿‡\nè¯·è¾“å…¥ 1 æˆ– 2: ").strip()
                    if retry_choice == "1":
                        process_retry_first = True
                        print(f"âœ… å°†å…ˆé‡æ–°ç”Ÿæˆ {len(retry_indices)} ä¸ªå¤‡ç”¨æ–¹æ¡ˆæ ·æœ¬")
                        break
                    elif retry_choice == "2":
                        print("âœ… è·³è¿‡å¤‡ç”¨æ–¹æ¡ˆæ ·æœ¬çš„é‡æ–°ç”Ÿæˆ")
                        break
                    else:
                        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")

            # è¯¢é—®åç»­å¤„ç†æ–¹å¼
            while True:
                choice_text = "\nè¯·é€‰æ‹©åç»­æ“ä½œ:\n1. æ¥ç€ç”Ÿæˆæ–°æ ·æœ¬ (ä»ç¬¬{}æ¡å¼€å§‹)\n2. é‡æ–°å¼€å§‹ (è¦†ç›–ç°æœ‰æ–‡ä»¶)\n3. ä»…å¤„ç†å¤‡ç”¨æ–¹æ¡ˆæ ·æœ¬åç»“æŸ\nè¯·è¾“å…¥é€‰æ‹©: ".format(len(existing_results) + 1)

                choice = input(choice_text).strip()

                if choice == "1":
                    print(f"âœ… é€‰æ‹©æ¥ç€ç”Ÿæˆï¼Œä»ç¬¬ {len(existing_results) + 1} æ¡å¼€å§‹")
                    start_index = len(existing_results)
                    continue_generation = True
                    break
                elif choice == "2":
                    print("âœ… é€‰æ‹©é‡æ–°å¼€å§‹ï¼Œå°†è¦†ç›–ç°æœ‰æ–‡ä»¶")
                    existing_results = []
                    start_index = 0
                    continue_generation = True
                    process_retry_first = False  # é‡æ–°å¼€å§‹æ—¶ä¸éœ€è¦å¤„ç†å¤‡ç”¨æ–¹æ¡ˆ
                    break
                elif choice == "3":
                    print("âœ… é€‰æ‹©ä»…å¤„ç†å¤‡ç”¨æ–¹æ¡ˆæ ·æœ¬")
                    start_index = len(existing_results)
                    continue_generation = False
                    if not process_retry_first:
                        print("âš ï¸ ä½†æ‚¨ä¹‹å‰é€‰æ‹©è·³è¿‡å¤‡ç”¨æ–¹æ¡ˆæ ·æœ¬ï¼Œå°†ç›´æ¥ç»“æŸ")
                        return
                    break
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2 æˆ– 3")
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: {e}")
            print("å°†é‡æ–°å¼€å§‹ç”Ÿæˆ")
            existing_results = []
            start_index = 0
            # åˆå§‹åŒ–ç¼ºå¤±çš„å˜é‡
            process_retry_first = False
            retry_indices = []
            continue_generation = True
    else:
        print(f"ğŸ“ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶: {output_file}")
        # åˆå§‹åŒ–å˜é‡
        process_retry_first = False
        retry_indices = []
        continue_generation = True

    # åˆå§‹åŒ–æ¨¡å‹
    print("ğŸš€ åˆå§‹åŒ–Qwen2-VLæ¨¡å‹...")
    initialize_qwen_model()

    # åŠ è½½æ•°æ®é›†
    all_samples = load_dataset_data(dataset_config)
    if not all_samples:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®é›†")
        return

    # æ˜¾ç¤ºæ•°æ®é›†æ€»æ•°é‡
    total_samples = len(all_samples)
    print(f"ğŸ“Š æ•°æ®é›†æ€»æ ·æœ¬æ•°: {total_samples:,}")

    # é™åˆ¶å¤„ç†æ•°é‡
    if max_samples is not None and len(all_samples) > max_samples:
        all_samples = all_samples[:max_samples]
        print(f"ğŸ“Š é™åˆ¶å¤„ç†æ•°é‡ä¸º: {max_samples:,}")
    else:
        print(f"ğŸ“Š å°†å¤„ç†å…¨éƒ¨ {len(all_samples):,} æ¡æ•°æ®")

    # æ£€æŸ¥å·²å­˜åœ¨çš„ç»“æœï¼Œè¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„æ ·æœ¬
    samples, existing_results = check_existing_results(output_file, all_samples)

    if len(samples) == 0:
        print("âœ… æ‰€æœ‰æ•°æ®éƒ½å·²å¤„ç†å®Œæˆï¼Œæ— éœ€é‡æ–°ç”Ÿæˆ")
        return

    results = existing_results.copy()  # å¤åˆ¶å·²æœ‰ç»“æœ
    remaining_samples = len(samples)  # éœ€è¦å¤„ç†çš„æ ·æœ¬æ•°é‡
    already_processed = len(existing_results)  # å·²å¤„ç†çš„æ ·æœ¬æ•°é‡

    print(f"ğŸ“Š å¤„ç†çŠ¶æ€: å·²å®Œæˆ {already_processed:,} æ¡ï¼Œå‰©ä½™ {remaining_samples:,} æ¡éœ€è¦å¤„ç†")
    consecutive_failures = 0  # è¿ç»­å¤±è´¥è®¡æ•°å™¨

    # ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†å¤‡ç”¨æ–¹æ¡ˆæ ·æœ¬ï¼ˆå¦‚æœé€‰æ‹©äº†çš„è¯ï¼‰
    if process_retry_first and retry_indices:
        print(f"\nğŸ”„ ç¬¬ä¸€é˜¶æ®µï¼šé‡æ–°ç”Ÿæˆ {len(retry_indices)} ä¸ªå¤‡ç”¨æ–¹æ¡ˆæ ·æœ¬")
        for idx, i in enumerate(retry_indices):
            entry = samples[i]
            sample_info = get_sample_info(entry, dataset_config)

            question_id = sample_info['question_id']
            question = sample_info['question']
            image_name = sample_info['image_name']

            old_layer = results[i].get('bbox_analysis', {}).get('generation_layer', 'unknown')
            print(f"\nğŸ”„ é‡æ–°ç”Ÿæˆæ ·æœ¬ {i+1} (è¿›åº¦: {idx+1}/{len(retry_indices)}, ID: {question_id})")
            print(f"   åŸgeneration_layer: {old_layer}")
            print(f"   é—®é¢˜: {question[:100]}...")

            # æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶
            data_format = sample_info.get('data_format', dataset_config.get('data_format', ''))
            image_path = find_image_file(image_name, dataset_config['image_folder'], data_format)

            if not image_path:
                print(f"âŒ æ‰¾ä¸åˆ°å›¾ç‰‡: {image_name}")
                continue

            try:
                # é‡æ–°ç”Ÿæˆbbox
                print("ğŸ” åˆ†æå›¾ç‰‡å’Œé—®é¢˜ï¼Œç”Ÿæˆç›¸å…³bbox...")
                bbox_analysis = generate_bboxes_for_question(image_path, question)

                # æ„å»ºç»“æœæ¡ç›®å¹¶æ›´æ–°åˆ°åŸä½ç½®
                result_entry = dict(sample_info)
                result_entry["bbox_analysis"] = bbox_analysis
                results[i] = result_entry

                # ç»Ÿè®¡ä¿¡æ¯
                if bbox_analysis and bbox_analysis.get('relevant_elements'):
                    bbox_count = len(bbox_analysis['relevant_elements'])
                    new_layer = bbox_analysis.get('generation_layer', 'unknown')
                    print(f"âœ… è¯†åˆ«äº† {bbox_count} ä¸ªç›¸å…³å…ƒç´ ")
                    if new_layer == 1:
                        print(f"ğŸ‰ é‡æ–°ç”ŸæˆæˆåŠŸï¼ä»layer {old_layer} æå‡åˆ° layer 1")
                    else:
                        print(f"âš ï¸ ä»ä¸ºå¤‡ç”¨æ–¹æ¡ˆ layer {new_layer}")
                    consecutive_failures = 0
                else:
                    print("âš ï¸ é‡æ–°ç”Ÿæˆä»æœªè¯†åˆ«åˆ°ç›¸å…³å…ƒç´ ")
                    consecutive_failures += 1

                    if consecutive_failures >= 3:
                        print(f"ğŸ”„ è¿ç»­å¤±è´¥{consecutive_failures}æ¬¡ï¼Œé‡ç½®æ¨¡å‹çŠ¶æ€...")
                        reset_model_state()
                        consecutive_failures = 0

            except Exception as e:
                print(f"âŒ é‡æ–°ç”Ÿæˆæ—¶å‡ºé”™: {e}")

            # æ¯å¤„ç†1ä¸ªæ ·æœ¬å°±ä¿å­˜ä¸€æ¬¡
            print(f"ğŸ’¾ ä¿å­˜ç»“æœ... (é‡æ–°ç”Ÿæˆè¿›åº¦: {idx+1}/{len(retry_indices)})")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            # æ¯å¤„ç†100ä¸ªæ ·æœ¬æ¸…ç†ä¸€æ¬¡GPUå†…å­˜
            if (idx + 1) % 100 == 0:
                print("ğŸ§¹ æ¸…ç†GPUå†…å­˜...")
                clear_gpu_memory()

        print(f"âœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼šé‡æ–°ç”Ÿæˆäº† {len(retry_indices)} ä¸ªå¤‡ç”¨æ–¹æ¡ˆæ ·æœ¬")

    # ç¬¬äºŒé˜¶æ®µï¼šç»§ç»­ç”Ÿæˆæ–°æ ·æœ¬ï¼ˆå¦‚æœé€‰æ‹©äº†çš„è¯ï¼‰
    if continue_generation:
        if start_index >= remaining_samples:
            print(f"âœ… æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæˆï¼æ€»å…± {len(results)} æ¡")
            return

        print(f"\nğŸ“Š ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†ç¬¬ {start_index + 1} åˆ°ç¬¬ {remaining_samples} æ¡æ–°æ•°æ®...")
        processing_indices = list(range(start_index, remaining_samples))
        total_to_process = remaining_samples - start_index

        for idx, i in enumerate(processing_indices):
            entry = samples[i]
            sample_info = get_sample_info(entry, dataset_config)

            question_id = sample_info['question_id']
            question = sample_info['question']
            image_name = sample_info['image_name']

            current_progress = idx + 1
            # æ­£å¸¸å¤„ç†æ¨¡å¼
            print(f"\nğŸ“‹ å¤„ç†æ ·æœ¬ {i+1}/{remaining_samples} (è¿›åº¦: {current_progress}/{total_to_process}, ID: {question_id})")
            print(f"   é—®é¢˜: {question[:100]}...")

            # æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶
            data_format = sample_info.get('data_format', dataset_config.get('data_format', ''))
            image_path = find_image_file(image_name, dataset_config['image_folder'], data_format)

            if not image_path:
                print(f"âŒ æ‰¾ä¸åˆ°å›¾ç‰‡: {image_name}")
                # ä»ç„¶ä¿å­˜æ¡ç›®ï¼Œä½†æ²¡æœ‰bbox
                result_entry = dict(sample_info)
                result_entry["bbox_analysis"] = None
                results.append(result_entry)
                continue

            try:
                # ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥åˆ†æå›¾ç‰‡å’Œé—®é¢˜ç”Ÿæˆbbox
                print("ğŸ” åˆ†æå›¾ç‰‡å’Œé—®é¢˜ï¼Œç”Ÿæˆç›¸å…³bbox...")
                bbox_analysis = generate_bboxes_for_question(image_path, question)

                # æ„å»ºç»“æœæ¡ç›®
                result_entry = dict(sample_info)
                result_entry["bbox_analysis"] = bbox_analysis

                # æ­£å¸¸æ¨¡å¼ï¼šè¿½åŠ åˆ°ç»“æœåˆ—è¡¨
                results.append(result_entry)

                # ç»Ÿè®¡ä¿¡æ¯å’Œå¤±è´¥å¤„ç†
                if bbox_analysis and bbox_analysis.get('relevant_elements'):
                    bbox_count = len(bbox_analysis['relevant_elements'])
                    print(f"âœ… è¯†åˆ«äº† {bbox_count} ä¸ªç›¸å…³å…ƒç´ ")
                    consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°å™¨
                else:
                    print("âš ï¸ æœªè¯†åˆ«åˆ°ç›¸å…³å…ƒç´ ")
                    consecutive_failures += 1

                    # è¿ç»­å¤±è´¥å¤ªå¤šæ¬¡æ—¶é‡ç½®æ¨¡å‹çŠ¶æ€
                    if consecutive_failures >= 3:
                        print(f"ğŸ”„ è¿ç»­å¤±è´¥{consecutive_failures}æ¬¡ï¼Œé‡ç½®æ¨¡å‹çŠ¶æ€...")
                        reset_model_state()
                        consecutive_failures = 0

            except Exception as e:
                print(f"âŒ å¤„ç†æ ·æœ¬æ—¶å‡ºé”™: {e}")
                # ä¿å­˜åŸºæœ¬ä¿¡æ¯
                result_entry = dict(sample_info)
                result_entry["bbox_analysis"] = None
                # æ­£å¸¸æ¨¡å¼ï¼šè¿½åŠ åˆ°ç»“æœåˆ—è¡¨
                results.append(result_entry)

            # æ¯å¤„ç†1ä¸ªæ ·æœ¬å°±ä¿å­˜ä¸€æ¬¡ï¼ˆå®æ—¶ä¿å­˜ï¼‰
            print(f"ğŸ’¾ ä¿å­˜ç»“æœ... (æ€»è¿›åº¦: {len(results)}/{total_samples:,})")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            # æ¯å¤„ç†100ä¸ªæ ·æœ¬æ¸…ç†ä¸€æ¬¡GPUå†…å­˜
            if (i + 1) % 100 == 0:
                print("ğŸ§¹ æ¸…ç†GPUå†…å­˜...")
                clear_gpu_memory()
    else:
        print("âœ… è·³è¿‡ç¬¬äºŒé˜¶æ®µï¼šä¸ç»§ç»­ç”Ÿæˆæ–°æ ·æœ¬")

    # æœ€ç»ˆä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆç»“æœåˆ°: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # ç»Ÿè®¡ä¿¡æ¯
    total_with_bbox = sum(1 for r in results if r.get('bbox_analysis') and r['bbox_analysis'] and r['bbox_analysis'].get('relevant_elements'))
    total_bbox_count = sum(
        len(r.get('bbox_analysis', {}).get('relevant_elements', []))
        for r in results
        if r.get('bbox_analysis') is not None
    )

    # ç»Ÿè®¡åŒ¹é…è´¨é‡ä¿¡æ¯
    similarity_scores = []
    ocr_confidences = []
    quality_scores = []

    for r in results:
        bbox_analysis = r.get('bbox_analysis', {})
        elements = bbox_analysis.get('relevant_elements', [])
        for element in elements:
            match_info = element.get('match_info', {})
            if match_info:
                similarity = match_info.get('semantic_similarity')
                if similarity is not None:
                    similarity_scores.append(similarity)

                confidence = match_info.get('ocr_confidence')
                if confidence is not None:
                    ocr_confidences.append(confidence)

                quality_score = match_info.get('match_quality_score')
                if quality_score is not None:
                    quality_scores.append(quality_score)

    print(f"\nğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(results)}")
    print(f"   æœ‰bboxçš„æ ·æœ¬: {total_with_bbox}")
    print(f"   æ€»bboxæ•°é‡: {total_bbox_count}")
    print(f"   æˆåŠŸç‡: {total_with_bbox/len(results)*100:.1f}%")

    # åŒ¹é…è´¨é‡åˆ†æ•°ç»Ÿè®¡
    if quality_scores:
        excellent_count = sum(1 for score in quality_scores if score >= 0.9)
        good_count = sum(1 for score in quality_scores if 0.8 <= score < 0.9)
        acceptable_count = sum(1 for score in quality_scores if 0.6 <= score < 0.8)
        poor_count = sum(1 for score in quality_scores if 0.5 <= score < 0.6)
        very_poor_count = sum(1 for score in quality_scores if score < 0.5)
        total_matches = len(quality_scores)

        print(f"\nğŸ¯ åŒ¹é…è´¨é‡åˆ†å¸ƒ:")
        print(f"   ä¼˜ç§€ (â‰¥0.9): {excellent_count} ({excellent_count/total_matches*100:.1f}%)")
        print(f"   è‰¯å¥½ (0.8-0.9): {good_count} ({good_count/total_matches*100:.1f}%)")
        print(f"   å¯æ¥å— (0.6-0.8): {acceptable_count} ({acceptable_count/total_matches*100:.1f}%)")
        print(f"   è¾ƒå·® (0.5-0.6): {poor_count} ({poor_count/total_matches*100:.1f}%)")
        if very_poor_count > 0:
            print(f"   å¾ˆå·® (<0.5): {very_poor_count} ({very_poor_count/total_matches*100:.1f}%)")

        avg_quality = sum(quality_scores) / len(quality_scores)
        print(f"\nğŸ“Š åŒ¹é…è´¨é‡åˆ†æ•°ç»Ÿè®¡:")
        print(f"   å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality:.3f}")
        print(f"   æœ€é«˜è´¨é‡åˆ†æ•°: {max(quality_scores):.3f}")
        print(f"   æœ€ä½è´¨é‡åˆ†æ•°: {min(quality_scores):.3f}")

    # ç›¸ä¼¼åº¦å’Œç½®ä¿¡åº¦ç»Ÿè®¡
    if similarity_scores:
        avg_similarity = sum(similarity_scores) / len(similarity_scores)
        print(f"\nğŸ“ˆ è¯­ä¹‰ç›¸ä¼¼åº¦ç»Ÿè®¡:")
        print(f"   å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
        print(f"   æœ€é«˜ç›¸ä¼¼åº¦: {max(similarity_scores):.3f}")
        print(f"   æœ€ä½ç›¸ä¼¼åº¦: {min(similarity_scores):.3f}")

    if ocr_confidences:
        avg_confidence = sum(ocr_confidences) / len(ocr_confidences)
        print(f"\nğŸ” OCRç½®ä¿¡åº¦ç»Ÿè®¡:")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
        print(f"   æœ€é«˜ç½®ä¿¡åº¦: {max(ocr_confidences):.3f}")
        print(f"   æœ€ä½ç½®ä¿¡åº¦: {min(ocr_confidences):.3f}")


def calculate_semantic_similarity(text1, text2):
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼æ€§
    ä½¿ç”¨å¤šç§ç­–ç•¥ï¼šç²¾ç¡®åŒ¹é…ã€åŒ…å«å…³ç³»ã€è¯æ±‡é‡å ã€ç¼–è¾‘è·ç¦»ç­‰
    è¿”å›0-1ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°
    """
    if not text1 or not text2:
        return 0.0

    # é¢„å¤„ç†ï¼šè½¬æ¢ä¸ºå°å†™ï¼Œå»é™¤å¤šä½™ç©ºæ ¼
    text1 = text1.strip().lower()
    text2 = text2.strip().lower()

    # 1. ç²¾ç¡®åŒ¹é… - æœ€é«˜åˆ†
    if text1 == text2:
        return 1.0

    # 2. åŒ…å«å…³ç³» - é«˜åˆ†
    if text1 in text2 or text2 in text1:
        # è®¡ç®—åŒ…å«æ¯”ä¾‹
        shorter = min(len(text1), len(text2))
        longer = max(len(text1), len(text2))
        return 0.9 * (shorter / longer)

    # 3. è¯æ±‡çº§åˆ«çš„åŒ¹é…
    import re
    words1 = set(re.findall(r'\b\w+\b', text1))
    words2 = set(re.findall(r'\b\w+\b', text2))

    if not words1 or not words2:
        return 0.0

    # è®¡ç®—è¯æ±‡é‡å ç‡
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    jaccard_similarity = len(intersection) / len(union) if union else 0.0

    # 4. å­—ç¬¦çº§åˆ«çš„ç›¸ä¼¼æ€§ï¼ˆç¼–è¾‘è·ç¦»ï¼‰
    def levenshtein_distance(s1, s2):
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    # è®¡ç®—ç¼–è¾‘è·ç¦»ç›¸ä¼¼æ€§
    max_len = max(len(text1), len(text2))
    if max_len == 0:
        return 0.0

    edit_distance = levenshtein_distance(text1, text2)
    edit_similarity = 1 - (edit_distance / max_len)

    # 5. ç»¼åˆè¯„åˆ†
    # è¯æ±‡é‡å æƒé‡æ›´é«˜ï¼Œç¼–è¾‘è·ç¦»ä½œä¸ºè¡¥å……
    final_score = 0.7 * jaccard_similarity + 0.3 * edit_similarity

    # ç¡®ä¿åˆ†æ•°åœ¨0-1èŒƒå›´å†…
    return max(0.0, min(1.0, final_score))


# åŒ¹é…å’Œqwenç›¸ç¬¦çš„ocræ–‡æœ¬
def match_content_with_ocr(qwen_elements, image_path, _=""):
    """
    ç®€åŒ–çš„è¯­ä¹‰åŒ¹é…ç­–ç•¥ï¼š
    1. ä¼˜å…ˆåœ¨ç²—ç•¥åŒºåŸŸå†…æ‰¾åˆ†æ•°>0.5çš„æœ€é«˜åˆ†åŒ¹é…
    2. å¦‚æœç²—ç•¥åŒºåŸŸå†…æ²¡æ‰¾åˆ°ï¼Œå†åœ¨æ•´å¼ å›¾æ‰¾åˆ†æ•°æœ€é«˜çš„åŒ¹é…
    3. ç›¸ä¼¼åº¦ç›¸åŒæ—¶ï¼Œé€‰æ‹©OCRç½®ä¿¡åº¦æœ€é«˜çš„
    """
    try:
        # åˆå§‹åŒ–OCR - ç›´æ¥åŠ è½½ï¼Œä¸ä½¿ç”¨å‡½æ•°
        from paddleocr import PaddleOCR
        ocr = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=False, show_log=False)

        # 1.è·å–OCRç»“æœ
        try:
            if hasattr(ocr, 'predict'):
                ocr_results = ocr.predict(image_path, cls=True)
            else:
                ocr_results = ocr.ocr(image_path, cls=True)
        except Exception as e:
            print(f"âŒ OCRå¤„ç†å‡ºé”™: {e}")
            ocr_results = None

        if not ocr_results or not ocr_results[0]:
            print("âš ï¸ OCRæœªæ£€æµ‹åˆ°æ–‡æœ¬ï¼Œä½¿ç”¨åŸå§‹ç»“æœ")
            return qwen_elements

        # è°ƒè¯•ï¼šæ˜¾ç¤ºOCRè¯†åˆ«åˆ°çš„æ‰€æœ‰æ–‡æœ¬
        print(f"ğŸ” OCRè¯†åˆ«åˆ° {len(ocr_results[0])} ä¸ªæ–‡æœ¬å—:")
        for i, ocr_line in enumerate(ocr_results[0][:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            ocr_text = ocr_line[1][0].strip()
            confidence = ocr_line[1][1]
            print(f"   {i+1}. '{ocr_text}' (conf: {confidence:.2f})")

        # è·å–å›¾åƒå°ºå¯¸
        from PIL import Image
        image = Image.open(image_path)
        image_width, image_height = image.size

        enhanced_elements = []

        # 2. æ‰¾åˆ°å’ŒqwenåŒ¹é…çš„bbox
        for qwen_element in qwen_elements:
            # åˆå§‹åŒ–å˜é‡é¿å…ä½œç”¨åŸŸé—®é¢˜
            rough_x1, rough_y1, rough_x2, rough_y2 = 0, 0, image_width, image_height
            # å¤„ç†ä¸¤ç§å¯èƒ½çš„æ ¼å¼ï¼šå­—ç¬¦ä¸²æˆ–å­—å…¸
            if isinstance(qwen_element, str):
                qwen_content = qwen_element.strip().lower()
                rough_bbox = None  # æ²¡æœ‰ç²—ç•¥åŒºåŸŸä¿¡æ¯
            else:
                # ä¼˜å…ˆä½¿ç”¨descriptionè¿›è¡ŒOCRåŒ¹é…ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨content_relationï¼Œæœ€åä½¿ç”¨selection_reason
                qwen_content = qwen_element.get('description',
                              qwen_element.get('content_relation',
                              qwen_element.get('selection_reason', ''))).strip().lower()
                rough_bbox = qwen_element.get('rough_bbox')  # ä»qwenå¤„è·å–ç²—ç•¥åŒºåŸŸ

            best_match = None

            print(f"ğŸ” å¯»æ‰¾æ–‡æœ¬: '{qwen_content}'")

            # 2.1 å°†ç²—ç•¥åŒºåŸŸè½¬æ¢ä¸ºåƒç´ åæ ‡ï¼Œå¹¶é€‚å½“æ‰©å¤§æœç´¢èŒƒå›´
            if rough_bbox:
                print(f"   åœ¨ç²—ç•¥åŒºåŸŸ: {rough_bbox}")
                # æ‰©å¤§è¾¹è·ï¼Œé¿å…åŒºåŸŸå¤ªå°
                margin_x = int(MARGIN_RATIO * image_width)  # æ°´å¹³è¾¹è·
                margin_y = int(MARGIN_RATIO * image_height)  # å‚ç›´è¾¹è·

                rough_x1 = max(0, int(rough_bbox[0] * image_width) - margin_x)
                rough_y1 = max(0, int(rough_bbox[1] * image_height) - margin_y)
                rough_x2 = min(image_width, int(rough_bbox[2] * image_width) + margin_x)
                rough_y2 = min(image_height, int(rough_bbox[3] * image_height) + margin_y)

                print(f"   æ‰©å¤§ååƒç´ åŒºåŸŸ: [{rough_x1}, {rough_y1}, {rough_x2}, {rough_y2}]")
            else:
                print(f"   åœ¨å…¨å›¾åŒºåŸŸ: [{rough_x1}, {rough_y1}, {rough_x2}, {rough_y2}]")

            # æ”¶é›†æ‰€æœ‰å€™é€‰åŒ¹é…é¡¹
            all_candidates = []

            # éå†æ‰€æœ‰OCRç»“æœï¼Œè®¡ç®—ç›¸ä¼¼åº¦
            for ocr_line in ocr_results[0]:
                bbox_points = ocr_line[0]
                ocr_text = ocr_line[1][0].strip().lower()
                confidence = ocr_line[1][1]

                # è®¡ç®—OCRæ–‡æœ¬çš„ä¸­å¿ƒç‚¹
                x_coords = [point[0] for point in bbox_points]
                y_coords = [point[1] for point in bbox_points]
                center_x = sum(x_coords) / len(x_coords)
                center_y = sum(y_coords) / len(y_coords)

                # æ£€æŸ¥æ˜¯å¦åœ¨ç²—ç•¥åŒºåŸŸå†…
                in_rough_area = (rough_x1 <= center_x <= rough_x2 and
                               rough_y1 <= center_y <= rough_y2)

                # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ - ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼æ€§åŒ¹é…
                text_match = False
                score = 0

                # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€§
                score = calculate_semantic_similarity(qwen_content, ocr_text)

                # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡0.5ï¼Œè®¤ä¸ºåŒ¹é…æˆåŠŸï¼ˆå› ä¸ºæˆ‘ä»¬ä¼šé€‰æ‹©æœ€é«˜åˆ†çš„ï¼‰
                if score >= 0.5:
                    text_match = True
                    # æ˜¾ç¤ºé«˜è´¨é‡åŒ¹é…çš„è¯¦ç»†ä¿¡æ¯
                    if score >= 0.95:
                        print(f"   ğŸ¯ é«˜è´¨é‡åŒ¹é… (ç›¸ä¼¼åº¦: {score:.3f}): '{qwen_content}' â‰ˆ '{ocr_text}'")
                    elif score >= 0.9:
                        print(f"   âœ… è‰¯å¥½åŒ¹é… (ç›¸ä¼¼åº¦: {score:.3f}): '{qwen_content}' â‰ˆ '{ocr_text}'")
                else:
                    text_match = False

                if text_match:
                    all_candidates.append({
                        'bbox_points': bbox_points,
                        'ocr_text': ocr_text,
                        'confidence': confidence,
                        'score': score,
                        'center': (int(center_x), int(center_y)),
                        'in_rough_area': in_rough_area
                    })

            # ç®€åŒ–çš„è¯­ä¹‰åŒ¹é…ç­–ç•¥ï¼šæ¸…æ™°çš„ä¼˜å…ˆçº§
            best_match = None
            match_strategy = ""

            if all_candidates:
                # è¿‡æ»¤å‡ºç½®ä¿¡åº¦è¶³å¤Ÿçš„å€™é€‰é¡¹
                valid_candidates = [c for c in all_candidates if c['confidence'] > CONFIDENCE_THRESHOLD]

                if valid_candidates:
                    if rough_bbox:
                        # ä¼˜å…ˆçº§1: ç²—ç•¥åŒºåŸŸå†…åˆ†æ•°>0.5çš„æœ€é«˜åˆ†åŒ¹é…
                        rough_area_candidates = [c for c in valid_candidates if c['in_rough_area'] and c['score'] > 0.5]
                        if rough_area_candidates:
                            best_match = max(rough_area_candidates, key=lambda x: (x['score'], x['confidence']))
                            match_strategy = f"ç²—ç•¥åŒºåŸŸå†…æœ€ä½³åŒ¹é… (ç›¸ä¼¼åº¦: {best_match['score']:.3f})"
                        else:
                            # ä¼˜å…ˆçº§2: æ•´å¼ å›¾æ‰¾åˆ†æ•°æœ€é«˜çš„åŒ¹é…
                            best_match = max(valid_candidates, key=lambda x: (x['score'], x['confidence']))
                            match_strategy = f"å…¨å›¾æœ€ä½³åŒ¹é… (ç›¸ä¼¼åº¦: {best_match['score']:.3f})"
                    else:
                        # æ²¡æœ‰ç²—ç•¥åŒºåŸŸæ—¶ï¼Œç›´æ¥é€‰æ‹©å…¨å›¾æœ€é«˜åˆ†
                        best_match = max(valid_candidates, key=lambda x: (x['score'], x['confidence']))
                        match_strategy = f"å…¨å›¾æœ€ä½³åŒ¹é… (ç›¸ä¼¼åº¦: {best_match['score']:.3f})"



            if best_match:
                # è½¬æ¢OCRåæ ‡ä¸ºå½’ä¸€åŒ–æ ¼å¼
                bbox_points = best_match['bbox_points']
                x_coords = [point[0] for point in bbox_points]
                y_coords = [point[1] for point in bbox_points]
                x1, y1, x2, y2 = min(x_coords), min(y_coords), max(x_coords), max(y_coords)

                # å½’ä¸€åŒ–åæ ‡
                norm_bbox = [
                    round(x1 / image_width, 3),
                    round(y1 / image_height, 3),
                    round(x2 / image_width, 3),
                    round(y2 / image_height, 3)
                ]

                # å¤„ç†ä¸¤ç§å¯èƒ½çš„æ ¼å¼ï¼šå­—ç¬¦ä¸²æˆ–å­—å…¸
                if isinstance(qwen_element, str):
                    content = qwen_element
                else:
                    # ä¼˜å…ˆä½¿ç”¨descriptionï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨content_relationï¼Œæœ€åä½¿ç”¨selection_reason
                    content = qwen_element.get('description',
                             qwen_element.get('content_relation',
                             qwen_element.get('selection_reason', qwen_element)))

                # è®¡ç®—ä¸ç²—ç•¥åŒºåŸŸçš„è·ç¦»ï¼ˆæ—¢ç„¶è¿›å…¥OCRå¢å¼ºï¼Œè‚¯å®šæœ‰ç²—ç•¥åŒºåŸŸï¼‰
                # ä½¿ç”¨åŸå§‹çš„rough_bboxæ¥è®¡ç®—è·ç¦»ï¼Œè€Œä¸æ˜¯æ‰©å¤§åçš„åŒºåŸŸ
                if rough_bbox and len(rough_bbox) == 4:
                    # ä½¿ç”¨åŸå§‹ç²—ç•¥åŒºåŸŸçš„ä¸­å¿ƒç‚¹
                    original_rough_center_x = (rough_bbox[0] + rough_bbox[2]) * image_width / 2
                    original_rough_center_y = (rough_bbox[1] + rough_bbox[3]) * image_height / 2

                    distance_to_rough = ((best_match['center'][0] - original_rough_center_x)**2 +
                                       (best_match['center'][1] - original_rough_center_y)**2)**0.5
                    # å½’ä¸€åŒ–è·ç¦»ï¼ˆç›¸å¯¹äºå›¾åƒå¯¹è§’çº¿é•¿åº¦ï¼‰
                    diagonal_length = (image_width**2 + image_height**2)**0.5
                    distance_to_rough = round(distance_to_rough / diagonal_length, 4)

                    print(f"   ğŸ“ è·ç¦»ç²—ç•¥åŒºåŸŸä¸­å¿ƒ: {distance_to_rough:.4f} (å½’ä¸€åŒ–)")
                else:
                    # ç†è®ºä¸Šä¸åº”è¯¥åˆ°è¿™é‡Œï¼Œå› ä¸ºè¿›å…¥OCRå¢å¼ºè¯´æ˜æœ‰ç²—ç•¥åŒºåŸŸ
                    print(f"âš ï¸ è­¦å‘Šï¼šè¿›å…¥OCRå¢å¼ºä½†æ²¡æœ‰æœ‰æ•ˆç²—ç•¥åŒºåŸŸ: {rough_bbox}")
                    distance_to_rough = None

                # æ„å»ºmatch_infoï¼Œä¸åŒ…å«in_rough_areaå‚æ•°
                match_info = {
                    'semantic_similarity': round(best_match['score'], 4),  # è¯­ä¹‰ç›¸ä¼¼åº¦
                    'ocr_confidence': round(best_match['confidence'], 4),  # OCRç½®ä¿¡åº¦
                    'ocr_text': best_match['ocr_text'],  # å®é™…åŒ¹é…çš„OCRæ–‡æœ¬
                    'match_strategy': match_strategy,  # åŒ¹é…ç­–ç•¥
                    'distance_to_rough': distance_to_rough,  # æ€»æ˜¯åŒ…å«è·ç¦»ä¿¡æ¯
                    'match_quality_score': round(best_match['score'], 4)  # åŒ¹é…è´¨é‡åˆ†æ•° (0-1)
                }

                # ä¿å­˜æ‰€æœ‰åŸå§‹å­—æ®µ
                element_with_match_info = {
                    'bbox': norm_bbox,
                    'match_info': match_info
                }

                # ä¿å­˜åŸå§‹çš„æ‰€æœ‰å­—æ®µ
                if isinstance(qwen_element, dict):
                    # ä¿å­˜description, selection_reason, content_relationç­‰å­—æ®µ
                    for key in ['description', 'selection_reason', 'content_relation']:
                        if key in qwen_element:
                            element_with_match_info[key] = qwen_element[key]

                    # ä¿å­˜rough_bboxå­—æ®µï¼ˆQwen2-VLç”Ÿæˆçš„åŸå§‹bboxï¼‰
                    if 'rough_bbox' in qwen_element:
                        element_with_match_info['rough_bbox'] = qwen_element['rough_bbox']

                    # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¹Ÿä¿å­˜contentå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if 'content' in qwen_element:
                        element_with_match_info['content'] = qwen_element['content']
                else:
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œä¿å­˜ä¸ºcontent
                    element_with_match_info['content'] = content

                enhanced_elements.append(element_with_match_info)

                print(f"âœ… åŒ¹é…æˆåŠŸ({match_strategy}): '{qwen_content}' -> OCR: '{best_match['ocr_text']}' (ç›¸ä¼¼åº¦: {best_match['score']:.3f}) -> {norm_bbox}")
            else:
                # åˆ†ææœªåŒ¹é…çš„åŸå› 
                total_ocr_texts = len(ocr_results[0]) if ocr_results and ocr_results[0] else 0
                candidates_found = len(all_candidates)

                # æ‰¾åˆ°æœ€é«˜ç›¸ä¼¼åº¦ï¼ˆå³ä½¿ä½äºé˜ˆå€¼ï¼‰
                best_similarity = 0
                best_ocr_text = ""
                if ocr_results and ocr_results[0]:
                    for ocr_line in ocr_results[0]:
                        ocr_text = ocr_line[1][0].strip().lower()
                        similarity = calculate_semantic_similarity(qwen_content, ocr_text)
                        if similarity > best_similarity:
                            best_similarity = similarity
                            best_ocr_text = ocr_text

                print(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…: '{qwen_content}' (OCRæ–‡æœ¬æ€»æ•°: {total_ocr_texts}, å€™é€‰æ•°: {candidates_found}, æœ€é«˜ç›¸ä¼¼åº¦: {best_similarity:.3f} vs '{best_ocr_text}')")
                
                # OCRåŒ¹é…å¤±è´¥æ—¶ï¼Œä¿ç•™åŸå§‹å…ƒç´ ï¼ˆåŒ…å«rough_bboxï¼‰
                if isinstance(qwen_element, dict):
                    enhanced_elements.append(qwen_element)
                else:
                    enhanced_elements.append({'content': qwen_element})

        return enhanced_elements

    except Exception as e:
        print(f"âŒ OCRåŒ¹é…å¤±è´¥: {e}")
        return qwen_elements



if __name__ == "__main__":

    # éªŒè¯CUDAè®¾ç½®
    if torch.cuda.is_available():
        print(f"ğŸ”§ å½“å‰å¯è§GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"ğŸ”§ å½“å‰GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        exit(1)

    # é€‰æ‹©æ•°æ®é›†
    dataset_key, dataset_config = select_dataset()

    # å¤„ç†æ•°æ®
    max_samples = dataset_config['default_max_samples']

    process_samples_with_config(dataset_config, max_samples)

